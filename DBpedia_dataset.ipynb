{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/Build_KGs_entities/blob/main/DBpedia_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "HN96nrT6_vv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed860ccd-320e-4733-fb16-8b5a8bafd7e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working folder ready!\n",
            "--------------\n"
          ]
        }
      ],
      "source": [
        "#@title Clones, Installs and functions\n",
        "from IPython.display import clear_output, HTML, display\n",
        "import os\n",
        "! pip install SPARQLWrapper\n",
        "! pip install colored\n",
        "! pip install xmltodict\n",
        "\n",
        "# Clone Build_KGs_entities repo\n",
        "! git clone https://github.com/mille-s/Build_KGs_entities.git\n",
        "# Delete locally to avoid confusion\n",
        "! rm '/content/Build_KGs_entities/DBpedia_dataset.ipynb'\n",
        "\n",
        "# clone wikipedia page generator repo\n",
        "! git clone https://github.com/mille-s/WikipediaPage_Generator.git\n",
        "# Delete locally to avoid confusion\n",
        "! rm 'WikipediaPage_Generator/Wikipedia_generator.ipynb'\n",
        "\n",
        "# clone dcu_tcd_webnlg repo\n",
        "! git clone https://github.com/mille-s/DCU_TCD_FORGe_WebNLG23.git\n",
        "# Delete locally to avoid confusion\n",
        "! rm 'DCU_TCD_FORGe_WebNLG23/DCU_TCD_FORGe_WebNLG23.ipynb'\n",
        "\n",
        "triple2predArg = os.path.join('/content', 'XML')\n",
        "os.makedirs(triple2predArg)\n",
        "\n",
        "def aggregate_info_and_get_propLabel(ontology_properties):\n",
        "  dico_properties = {}\n",
        "  for prop in ontology_properties:\n",
        "    # Get raw labels without the url part\n",
        "    _ , property_no_prefix = os.path.split(prop['property'])\n",
        "    domain_no_prefix = 'Unknown'\n",
        "    if not prop['domain'] == 'Unknown':\n",
        "      _ , domain_no_prefix = os.path.split(prop['domain'])\n",
        "    range_no_prefix = 'Unknown'\n",
        "    if not prop['range'] == 'Unknown':\n",
        "      _ , range_no_prefix = os.path.split(prop['range'])\n",
        "    # The first time a property is found, create a dico entry with domain and range info\n",
        "    if property_no_prefix not in dico_properties.keys():\n",
        "      dico_properties[property_no_prefix] = {'domain': [domain_no_prefix], 'range': [range_no_prefix]}\n",
        "    # The second time, only append the domain and range if they haven't been seen to this point\n",
        "    else:\n",
        "      if domain_no_prefix not in dico_properties[property_no_prefix]['domain']:\n",
        "        dico_properties[property_no_prefix]['domain'].append(domain_no_prefix)\n",
        "      if range_no_prefix not in dico_properties[property_no_prefix]['range']:\n",
        "        dico_properties[property_no_prefix]['range'].append(range_no_prefix)\n",
        "  return dico_properties\n",
        "\n",
        "clear_output()\n",
        "print('Working folder ready!\\n--------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4qti7Qw_Yxo"
      },
      "source": [
        "# Preliminary work: get info to build datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nrZyZgVBd30D"
      },
      "outputs": [],
      "source": [
        "#@title SPARQL query to get all properties in DBpedia (from ChatGPT)\n",
        "\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "def getDBpediaProperties():\n",
        "  # Set up the SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query to retrieve properties with the prefix \"http://dbpedia.org/ontology\"\n",
        "  query = \"\"\"\n",
        "  SELECT DISTINCT ?property\n",
        "  WHERE {\n",
        "    ?property a rdf:Property .\n",
        "    FILTER(STRSTARTS(STR(?property), \"http://dbpedia.org/ontology/\"))\n",
        "  }\n",
        "  ORDER BY ?property\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  try:\n",
        "      # Execute the query\n",
        "      results = sparql.query().convert()\n",
        "\n",
        "      # Process and display the results\n",
        "      properties = [result[\"property\"][\"value\"] for result in results[\"results\"][\"bindings\"]]\n",
        "      print(f\"Number of properties used in DBpedia: {str(len(properties))}\")\n",
        "      # for prop in properties:\n",
        "      #     print(prop)\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "\n",
        "  return properties\n",
        "\n",
        "list_properties = getDBpediaProperties()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "pc4ucYC1fFB1"
      },
      "outputs": [],
      "source": [
        "#@title SPARQL query to get the number of instances of each property\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import os\n",
        "import json\n",
        "\n",
        "def getNumInstancesProperty(property_label):\n",
        "  # lowercase first character\n",
        "  head, tail = os.path.split(property_label)\n",
        "  lowCase_tail = tail[0].lower() + tail[1:]\n",
        "  lowCase_property_label = os.path.join(head, lowCase_tail)\n",
        "\n",
        "  # Set up the SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query to count the number of instances of the property 'dbo:birthDate'\n",
        "  query = f\"\"\"\n",
        "  SELECT (COUNT(*) AS ?count)\n",
        "  WHERE {{\n",
        "    ?subject <{lowCase_property_label}> ?object .\n",
        "  }}\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  try:\n",
        "      # Execute the query\n",
        "      results = sparql.query().convert()\n",
        "\n",
        "      # Extract and print the count\n",
        "      count = results[\"results\"][\"bindings\"][0][\"count\"][\"value\"]\n",
        "      # print(f\"Number of instances of {lowCase_property_label}: {count}\")\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "\n",
        "  return(lowCase_property_label, count)\n",
        "\n",
        "def createDicoCountOccurrenceProperties(list_properties):\n",
        "  dico_count_occurrences = {}\n",
        "  for i, property_label in enumerate(list_properties):\n",
        "    lowCase_property_label, count = getNumInstancesProperty(property_label)\n",
        "    dico_count_occurrences[lowCase_property_label] = int(count)\n",
        "    print(f'{str(i)}/{str(len(list_properties))}: {property_label} = {count}')\n",
        "\n",
        "  sorted_dico_count_occurrences = {k: v for k, v in sorted(dico_count_occurrences.items(), key=lambda item: item[1], reverse=True)}\n",
        "  with open(\"dico_count_occurrences_dbp_props.json\", \"w\") as outfile:\n",
        "      json.dump(sorted_dico_count_occurrences, outfile)\n",
        "\n",
        "createDicoCountOccurrenceProperties(list_properties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "w83ObZjQ_Pn8"
      },
      "outputs": [],
      "source": [
        "#@title SPARQL query for all properties getting domain/range class\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "# Set up the DBpedia SPARQL endpoint\n",
        "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "sparql.setReturnFormat(JSON)\n",
        "\n",
        "# SPARQL query to select ontology properties with domain and range\n",
        "# This is supposed to return all properties, but a lot seem to be missing, not sure why\n",
        "query = \"\"\"\n",
        "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "\n",
        "SELECT ?property ?domain ?range WHERE {\n",
        "    ?property a rdf:Property .\n",
        "    FILTER(STRSTARTS(STR(?property), \"http://dbpedia.org/ontology/\")) .\n",
        "    OPTIONAL { ?property rdfs:domain ?domain . }\n",
        "    OPTIONAL { ?property rdfs:range ?range . }\n",
        "}\n",
        "LIMIT 50000  # Increase this limit if needed\n",
        "\"\"\"\n",
        "\n",
        "# Run the query\n",
        "sparql.setQuery(query)\n",
        "results = sparql.query().convert()\n",
        "\n",
        "# Extract and display the results\n",
        "ontology_properties = []\n",
        "for result in results[\"results\"][\"bindings\"]:\n",
        "    property_uri = result[\"property\"][\"value\"]\n",
        "    domain = result.get(\"domain\", {}).get(\"value\", \"Unknown\")\n",
        "    range_class = result.get(\"range\", {}).get(\"value\", \"Unknown\")\n",
        "    ontology_properties.append({\n",
        "        \"property\": property_uri,\n",
        "        \"domain\": domain,\n",
        "        \"range\": range_class\n",
        "    })\n",
        "\n",
        "# Print or process the results\n",
        "for prop in ontology_properties[:100]:\n",
        "    print(f\"Property: {prop['property']}\")\n",
        "    print(f\"  Domain: {prop['domain']}\")\n",
        "    print(f\"  Range: {prop['range']}\")\n",
        "    print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "b1Q4FSq8O4BU"
      },
      "outputs": [],
      "source": [
        "#@title SPARQL query for selected properties getting domain/range class\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "WebNLG_properties_list = ['http://dbpedia.org/ontology/'+line.strip() for line in codecs.open('/content/all_properties.txt', 'r', 'utf-8').readlines()]\n",
        "\n",
        "def get_domain_range(properties):\n",
        "    domain_range_info = []\n",
        "\n",
        "    for property_uri in properties:\n",
        "        print(f'Cheking property {property_uri}')\n",
        "        # SPARQL query to get domain and range for the specific property\n",
        "        query = f\"\"\"\n",
        "        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "\n",
        "        SELECT ?domain ?range WHERE {{\n",
        "            <{property_uri}> a rdf:Property .\n",
        "            OPTIONAL {{ <{property_uri}> rdfs:domain ?domain . }}\n",
        "            OPTIONAL {{ <{property_uri}> rdfs:range ?range . }}\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        # Run the query\n",
        "        sparql.setQuery(query)\n",
        "        results = sparql.query().convert()\n",
        "\n",
        "        # Extract domain and range information from the results\n",
        "        for result in results[\"results\"][\"bindings\"]:\n",
        "            domain = result.get(\"domain\", {}).get(\"value\", \"Unknown\")\n",
        "            range_class = result.get(\"range\", {}).get(\"value\", \"Unknown\")\n",
        "            domain_range_info.append({\n",
        "                \"property\": property_uri,\n",
        "                \"domain\": domain,\n",
        "                \"range\": range_class\n",
        "            })\n",
        "\n",
        "    return domain_range_info\n",
        "\n",
        "# Retrieve domain and range for each property in the list\n",
        "ontology_properties = get_domain_range(WebNLG_properties_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "cpS46ZYSTpIn"
      },
      "outputs": [],
      "source": [
        "#@title Aggregate possible domain/ranges for each property\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "dico_properties = aggregate_info_and_get_propLabel(ontology_properties)\n",
        "\n",
        "# for prop in properties_info:\n",
        "#     print(f\"Property: {prop['property']}\", f\"  Domain: {prop['domain']}\", f\"  Range: {prop['range']}\")\n",
        "\n",
        "# print(len(dico_properties.keys()))\n",
        "# print(len(properties_info))\n",
        "\n",
        "list_properties = [line.strip() for line in codecs.open('/content/all_properties.txt', 'r', 'utf-8').readlines()]\n",
        "missing_props = []\n",
        "for WebNLG_property in list_properties:\n",
        "  if WebNLG_property not in dico_properties.keys():\n",
        "    missing_props.append(WebNLG_property)\n",
        "\n",
        "print('Missing properties: '+str(len(missing_props))+'/'+str(len(list_properties)), missing_props)\n",
        "# We need to check the Original property labels, not the modified ones, that way we should get them all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "XYZTIMXpfs1b"
      },
      "outputs": [],
      "source": [
        "#@title Get examples for properties\n",
        "# Get a list of sample subject-object values given an input list of properties.\n",
        "\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import pandas as pd\n",
        "import csv\n",
        "import json\n",
        "\n",
        "num_examples_desired = 30#@param\n",
        "\n",
        "def get_dbpedia_property_examples(property_uri, dataframe_examples, count_props, num_occ_uri, limit=10):\n",
        "  sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
        "  query = f\"\"\"\n",
        "  SELECT DISTINCT ?subject ?object WHERE {{\n",
        "    ?subject <{property_uri}> ?object .\n",
        "  }} LIMIT {limit}\n",
        "  \"\"\"\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  count_examples = 0\n",
        "  for result in results[\"results\"][\"bindings\"]:\n",
        "    subject_uri = result[\"subject\"][\"value\"]\n",
        "    object_value = result[\"object\"][\"value\"]\n",
        "\n",
        "    # Extract a readable name from the subject and object URI\n",
        "    subject_label = \"\"\n",
        "    if subject_uri.startswith(\"http://dbpedia.org/resource/\"):\n",
        "      subject_label = subject_uri.split(\"/\")[-1].replace(\"_\", \" \")\n",
        "    else:\n",
        "      subject_label = subject_uri\n",
        "    object_label = \"\"\n",
        "    if object_value.startswith(\"http://dbpedia.org/resource/\"):\n",
        "      object_label = object_value.split(\"/\")[-1].replace(\"_\", \" \")\n",
        "    else:\n",
        "      object_label = object_value\n",
        "\n",
        "    prop_label = property_uri.split(\"/\")[-1]  # e.g., 'birthDate'\n",
        "    dataframe_examples.loc[count_props*limit+count_examples] = [count_props, num_occ_uri, subject_label, prop_label, object_label]\n",
        "    count_examples += 1\n",
        "\n",
        "# properties_uri = [\"http://dbpedia.org/ontology/birthDate\", \"http://dbpedia.org/ontology/birthPlace\"]\n",
        "list_props_dico = json.load(open('/content/dico_count_occurrences_dbp_props.json', 'r'))\n",
        "# Get list of URIs for which there is at least 10 occurrences of the property (to filter out possibly bad properties)\n",
        "properties_uri = []\n",
        "# Also get list of the number of occurrences of each URI to store in the final CSV\n",
        "num_occ_uris = []\n",
        "for prop in list_props_dico.keys():\n",
        "  if list_props_dico[prop] >= 10:\n",
        "    properties_uri.append(prop)\n",
        "    num_occ_uris.append(list_props_dico[prop])\n",
        "assert len(properties_uri) == len(num_occ_uris)\n",
        "print(f'Number of properties with at least 10 occurrences: {len(properties_uri)}')\n",
        "\n",
        "# Create dataframe\n",
        "dataframe_examples = pd.DataFrame(columns=[\"id\", \"num-occurrences\", \"Subject\", \"Property\", \"Object\"])\n",
        "# Example usage:\n",
        "for counter_uris, property_uri in enumerate(properties_uri):\n",
        "  print(f'{str(counter_uris)}/{str(len(properties_uri))}: {property_uri}...')\n",
        "  num_occ_uri = num_occ_uris[counter_uris]\n",
        "  get_dbpedia_property_examples(property_uri, dataframe_examples, counter_uris, num_occ_uri, int(num_examples_desired))\n",
        "\n",
        "# Save dataframe as CSV\n",
        "dataframe_examples.to_csv('dbp_props_examples.csv', index=False)\n",
        "\n",
        "print(dataframe_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "oRDQt9UED6Bh"
      },
      "outputs": [],
      "source": [
        "#@title Process CSV created from annotated examples for properties and get lists of properties to ignore and that can happen only once.\n",
        "# The produced files can be found in the GitHub WikipediaPage_Generator repo.\n",
        "# I needed to extract which properties can happen multiple times for a given subject and which can't, as annotated manually in a spreadsheet.\n",
        "# Guidelines for annotating a property are \"is it confusing if ever a text contains two values for that property\". E.g. \"X was born in Finland and Britain\" is weird, but \"X is the sister of Y and Z\" is not.\n",
        "# It doesn't necessarily have to do with factual truth, e.g. it is almost certain that a ID for an entity in a database is unique, but it's not shocking to say \"X has IDs A and B in database W\".\n",
        "# On the other hand, some cases with multiple object values are annotated as having a single value to avoid poorly entered DBpedia values: e.g. locations (frequently, people put as 3 values a town, the state, the country), subclasses/types (one value is more natural in a text), etc.\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "import json\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "# CSV file in C:\\Users\\sfmil\\Desktop\\ADAPT-2025-2026\\MyPapers\\2025-06_INLG-longText-D2T\n",
        "df = pd.read_csv('/content/dbp_props_examples_annotated.csv')\n",
        "\n",
        "# Make a new dataframe with the first 1000 rows of df\n",
        "# df_1000 = df.head(1000)\n",
        "\n",
        "list_props_that_can_happen_once_only = []\n",
        "list_props_that_can_happen_more_than_once = []\n",
        "list_props_to_filter = []\n",
        "# Iterate over the rows of the dataframe\n",
        "current_property = None\n",
        "for index, df_row in df.iterrows():\n",
        "  if df_row['Property'] != current_property:\n",
        "    current_property = df_row['Property']\n",
        "    current_num_possible_values = df_row['num-possible\\nvalues']\n",
        "    current_id = df_row['id']\n",
        "    # print(f\"Index: {index} - Subject: {df_row['Subject']} - Property: {df_row['Property']} - Object: {df_row['Object']}\")\n",
        "    if current_num_possible_values == '1':\n",
        "      list_props_that_can_happen_once_only.append(current_property)\n",
        "      # print(f'ID: {str(current_id).zfill(4)} - Status: {current_num_possible_values} {current_property}')\n",
        "    elif current_num_possible_values == 'Multiple':\n",
        "      list_props_that_can_happen_more_than_once.append(current_property)\n",
        "      # print(f'ID: {str(current_id).zfill(4)} - Status: M {current_property}')\n",
        "    # else is \"?\" or N/A\n",
        "    elif current_num_possible_values == '?' or current_num_possible_values == 'IgnoreProp':\n",
        "      list_props_to_filter.append(current_property)\n",
        "      # print(f'ID: {str(current_id).zfill(4)} - Status: - {current_property}')\n",
        "    else:\n",
        "      print('ERROR! Unexpected value')\n",
        "      break\n",
        "\n",
        "print(sorted(list_props_that_can_happen_once_only))\n",
        "print(len(list_props_that_can_happen_once_only))\n",
        "print()\n",
        "print(sorted(list_props_that_can_happen_more_than_once))\n",
        "print(len(list_props_that_can_happen_more_than_once))\n",
        "print()\n",
        "print(sorted(list_props_to_filter))\n",
        "print(len(list_props_to_filter))\n",
        "\n",
        "print(f'There are {len(list_props_that_can_happen_once_only)+len(list_props_that_can_happen_more_than_once)+len(list_props_to_filter)} properties collected (expected: 1208).')\n",
        "\n",
        "# Save lists as json\n",
        "with open(\"list_props_that_can_happen_once_only.json\", \"w\") as outfile:\n",
        "    json.dump(sorted(list_props_that_can_happen_once_only), outfile)\n",
        "# with open(\"list_props_that_can_happen_more_than_once.json\", \"w\") as outfile:\n",
        "#     json.dump(list_props_that_can_happen_more_than_once, outfile)\n",
        "with open(\"list_props_to_filter.json\", \"w\") as outfile:\n",
        "    json.dump(sorted(list_props_to_filter), outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "STMOXxbNJaV2"
      },
      "outputs": [],
      "source": [
        "#@title SPARQL query for finding all the possible values for gold:hypernym on dbpedia\n",
        "# Could use dbo:type or rdf:type, but both look a bit messy at first sight\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "def get_types():\n",
        "  # Define the SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query\n",
        "  query = \"\"\"\n",
        "  SELECT DISTINCT ?type\n",
        "  WHERE {\n",
        "    ?s gold:hypernym ?type .\n",
        "  }\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query and the return format\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  # Execute the query and retrieve results\n",
        "  # Returns 10k results, and running it several times in a row always returns the same 10k results\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  return results[\"results\"][\"bindings\"]\n",
        "\n",
        "def count_entities_of_type(hypernym_type):\n",
        "  # Define the DBpedia SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query\n",
        "  query = f\"\"\"\n",
        "  SELECT (COUNT(?s) AS ?count)\n",
        "  WHERE {{\n",
        "      ?s gold:hypernym <{hypernym_type}> .\n",
        "  }}\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query and the return format\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  # Execute the query and retrieve results\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  # Extract and return the count\n",
        "  count = results[\"results\"][\"bindings\"][0][\"count\"][\"value\"]\n",
        "  return int(count)\n",
        "\n",
        "dico_hypernym_types = {}\n",
        "# Extract and print the types\n",
        "results_types = get_types()\n",
        "for i, result in enumerate(results_types):\n",
        "  hypernym_url = result['type']['value']\n",
        "  if hypernym_url not in dico_hypernym_types.keys():\n",
        "    count_occurrences = count_entities_of_type(hypernym_url)\n",
        "    dico_hypernym_types[hypernym_url] = count_occurrences\n",
        "    print(f'{str(i)}/{str(len(results_types))}: {hypernym_url} = {count_occurrences}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HrXI2slUTtnQ"
      },
      "outputs": [],
      "source": [
        "#@title Save dico_hypernym_types to a json and download\n",
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "with open(\"dico_hypernym_types.json\", \"w\") as outfile:\n",
        "  sorted_dico_hypernym_types = {k: v for k, v in sorted(dico_hypernym_types.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "  json.dump(sorted_dico_hypernym_types, outfile)\n",
        "\n",
        "# Download\n",
        "files.download('dico_hypernym_types.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "svm7JHYlGOyX"
      },
      "outputs": [],
      "source": [
        "#@title Make a list of random entities to query for each hypernym (SPARQL query for getting n entities that have a specific hypernym)\n",
        "import random\n",
        "import json\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "def get_random_entities(hypernym: str, limit):\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "  query = f\"\"\"\n",
        "  SELECT DISTINCT ?entity WHERE {{\n",
        "      ?entity gold:hypernym <{hypernym}> .\n",
        "  }} LIMIT 50000\n",
        "  \"\"\"\n",
        "\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  entities = [result['entity']['value'] for result in results['results']['bindings']]\n",
        "  return random.sample(entities, min(len(entities), limit))\n",
        "\n",
        "# Load json that contains hypernyms as keys and count of instances of that hypernym on DBpedia as value\n",
        "dico_hypernym_types = None\n",
        "with open('/content/dico_hypernym_types_incomplete.json', 'r') as file:\n",
        "    dico_hypernym_types = json.load(file)\n",
        "\n",
        "# Get up to 1000 random entities for the classes that have at least 100 members\n",
        "dico_hypernym_sample_entities = {}\n",
        "for i, hypernym in enumerate(dico_hypernym_types.keys()):\n",
        "  if i < 5:\n",
        "    if dico_hypernym_types[hypernym] >= 100:\n",
        "      print(hypernym, dico_hypernym_types[hypernym])\n",
        "      dico_hypernym_sample_entities[hypernym] = get_random_entities(hypernym, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "yngHyKmiEbQh"
      },
      "outputs": [],
      "source": [
        "#@title Save dico_hypernym_sample_entities as JSON and download\n",
        "from google.colab import files\n",
        "with open(\"dico_hypernym_sample_entities.json\", \"w\") as outfile:\n",
        "    json.dump(dico_hypernym_sample_entities, outfile)\n",
        "# Dowload json\n",
        "files.download('dico_hypernym_sample_entities.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aNtg43_AJCE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EWzVztW9vaZ"
      },
      "source": [
        "# Build dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxgI7FyZCzog"
      },
      "source": [
        "## Get properties for list of entities\n",
        "Creates dico_input_contents_DBp.pickle file.\n",
        "Skip if you want to use an already generated pickle file.\n",
        "\n",
        "Triple validation takes a lot of time currently, so only activate here if you don't have a large amount of triples to check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "SSw0hkloXnV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efc1c120-73c2-46ca-c2ba-3a46fb97edcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "geography\n",
            "  Addis_Ababa: found 625 properties.\n",
            "  Adriatic_Sea: found 281 properties.\n",
            "  Afghanistan: found 2968 properties.\n",
            "  Algeria: found 4488 properties.\n",
            "  Amur: found 31 properties.\n",
            "  Angola: found 1543 properties.\n",
            "  Apennine_Mountains: found 135 properties.\n",
            "  Arabian_Sea: found 242 properties.\n",
            "  Arctic_Circle: found 2 properties.\n",
            "  Armenia: found 2832 properties.\n",
            "  Athens: found 3113 properties.\n",
            "  Atlanta: found 4106 properties.\n",
            "  Atlantic_Ocean: found 1458 properties.\n",
            "  Atlas: found 15 properties.\n",
            "  Atlas_Mountains: found 24 properties.\n",
            "  Aïr_Mountains: found 11 properties.\n",
            "  Baffin_Bay: found 56 properties.\n",
            "  Balkans: found 149 properties.\n",
            "  Baltic_Sea: found 353 properties.\n",
            "  Bay_of_Biscay: found 67 properties.\n",
            "  Beaufort_Sea: found 36 properties.\n",
            "  Belgrade: found 4217 properties.\n",
            "  Berlin: found 6270 properties.\n",
            "  Bermuda: found 538 properties.\n",
            "  Black_Sea: found 199 properties.\n",
            "  Bogotá: found 1812 properties.\n",
            "  Borneo: found 116 properties.\n",
            "  Bosnia_and_Herzegovina: found 6344 properties.\n",
            "  Brazil: found 9713 properties.\n",
            "  Brunei: found 988 properties.\n",
            "  Budapest: found 5404 properties.\n",
            "  Bulgaria: found 7202 properties.\n",
            "  Burkina_Faso: found 3344 properties.\n",
            "  Cape_Town: found 2139 properties.\n",
            "  Cape_Verde: found 487 properties.\n",
            "  Caspian_Sea: found 83 properties.\n",
            "  Castries: found 100 properties.\n",
            "  Central_African_Republic: found 820 properties.\n",
            "  Chad: found 871 properties.\n",
            "  Chukchi_Sea: found 25 properties.\n",
            "  City_of_Brussels: found 289 properties.\n",
            "  Colorado_River: found 97 properties.\n",
            "  Columbia_Plateau: found 6 properties.\n",
            "  Comoros: found 313 properties.\n",
            "  Country: found 4 properties.\n",
            "  Czech_Republic: found 9435 properties.\n",
            "  Denver: found 2302 properties.\n",
            "  Dependent_territory: found 10 properties.\n",
            "  Detroit: found 4645 properties.\n",
            "  Djibouti: found 316 properties.\n",
            "  Dominica: found 269 properties.\n",
            "  Drakensberg: found 26 properties.\n",
            "  Dresden: found 888 properties.\n",
            "  Easter_Island: found 50 properties.\n",
            "  El_Salvador: found 1344 properties.\n",
            "  Equator: found 1 properties.\n",
            "  Ethiopia: found 2230 properties.\n",
            "  Ethiopian_Highlands: found 6 properties.\n",
            "  Europe: found 811 properties.\n",
            "  Gabon: found 561 properties.\n",
            "  Genoa: found 1476 properties.\n",
            "  Geodetic_datum: found 1 properties.\n",
            "  Geoid: found 1 properties.\n",
            "  Georgetown,_Guyana: found 464 properties.\n",
            "  Godavari_River: found 46 properties.\n",
            "  Great_Lakes: found 60 properties.\n",
            "  Great_Smoky_Mountains_National_Park: found 24 properties.\n",
            "  Greenland_Sea: found 93 properties.\n",
            "  Grenada: found 357 properties.\n",
            "  Gulf_of_Alaska: found 18 properties.\n",
            "  Guyana: found 739 properties.\n",
            "  Hanover: found 794 properties.\n",
            "  Honolulu: found 1703 properties.\n",
            "  Hungary: found 8689 properties.\n",
            "  Iberian_Peninsula: found 79 properties.\n",
            "  Iraq: found 2882 properties.\n",
            "  Ireland: found 5597 properties.\n",
            "  Irish_Sea: found 106 properties.\n",
            "  Java_Sea: found 79 properties.\n",
            "  Jaú_National_Park: found 3 properties.\n",
            "  K2: found 32 properties.\n",
            "  Kalahari_Desert: found 4 properties.\n",
            "  Kathmandu: found 880 properties.\n",
            "  Kaziranga_National_Park: found 5 properties.\n",
            "  Khartoum: found 320 properties.\n",
            "  Kingston,_Jamaica: found 1258 properties.\n",
            "  Kolyma_(river): found 30 properties.\n",
            "  Kosovo: found 1299 properties.\n",
            "  Kraków: found 1569 properties.\n",
            "  Krishna_River: found 35 properties.\n",
            "  Kuwait: found 905 properties.\n",
            "  La_Paz: found 465 properties.\n",
            "  Labrador_Sea: found 51 properties.\n",
            "  Lahore: found 2306 properties.\n",
            "  Lake_Balkhash: found 16 properties.\n",
            "  Lake_Maracaibo: found 22 properties.\n",
            "  Lake_Superior: found 204 properties.\n",
            "  Lake_Van: found 27 properties.\n",
            "  Lake_Winnipeg: found 11 properties.\n",
            "  Laos: found 819 properties.\n",
            "  Laptev_Sea: found 47 properties.\n",
            "  Liechtenstein: found 615 properties.\n",
            "  Ligurian_Sea: found 31 properties.\n",
            "  Ljubljana: found 1331 properties.\n",
            "  Los_Angeles: found 10001 properties.\n",
            "  Lyon: found 1331 properties.\n",
            "  Malabo: found 121 properties.\n",
            "  Malay_Peninsula: found 44 properties.\n",
            "  Managua: found 278 properties.\n",
            "  Map_projection: found 1 properties.\n",
            "  Mariana_Trench: found 3 properties.\n",
            "  Maseru: found 95 properties.\n",
            "  Mauna_Loa: found 2 properties.\n",
            "  Mauritania: found 609 properties.\n",
            "  Mauritius: found 657 properties.\n",
            "  Minsk: found 1469 properties.\n",
            "  Mogadishu: found 378 properties.\n",
            "  Molucca_Sea: found 10 properties.\n",
            "  Mongolian_Plateau: found 3 properties.\n",
            "  Mount_Elbrus: found 4 properties.\n",
            "  Mount_Kenya: found 9 properties.\n",
            "  Mount_Kilimanjaro: found 22 properties.\n",
            "  Nagoya: found 513 properties.\n",
            "  New_Orleans: found 3223 properties.\n",
            "  Nigeria: found 7607 properties.\n",
            "  Nile: found 37 properties.\n",
            "  North_China_Plain: found 8 properties.\n",
            "  North_Korea: found 1254 properties.\n",
            "  Oceania: found 231 properties.\n",
            "  Oman: found 746 properties.\n",
            "  Osaka: found 1226 properties.\n",
            "  Pacific_Coast_Ranges: found 59 properties.\n",
            "  Paraná_River: found 68 properties.\n",
            "  Paris: found 9986 properties.\n",
            "  Porto-Novo: found 90 properties.\n",
            "  Portugal: found 5107 properties.\n",
            "  Poyang_Lake: found 18 properties.\n",
            "  Pretoria: found 1184 properties.\n",
            "  Qatar: found 1228 properties.\n",
            "  Quebec: found 9930 properties.\n",
            "  Rabat: found 427 properties.\n",
            "  Ramallah: found 140 properties.\n",
            "  Rio_Grande: found 88 properties.\n",
            "  Rio_Negro_(Amazon): found 28 properties.\n",
            "  Rio_de_Janeiro: found 4613 properties.\n",
            "  Riyadh: found 922 properties.\n",
            "  Rome: found 8638 properties.\n",
            "  Roseau: found 79 properties.\n",
            "  Sahara: found 17 properties.\n",
            "  Saint_Vincent_and_the_Grenadines: found 221 properties.\n",
            "  Sakhalin: found 29 properties.\n",
            "  Salonga_National_Park: found 3 properties.\n",
            "  San_Francisco: found 6236 properties.\n",
            "  San_Juan,_Puerto_Rico: found 1390 properties.\n",
            "  San_Salvador: found 487 properties.\n",
            "  Sarajevo: found 1434 properties.\n",
            "  Sea_of_Azov: found 35 properties.\n",
            "  Sea_of_Okhotsk: found 85 properties.\n",
            "  Seoul: found 4576 properties.\n",
            "  Siberia: found 95 properties.\n",
            "  Slovakia: found 5033 properties.\n",
            "  South_China_Sea: found 284 properties.\n",
            "  South_Pole: found 9 properties.\n",
            "  Sudan: found 1130 properties.\n",
            "  Sumatra: found 210 properties.\n",
            "  Switzerland: found 9885 properties.\n",
            "  Sydney: found 6448 properties.\n",
            "  The_Gambia: found 502 properties.\n",
            "  Thessaloniki: found 881 properties.\n",
            "  Torres_Strait: found 19 properties.\n",
            "  Toulouse: found 743 properties.\n",
            "  Transantarctic_Mountains: found 16 properties.\n",
            "  Tropic_of_Capricorn: found 1 properties.\n",
            "  Turan_Depression: found 1 properties.\n",
            "  Ural_Mountains: found 25 properties.\n",
            "  Uruguay_River: found 62 properties.\n",
            "  Valletta: found 309 properties.\n",
            "  Vancouver: found 3655 properties.\n",
            "  Vancouver_Island: found 323 properties.\n",
            "  Vanuatu: found 550 properties.\n",
            "  Venice: found 1216 properties.\n",
            "  Wales: found 5187 properties.\n",
            "  Warsaw: found 4633 properties.\n",
            "  Wuhan: found 569 properties.\n",
            "  Yamuna: found 23 properties.\n",
            "  Yangtze: found 120 properties.\n",
            "  Yellow_River: found 43 properties.\n",
            "  Zanzibar_Archipelago: found 36 properties.\n",
            "history\n",
            "  1556_Shaanxi_earthquake: found 1 properties.\n",
            "  2011_Tōhoku_earthquake_and_tsunami: found 4 properties.\n",
            "  Abolitionism: found 92 properties.\n",
            "  American_Civil_War: found 6544 properties.\n",
            "  Ancient_Rome: found 223 properties.\n",
            "  Ancient_history: found 18 properties.\n",
            "  Angolan_Civil_War: found 158 properties.\n",
            "  Arab_Revolt: found 30 properties.\n",
            "  Archaeogenetics: found 1 properties.\n",
            "  Archaeological_culture: found 1 properties.\n",
            "  Atlantic_slave_trade: found 9 properties.\n",
            "  Atomic_bombings_of_Hiroshima_and_Nagasaki: found 18 properties.\n",
            "  Babylonia: found 37 properties.\n",
            "  Balkan_Wars: found 418 properties.\n",
            "  Battle_of_Britain: found 514 properties.\n",
            "  Beer_Hall_Putsch: found 38 properties.\n",
            "  Black_Death: found 2 properties.\n",
            "  COVID-19_pandemic: found 20 properties.\n",
            "  Caucasian_Albania: found 7 properties.\n",
            "  Chalcolithic: found 7 properties.\n",
            "  Chola_dynasty: found 5 properties.\n",
            "  Cold_War: found 701 properties.\n",
            "  Congress_of_Vienna: found 2 properties.\n",
            "  Counter-Reformation: found 10 properties.\n",
            "  Crimean_Khanate: found 19 properties.\n",
            "  Crimean_War: found 813 properties.\n",
            "  Dacia: found 15 properties.\n",
            "  Decolonization: found 58 properties.\n",
            "  Dissolution_of_the_Soviet_Union: found 17 properties.\n",
            "  Duchy_of_Burgundy: found 21 properties.\n",
            "  Dutch_East_India_Company: found 91 properties.\n",
            "  Early_modern_period: found 8 properties.\n",
            "  Eastern_Bloc: found 9 properties.\n",
            "  East–West_Schism: found 3 properties.\n",
            "  Eighty_Years'_War: found 271 properties.\n",
            "  Enclosure: found 1 properties.\n",
            "  European_integration: found 7 properties.\n",
            "  Fall_of_Constantinople: found 37 properties.\n",
            "  Fertile_Crescent: found 5 properties.\n",
            "  Feudalism: found 165 properties.\n",
            "  First_Boer_War: found 95 properties.\n",
            "  French_Revolution: found 42 properties.\n",
            "  French_Wars_of_Religion: found 59 properties.\n",
            "  French_and_Indian_War: found 274 properties.\n",
            "  French_and_Indian_Wars: found 4 properties.\n",
            "  German_reunification: found 43 properties.\n",
            "  Ghana_Empire: found 6 properties.\n",
            "  Golden_Horde: found 19 properties.\n",
            "  Graf: found 10 properties.\n",
            "  Great_Depression: found 4 properties.\n",
            "  Gulf_War: found 1767 properties.\n",
            "  Hellenistic_period: found 9 properties.\n",
            "  History_of_Armenia: found 3 properties.\n",
            "  History_of_Asia: found 7 properties.\n",
            "  History_of_China: found 61 properties.\n",
            "  History_of_Europe: found 15 properties.\n",
            "  History_of_France: found 11 properties.\n",
            "  History_of_Germany: found 10 properties.\n",
            "  History_of_Iceland: found 28 properties.\n",
            "  History_of_India: found 13 properties.\n",
            "  History_of_Indonesia: found 1 properties.\n",
            "  History_of_Mexico: found 3 properties.\n",
            "  History_of_Paraguay: found 3 properties.\n",
            "  History_of_Scandinavia: found 1 properties.\n",
            "  History_of_Spain: found 4 properties.\n",
            "  History_of_Switzerland: found 2 properties.\n",
            "  History_of_Turkey: found 1 properties.\n",
            "  History_of_Venezuela: found 1 properties.\n",
            "  History_of_Yemen: found 1 properties.\n",
            "  History_of_astronomy: found 13 properties.\n",
            "  History_of_computing_hardware: found 1 properties.\n",
            "  History_of_the_Balkans: found 1 properties.\n",
            "  House_of_Plantagenet: found 1 properties.\n",
            "  Hundred_Years'_War: found 156 properties.\n",
            "  Huns: found 7 properties.\n",
            "  Indonesian_National_Revolution: found 282 properties.\n",
            "  Iran–Iraq_War: found 553 properties.\n",
            "  Iraq_War: found 2595 properties.\n",
            "  Iron_Age: found 1 properties.\n",
            "  Joseon: found 325 properties.\n",
            "  Khmer_Empire: found 6 properties.\n",
            "  Kievan_Rus': found 40 properties.\n",
            "  Kingdom_of_Bohemia: found 378 properties.\n",
            "  Kingdom_of_France: found 1464 properties.\n",
            "  Knight: found 68 properties.\n",
            "  Kushan_Empire: found 9 properties.\n",
            "  Livonian_War: found 54 properties.\n",
            "  Macedonia_(ancient_kingdom): found 33 properties.\n",
            "  Meiji_Restoration: found 4 properties.\n",
            "  Mesopotamia: found 97 properties.\n",
            "  Mexican_Revolution: found 216 properties.\n",
            "  Minoan_civilization: found 3 properties.\n",
            "  Nazi_Germany: found 5356 properties.\n",
            "  Nerva–Antonine_dynasty: found 2 properties.\n",
            "  Ninja: found 3 properties.\n",
            "  North_Yemen_Civil_War: found 58 properties.\n",
            "  Novgorod_Republic: found 10 properties.\n",
            "  Nubia: found 7 properties.\n",
            "  Ottoman_Empire: found 4159 properties.\n",
            "  Papal_States: found 1676 properties.\n",
            "  Paraguayan_War: found 229 properties.\n",
            "  Partition_of_India: found 10 properties.\n",
            "  Pompeii: found 12 properties.\n",
            "  Prague_Spring: found 3 properties.\n",
            "  Prussia: found 539 properties.\n",
            "  Ptolemaic_Kingdom: found 29 properties.\n",
            "  Reconquista: found 163 properties.\n",
            "  Roman_Empire: found 557 properties.\n",
            "  Rus'_people: found 1 properties.\n",
            "  Russo-Japanese_War: found 583 properties.\n",
            "  Saka: found 1 properties.\n",
            "  Scramble_for_Africa: found 25 properties.\n",
            "  Second_Balkan_War: found 336 properties.\n",
            "  Second_Spanish_Republic: found 399 properties.\n",
            "  Seleucid_Empire: found 21 properties.\n",
            "  Silk_Road: found 1 properties.\n",
            "  Sixteen_Kingdoms: found 19 properties.\n",
            "  Sokoto_Caliphate: found 19 properties.\n",
            "  Solidarity_(Polish_trade_union): found 24 properties.\n",
            "  Songhai_Empire: found 6 properties.\n",
            "  Space_Race: found 1 properties.\n",
            "  Spanish_Inquisition: found 2 properties.\n",
            "  Spanish_colonization_of_the_Americas: found 22 properties.\n",
            "  Spanish_conquest_of_the_Aztec_Empire: found 88 properties.\n",
            "  Sparta: found 25 properties.\n",
            "  Sultanate_of_Rum: found 27 properties.\n",
            "  Susa: found 12 properties.\n",
            "  Syrian_Wars: found 29 properties.\n",
            "  The_Holocaust: found 114 properties.\n",
            "  The_Troubles: found 364 properties.\n",
            "  Thirty_Years'_War: found 248 properties.\n",
            "  Timbuktu: found 40 properties.\n",
            "  Tithe: found 1 properties.\n",
            "  Treaty_of_Trianon: found 1 properties.\n",
            "  Tsardom_of_Russia: found 109 properties.\n",
            "  Turkish_War_of_Independence: found 223 properties.\n",
            "  Ugarit: found 5 properties.\n",
            "  Umayyad_Caliphate: found 154 properties.\n",
            "  Upper_Paleolithic: found 1 properties.\n",
            "  Ur: found 5 properties.\n",
            "  Vikings: found 30 properties.\n",
            "  Vladimir-Suzdal: found 11 properties.\n",
            "  Wari_culture: found 2 properties.\n",
            "  Warring_States_period: found 4 properties.\n",
            "  Wars_of_the_Roses: found 50 properties.\n",
            "  Wends: found 1 properties.\n",
            "  Women's_suffrage: found 93 properties.\n",
            "  World_War_II: found 9792 properties.\n",
            "  Wounded_Knee_Massacre: found 40 properties.\n",
            "  Xiongnu: found 8 properties.\n",
            "  Zapotec_civilization: found 3 properties.\n",
            "people\n",
            "  Abel_Tasman: found 11 properties.\n",
            "  Abu_Bakr: found 42 properties.\n",
            "  Adam_Smith: found 72 properties.\n",
            "  Adolf_Hitler: found 229 properties.\n",
            "  Adriano_Celentano: found 81 properties.\n",
            "  Aisha: found 20 properties.\n",
            "  Al-Nawawi: found 2 properties.\n",
            "  Alan_Moore: found 101 properties.\n",
            "  Alan_Parker: found 40 properties.\n",
            "  Albert_Bandura: found 18 properties.\n",
            "  Aldous_Huxley: found 53 properties.\n",
            "  Aleksandr_Solzhenitsyn: found 28 properties.\n",
            "  Aleksey_Nikolayevich_Tolstoy: found 9 properties.\n",
            "  Alessandro_Volta: found 12 properties.\n",
            "  Alexander_Graham_Bell: found 41 properties.\n",
            "  Alexander_Grothendieck: found 31 properties.\n",
            "  Alexander_Pushkin: found 51 properties.\n",
            "  Alexander_the_Great: found 55 properties.\n",
            "  Alexander_von_Humboldt: found 32 properties.\n",
            "  Alfred_Adler: found 21 properties.\n",
            "  Alfred_Nobel: found 4 properties.\n",
            "  Alfredo_Stroessner: found 14 properties.\n",
            "  Alice_Walker: found 26 properties.\n",
            "  Alvin_Ailey: found 13 properties.\n",
            "  Ambrose: found 31 properties.\n",
            "  Amedeo_Avogadro: found 11 properties.\n",
            "  Amitabh_Bachchan: found 222 properties.\n",
            "  Andre_Agassi: found 76 properties.\n",
            "  Andrea_Bocelli: found 52 properties.\n",
            "  Andrea_del_Sarto: found 20 properties.\n",
            "  Andrew_Lloyd_Webber: found 146 properties.\n",
            "  André-Marie_Ampère: found 19 properties.\n",
            "  Ang_Lee: found 42 properties.\n",
            "  Antonio_Gramsci: found 41 properties.\n",
            "  Antonio_Vivaldi: found 25 properties.\n",
            "  António_de_Oliveira_Salazar: found 44 properties.\n",
            "  Anwar_Sadat: found 72 properties.\n",
            "  Apollonius_of_Perga: found 5 properties.\n",
            "  Archytas: found 9 properties.\n",
            "  Aristophanes: found 20 properties.\n",
            "  Aristotle: found 147 properties.\n",
            "  Arminius: found 17 properties.\n",
            "  Arnold_Schoenberg: found 8 properties.\n",
            "  Arthur_C._Clarke: found 116 properties.\n",
            "  Arthur_Miller: found 52 properties.\n",
            "  Arthur_Wellesley,_1st_Duke_of_Wellington: found 167 properties.\n",
            "  Ashoka: found 31 properties.\n",
            "  Athanasius_of_Alexandria: found 15 properties.\n",
            "  Audrey_Hepburn: found 47 properties.\n",
            "  Augustin-Jean_Fresnel: found 9 properties.\n",
            "  Augustine_of_Hippo: found 89 properties.\n",
            "  Aurangzeb: found 64 properties.\n",
            "  Averroes: found 29 properties.\n",
            "  B._F._Skinner: found 26 properties.\n",
            "  B._R._Ambedkar: found 50 properties.\n",
            "  Babe_Didrikson_Zaharias: found 17 properties.\n",
            "  Babur: found 65 properties.\n",
            "  Barack_Obama: found 2019 properties.\n",
            "  Barbara_Stanwyck: found 95 properties.\n",
            "  Basil_II: found 16 properties.\n",
            "  Bede: found 13 properties.\n",
            "  Bertrand_Russell: found 101 properties.\n",
            "  Bill_Gates: found 37 properties.\n",
            "  Black_Sabbath: found 113 properties.\n",
            "  Boethius: found 12 properties.\n",
            "  Bonaventura_Cavalieri: found 15 properties.\n",
            "  Boris_Pasternak: found 11 properties.\n",
            "  Brigitte_Bardot: found 59 properties.\n",
            "  Brontë_family: found 7 properties.\n",
            "  Báb: found 12 properties.\n",
            "  Béla_Bartók: found 7 properties.\n",
            "  C._V._Raman: found 34 properties.\n",
            "  Camillo_Benso,_Count_of_Cavour: found 17 properties.\n",
            "  Caravaggio: found 91 properties.\n",
            "  Carl_Friedrich_Gauss: found 43 properties.\n",
            "  Carl_von_Clausewitz: found 16 properties.\n",
            "  Carlos_Saura: found 83 properties.\n",
            "  Carlos_Slim: found 29 properties.\n",
            "  Cary_Grant: found 85 properties.\n",
            "  Casimir_Funk: found 10 properties.\n",
            "  Caspar_David_Friedrich: found 33 properties.\n",
            "  Catherine_de'_Medici: found 25 properties.\n",
            "  Celine_Dion: found 247 properties.\n",
            "  Charles_Algernon_Parsons: found 16 properties.\n",
            "  Charles_Hermite: found 35 properties.\n",
            "  Charles_IV,_Holy_Roman_Emperor: found 36 properties.\n",
            "  Charles_V,_Holy_Roman_Emperor: found 51 properties.\n",
            "  Christopher_Wren: found 67 properties.\n",
            "  Chrétien_de_Troyes: found 2 properties.\n",
            "  Chuck_Berry: found 105 properties.\n",
            "  Claudette_Colbert: found 82 properties.\n",
            "  Clement_Attlee: found 220 properties.\n",
            "  Confucius: found 16 properties.\n",
            "  Constantine_the_Great: found 35 properties.\n",
            "  Darryl_F._Zanuck: found 237 properties.\n",
            "  Dashiell_Hammett: found 33 properties.\n",
            "  David_Coverdale: found 37 properties.\n",
            "  David_Hilbert: found 99 properties.\n",
            "  David_McClelland: found 10 properties.\n",
            "  Dean_Martin: found 110 properties.\n",
            "  Dhyan_Chand: found 8 properties.\n",
            "  Diego_Maradona: found 49 properties.\n",
            "  Dietrich_Bonhoeffer: found 18 properties.\n",
            "  Dmitri_Mendeleev: found 7 properties.\n",
            "  Domitian: found 14 properties.\n",
            "  Douglas_MacArthur: found 84 properties.\n",
            "  Du_Fu: found 4 properties.\n",
            "  Dwight_D._Eisenhower: found 535 properties.\n",
            "  Edmund_Burke: found 38 properties.\n",
            "  Edmund_Hillary: found 15 properties.\n",
            "  Edward_I_of_England: found 56 properties.\n",
            "  Edward_R._Murrow: found 31 properties.\n",
            "  Edward_Teller: found 18 properties.\n",
            "  El_Greco: found 110 properties.\n",
            "  Elizabeth_II: found 254 properties.\n",
            "  Elvis_Presley: found 394 properties.\n",
            "  Emma_Goldman: found 22 properties.\n",
            "  Emmy_Noether: found 31 properties.\n",
            "  Emperor_Gaozu_of_Han: found 19 properties.\n",
            "  Emperor_Meiji: found 32 properties.\n",
            "  Epicurus: found 20 properties.\n",
            "  Eric_Clapton: found 203 properties.\n",
            "  Erich_Maria_Remarque: found 24 properties.\n",
            "  Erik_Bruhn: found 13 properties.\n",
            "  Ernest_Hemingway: found 65 properties.\n",
            "  Ernest_Lawrence: found 35 properties.\n",
            "  Ernst_Cassirer: found 25 properties.\n",
            "  Euclid: found 16 properties.\n",
            "  Eudoxus_of_Cnidus: found 1 properties.\n",
            "  Eugene_Wigner: found 18 properties.\n",
            "  Eusébio: found 21 properties.\n",
            "  Ezra_Pound: found 8 properties.\n",
            "  Fausto_Coppi: found 8 properties.\n",
            "  Florence_Nightingale: found 27 properties.\n",
            "  France_Prešeren: found 15 properties.\n",
            "  Francesco_Borromini: found 22 properties.\n",
            "  Francis_Bacon: found 13 properties.\n",
            "  Francis_Ford_Coppola: found 131 properties.\n",
            "  François_Mansart: found 15 properties.\n",
            "  Frederick_Soddy: found 23 properties.\n",
            "  Frei_Otto: found 6 properties.\n",
            "  Friedrich_Engels: found 38 properties.\n",
            "  Friedrich_Schiller: found 31 properties.\n",
            "  Frédéric_Chopin: found 27 properties.\n",
            "  G._H._Hardy: found 59 properties.\n",
            "  Garrincha: found 18 properties.\n",
            "  Gaspard_Monge: found 18 properties.\n",
            "  Georg_Cantor: found 18 properties.\n",
            "  Georg_Wilhelm_Friedrich_Hegel: found 112 properties.\n",
            "  George_Bernard_Shaw: found 81 properties.\n",
            "  George_Boole: found 15 properties.\n",
            "  George_Carlin: found 42 properties.\n",
            "  George_III: found 80 properties.\n",
            "  George_Washington: found 166 properties.\n",
            "  George_Westinghouse: found 14 properties.\n",
            "  Gerd_Müller: found 21 properties.\n",
            "  Ghalib: found 25 properties.\n",
            "  Gilbert_and_Sullivan: found 6 properties.\n",
            "  Gioachino_Rossini: found 15 properties.\n",
            "  Giovanni_Bellini: found 89 properties.\n",
            "  Giovanni_Boccaccio: found 17 properties.\n",
            "  Giuseppe_Verdi: found 26 properties.\n",
            "  Golda_Meir: found 26 properties.\n",
            "  Gordie_Howe: found 14 properties.\n",
            "  Graham_Greene: found 71 properties.\n",
            "  Gregor_Mendel: found 14 properties.\n",
            "  Gregory_of_Nazianzus: found 13 properties.\n",
            "  Gregory_of_Nyssa: found 22 properties.\n",
            "  Greta_Garbo: found 42 properties.\n",
            "  Grigori_Perelman: found 9 properties.\n",
            "  Guru_Gobind_Singh: found 44 properties.\n",
            "  Gustav_Fechner: found 13 properties.\n",
            "  Gustav_Holst: found 10 properties.\n",
            "  Günter_Behnisch: found 7 properties.\n",
            "  Hammurabi: found 7 properties.\n",
            "  Hank_Aaron: found 10 properties.\n",
            "  Hans_Christian_Ørsted: found 14 properties.\n",
            "  Hayao_Miyazaki: found 79 properties.\n",
            "  Hector_Berlioz: found 4 properties.\n",
            "  Helen_Keller: found 15 properties.\n",
            "  Helmut_Kohl: found 116 properties.\n",
            "  Hendrik_Lorentz: found 37 properties.\n",
            "  Henry_Armstrong: found 6 properties.\n",
            "  Henry_Fonda: found 130 properties.\n",
            "  Henry_Miller: found 19 properties.\n",
            "  Henry_the_Fowler: found 27 properties.\n",
            "  Henryk_Sienkiewicz: found 32 properties.\n",
            "  Heraclitus: found 16 properties.\n",
            "  Hermann_Maier: found 18 properties.\n",
            "  Hermann_Weyl: found 31 properties.\n",
            "  Hernán_Cortés: found 38 properties.\n",
            "  Hirohito: found 35 properties.\n",
            "  Hiroshige: found 21 properties.\n",
            "  Horace: found 20 properties.\n",
            "  Horatio_Nelson,_1st_Viscount_Nelson: found 45 properties.\n",
            "  Howard_Hughes: found 65 properties.\n",
            "  Huineng: found 13 properties.\n",
            "  Humayun: found 42 properties.\n",
            "  Ibn_Arabi: found 5 properties.\n",
            "  Idi_Amin: found 74 properties.\n",
            "  Ignatius_of_Loyola: found 64 properties.\n",
            "  Ilf_and_Petrov: found 12 properties.\n",
            "  Ingemar_Johansson: found 9 properties.\n",
            "  Inigo_Jones: found 17 properties.\n",
            "  Irving_Thalberg: found 113 properties.\n",
            "  Isaac_Singer: found 12 properties.\n",
            "  Isadora_Duncan: found 16 properties.\n",
            "  Ismail_Kadare: found 56 properties.\n",
            "  Ivan_III_of_Russia: found 34 properties.\n",
            "  Ivan_Mazepa: found 24 properties.\n",
            "  Ivan_the_Terrible: found 50 properties.\n",
            "  Ivano_Balić: found 6 properties.\n",
            "  J._P._Morgan: found 45 properties.\n",
            "  J._Paul_Getty: found 11 properties.\n",
            "  Jack_Nicholson: found 84 properties.\n",
            "  Jack_Nicklaus: found 32 properties.\n",
            "  Jacobus_Henricus_van_'t_Hoff: found 36 properties.\n",
            "  Jacques_Anquetil: found 9 properties.\n",
            "  James_Clerk_Maxwell: found 17 properties.\n",
            "  James_Cook: found 20 properties.\n",
            "  James_K._Polk: found 101 properties.\n",
            "  James_Madison: found 122 properties.\n",
            "  Jan_Hus: found 1 properties.\n",
            "  Jan_van_Eyck: found 20 properties.\n",
            "  Jean-Luc_Godard: found 191 properties.\n",
            "  Jean-Victor_Poncelet: found 17 properties.\n",
            "  Jean_Marais: found 75 properties.\n",
            "  Jean_Sibelius: found 15 properties.\n",
            "  Jean_le_Rond_d'Alembert: found 3 properties.\n",
            "  Jerry_Goldsmith: found 231 properties.\n",
            "  Jerry_Rice: found 13 properties.\n",
            "  Jesse_Owens: found 14 properties.\n",
            "  Joan_Miró: found 46 properties.\n",
            "  Joan_of_Arc: found 30 properties.\n",
            "  Joe_Cocker: found 76 properties.\n",
            "  Joe_Hisaishi: found 84 properties.\n",
            "  Johan_Cruyff: found 36 properties.\n",
            "  Johannes_Brahms: found 11 properties.\n",
            "  Johannes_Gutenberg: found 15 properties.\n",
            "  Johannes_Rydberg: found 10 properties.\n",
            "  John_B._Watson: found 13 properties.\n",
            "  John_Calvin: found 15 properties.\n",
            "  John_Chrysostom: found 13 properties.\n",
            "  John_D._Rockefeller: found 23 properties.\n",
            "  John_Donne: found 13 properties.\n",
            "  John_III_Sobieski: found 39 properties.\n",
            "  John_Jacob_Astor: found 21 properties.\n",
            "  John_Logie_Baird: found 11 properties.\n",
            "  John_Steinbeck: found 57 properties.\n",
            "  John_Wayne: found 191 properties.\n",
            "  John_the_Baptist: found 146 properties.\n",
            "  Joni_Mitchell: found 128 properties.\n",
            "  Joseph_Conrad: found 59 properties.\n",
            "  Joseph_Fourier: found 17 properties.\n",
            "  Joseph_II,_Holy_Roman_Emperor: found 17 properties.\n",
            "  Josiah_Willard_Gibbs: found 59 properties.\n",
            "  Juan_Antonio_Samaranch: found 16 properties.\n",
            "  Judith_Butler: found 28 properties.\n",
            "  Jules_Verne: found 107 properties.\n",
            "  Julius_Caesar: found 75 properties.\n",
            "  Józef_Piłsudski: found 64 properties.\n",
            "  Jürgen_Habermas: found 51 properties.\n",
            "  Karl_Barth: found 6 properties.\n",
            "  Karl_Jaspers: found 29 properties.\n",
            "  Karl_Popper: found 26 properties.\n",
            "  Keir_Starmer: found 391 properties.\n",
            "  Ken_Kesey: found 24 properties.\n",
            "  Kenzaburō_Ōe: found 18 properties.\n",
            "  Kublai_Khan: found 29 properties.\n",
            "  Kurt_Vonnegut: found 60 properties.\n",
            "  Larisa_Latynina: found 21 properties.\n",
            "  Lata_Mangeshkar: found 55 properties.\n",
            "  Laurence_Olivier: found 95 properties.\n",
            "  Lech_Wałęsa: found 54 properties.\n",
            "  Leland_Stanford: found 23 properties.\n",
            "  Leo_III_the_Isaurian: found 15 properties.\n",
            "  Leon_Festinger: found 8 properties.\n",
            "  Leonardo_da_Vinci: found 69 properties.\n",
            "  Leonid_Brezhnev: found 48 properties.\n",
            "  Leopold_II_of_Belgium: found 22 properties.\n",
            "  Lev_Landau: found 84 properties.\n",
            "  Li_Bai: found 12 properties.\n",
            "  Li_Si: found 2 properties.\n",
            "  Lilia_Podkopayeva: found 9 properties.\n",
            "  Livy: found 1 properties.\n",
            "  Lope_de_Vega: found 16 properties.\n",
            "  Lorenzo_de'_Medici: found 28 properties.\n",
            "  Lothair_I: found 18 properties.\n",
            "  Louis_Armstrong: found 92 properties.\n",
            "  Louis_de_Funès: found 88 properties.\n",
            "  Luciano_Pavarotti: found 21 properties.\n",
            "  Ludwig_Mies_van_der_Rohe: found 29 properties.\n",
            "  Ludwig_van_Beethoven: found 47 properties.\n",
            "  Luigi_Pirandello: found 40 properties.\n",
            "  Luís_de_Camões: found 14 properties.\n",
            "  Lyndon_B._Johnson: found 499 properties.\n",
            "  Léon_Foucault: found 21 properties.\n",
            "  Marcello_Mastroianni: found 131 properties.\n",
            "  Maria_Montessori: found 17 properties.\n",
            "  Mario_Puzo: found 65 properties.\n",
            "  Marius_Petipa: found 14 properties.\n",
            "  Mark_Zuckerberg: found 18 properties.\n",
            "  Martin_Scorsese: found 142 properties.\n",
            "  Mary_Baker_Eddy: found 22 properties.\n",
            "  Masaccio: found 20 properties.\n",
            "  Matsuo_Bashō: found 15 properties.\n",
            "  Maudgalyayana: found 4 properties.\n",
            "  Maurice_Béjart: found 10 properties.\n",
            "  Max_Weber: found 69 properties.\n",
            "  Maxim_Gorky: found 54 properties.\n",
            "  Maximilian_Schell: found 77 properties.\n",
            "  Mehmed_IV: found 34 properties.\n",
            "  Metallica: found 127 properties.\n",
            "  Meyer_Guggenheim: found 18 properties.\n",
            "  Michel_Fokine: found 16 properties.\n",
            "  Michel_Foucault: found 87 properties.\n",
            "  Michel_de_Montaigne: found 95 properties.\n",
            "  Michelangelo_Antonioni: found 59 properties.\n",
            "  Michelle_Kwan: found 9 properties.\n",
            "  Mihai_Eminescu: found 25 properties.\n",
            "  Mikhail_Bakhtin: found 30 properties.\n",
            "  Miklós_Rózsa: found 108 properties.\n",
            "  Miloš_Forman: found 37 properties.\n",
            "  Mily_Balakirev: found 8 properties.\n",
            "  Modest_Mussorgsky: found 8 properties.\n",
            "  Mohammad_Reza_Pahlavi: found 51 properties.\n",
            "  Mohammed_bin_Salman: found 46 properties.\n",
            "  Montesquieu: found 31 properties.\n",
            "  Muhammad_Ali_Jinnah: found 50 properties.\n",
            "  Muhammad_al-Bukhari: found 12 properties.\n",
            "  Nelson_Mandela: found 171 properties.\n",
            "  Nestorius: found 8 properties.\n",
            "  Nicolae_Ceaușescu: found 55 properties.\n",
            "  Nikita_Mikhalkov: found 87 properties.\n",
            "  Niklas_Luhmann: found 19 properties.\n",
            "  Nikola_Tesla: found 14 properties.\n",
            "  Nikolai_Lobachevsky: found 11 properties.\n",
            "  Nikolai_Rimsky-Korsakov: found 13 properties.\n",
            "  Nikolay_Nekrasov: found 18 properties.\n",
            "  Nikos_Kazantzakis: found 19 properties.\n",
            "  Nizami_Ganjavi: found 11 properties.\n",
            "  Nolan_Bushnell: found 22 properties.\n",
            "  Oda_Nobunaga: found 71 properties.\n",
            "  Olga_Korbut: found 11 properties.\n",
            "  Oprah_Winfrey: found 81 properties.\n",
            "  Osama_bin_Laden: found 38 properties.\n",
            "  Osborne_Reynolds: found 16 properties.\n",
            "  Oscar_Niemeyer: found 47 properties.\n",
            "  Paul_Dirac: found 35 properties.\n",
            "  Paul_Gauguin: found 50 properties.\n",
            "  Paul_Klee: found 34 properties.\n",
            "  Pearl_S._Buck: found 26 properties.\n",
            "  Pedro_Álvares_Cabral: found 15 properties.\n",
            "  Peter_Debye: found 21 properties.\n",
            "  Peter_Kropotkin: found 29 properties.\n",
            "  Philip_K._Dick: found 120 properties.\n",
            "  Pierre_Curie: found 25 properties.\n",
            "  Pierre_de_Coubertin: found 15 properties.\n",
            "  Pink_Floyd: found 262 properties.\n",
            "  Pitirim_Sorokin: found 12 properties.\n",
            "  Pliny_the_Elder: found 16 properties.\n",
            "  Pol_Pot: found 37 properties.\n",
            "  Pope_Gregory_I: found 26 properties.\n",
            "  Pope_Innocent_III: found 10 properties.\n",
            "  Pope_Julius_II: found 18 properties.\n",
            "  Pope_Leo_I: found 18 properties.\n",
            "  Pyotr_Ilyich_Tchaikovsky: found 28 properties.\n",
            "  Qin_Shi_Huang: found 13 properties.\n",
            "  Queen_Victoria: found 68 properties.\n",
            "  Quentin_Tarantino: found 84 properties.\n",
            "  Rafael_Nadal: found 121 properties.\n",
            "  Raj_Kapoor: found 110 properties.\n",
            "  Rashi: found 18 properties.\n",
            "  Ravi_Shankar: found 57 properties.\n",
            "  Ray_Bradbury: found 118 properties.\n",
            "  Richard_Branson: found 42 properties.\n",
            "  Richard_Feynman: found 44 properties.\n",
            "  Richard_Neutra: found 23 properties.\n",
            "  Richard_Nixon: found 659 properties.\n",
            "  Richard_Wagner: found 32 properties.\n",
            "  Robert_Burns_Woodward: found 46 properties.\n",
            "  Robert_Frost: found 18 properties.\n",
            "  Robert_Fulton: found 23 properties.\n",
            "  Robert_Hooke: found 30 properties.\n",
            "  Robert_Mugabe: found 212 properties.\n",
            "  Robert_Walpole: found 54 properties.\n",
            "  Robin_Williams: found 77 properties.\n",
            "  Ronnie_James_Dio: found 37 properties.\n",
            "  Rosa_Luxemburg: found 44 properties.\n",
            "  Rosa_Parks: found 18 properties.\n",
            "  Roy_J._Plunkett: found 8 properties.\n",
            "  Rudaki: found 5 properties.\n",
            "  Sachin_Tendulkar: found 12 properties.\n",
            "  Salvador_Dalí: found 86 properties.\n",
            "  Samuel_Johnson: found 18 properties.\n",
            "  Sandro_Botticelli: found 61 properties.\n",
            "  Sebastian_Coe: found 24 properties.\n",
            "  Seki_Takakazu: found 7 properties.\n",
            "  Seleucus_I_Nicator: found 28 properties.\n",
            "  Sergei_Eisenstein: found 40 properties.\n",
            "  Seymour_Cray: found 14 properties.\n",
            "  Shah_Jahan: found 37 properties.\n",
            "  Shaka: found 12 properties.\n",
            "  Shirley_Temple: found 60 properties.\n",
            "  Sigismund,_Holy_Roman_Emperor: found 33 properties.\n",
            "  Simón_Bolívar: found 77 properties.\n",
            "  Sinclair_Lewis: found 25 properties.\n",
            "  Slobodan_Milošević: found 46 properties.\n",
            "  Stendhal: found 23 properties.\n",
            "  Steven_Chu: found 23 properties.\n",
            "  Stevie_Ray_Vaughan: found 45 properties.\n",
            "  Stevie_Wonder: found 342 properties.\n",
            "  Sulla: found 30 properties.\n",
            "  Suzanne_Lenglen: found 39 properties.\n",
            "  Sándor_Petőfi: found 19 properties.\n",
            "  Søren_Kierkegaard: found 56 properties.\n",
            "  T._S._Eliot: found 39 properties.\n",
            "  Tacitus: found 5 properties.\n",
            "  Talcott_Parsons: found 8 properties.\n",
            "  Tara_Lipinski: found 9 properties.\n",
            "  Tenzing_Norgay: found 5 properties.\n",
            "  The_Rolling_Stones: found 308 properties.\n",
            "  Thomas_Cranmer: found 12 properties.\n",
            "  Thomas_Hardy: found 29 properties.\n",
            "  Tiger_Woods: found 61 properties.\n",
            "  Titian: found 157 properties.\n",
            "  Tokugawa_Ieyasu: found 73 properties.\n",
            "  Tom_Brokaw: found 13 properties.\n",
            "  Tom_Hanks: found 152 properties.\n",
            "  Tristan_Tzara: found 25 properties.\n",
            "  Ty_Cobb: found 9 properties.\n",
            "  Umm_Kulthum: found 34 properties.\n",
            "  Ursula_von_der_Leyen: found 100 properties.\n",
            "  Vangelis: found 75 properties.\n",
            "  Vasco_da_Gama: found 34 properties.\n",
            "  Vasil_Levski: found 17 properties.\n",
            "  Vincent_van_Gogh: found 164 properties.\n",
            "  Vitaly_Scherbo: found 14 properties.\n",
            "  Vivien_Leigh: found 40 properties.\n",
            "  Vladimir_Vysotsky: found 34 properties.\n",
            "  Wallace_Carothers: found 11 properties.\n",
            "  Wang_Yangming: found 11 properties.\n",
            "  Warren_Buffett: found 32 properties.\n",
            "  Washington_Irving: found 36 properties.\n",
            "  Wilhelm_Röntgen: found 32 properties.\n",
            "  William_Blake: found 23 properties.\n",
            "  William_Golding: found 11 properties.\n",
            "  William_Harvey: found 15 properties.\n",
            "  William_Henry_Bragg: found 8 properties.\n",
            "  William_Herschel: found 17 properties.\n",
            "  William_I,_German_Emperor: found 24 properties.\n",
            "  William_Vickrey: found 14 properties.\n",
            "  William_the_Silent: found 52 properties.\n",
            "  Wilt_Chamberlain: found 41 properties.\n",
            "  Winston_Churchill: found 373 properties.\n",
            "  Wole_Soyinka: found 36 properties.\n",
            "  Wolfgang_Amadeus_Mozart: found 33 properties.\n",
            "  Woodrow_Wilson: found 255 properties.\n",
            "  Woody_Allen: found 192 properties.\n",
            "  Zheng_He: found 4 properties.\n",
            "  Zinedine_Zidane: found 36 properties.\n",
            "  Élie_Cartan: found 35 properties.\n"
          ]
        }
      ],
      "source": [
        "# @title Get DBpedia properties online for an entity list (about 1h for GREC entities)\n",
        "import os\n",
        "import codecs\n",
        "import json\n",
        "import re\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout\n",
        "from WikipediaPage_Generator.code.queryDBpediaProps import get_dbpedia_properties\n",
        "from WikipediaPage_Generator.code.utils import removeReservedCharsFileName\n",
        "\n",
        "# Input json should be a dico_1 with category names (urls or name) as keys, and a list of entities (urls or names) as value.\n",
        "input_json_path = '/content/random_output.json'#@param{type:\"string\"}\n",
        "# triple-source should be Ontology for this experiment\n",
        "triple_source = 'Ontology' #@param['Infobox', 'Ontology', 'Wikidata']\n",
        "# (Sub)set of properties to consider (about 400 in WebNLG, about 1.2K in DBpedia all)\n",
        "consider_properties = 'DBpedia_all' #@param['DBpedia_WebNLG20', 'DBpedia_all']\n",
        "# Store here \"dirty\" properties; string expected, it is later on split by ','\n",
        "# ignore_properties = 'width,title' # Used for semantic accuracy experiments with Rudali\n",
        "ignore_properties = ','.join(json.loads(open('/content/WikipediaPage_Generator/resources/list_props_to_filter.json', 'r').read()))\n",
        "get_triples_where_entity_is_subj = True #@param {type:\"boolean\"}\n",
        "get_triples_where_entity_is_obj = True #@param {type:\"boolean\"}\n",
        "triple_validation = False #@param {type:\"boolean\"}\n",
        "props_list_path = ''\n",
        "if consider_properties == 'DBpedia_WebNLG20':\n",
        "  props_list_path = os.path.join('/content', 'DCU_TCD_FORGe_WebNLG23', 'code', 'sorted_properties.txt')\n",
        "elif consider_properties == 'DBpedia_all':\n",
        "  props_list_path = os.path.join('/content', 'WikipediaPage_Generator', 'resources', 'list_DBpedia_props_seenOver10times.txt')\n",
        "\n",
        "# Load json dico with sample entities for each hypernym\n",
        "dico_hypernym_sample_entities_loaded = None\n",
        "with open(input_json_path, 'r') as file:\n",
        "    dico_hypernym_sample_entities_loaded = json.load(file)\n",
        "\n",
        "# dico_input_contents will contain category keys, which contain entity keys, which contain a list of triple objects\n",
        "dico_input_contents = {}\n",
        "for hypernym in sorted(dico_hypernym_sample_entities_loaded.keys()):\n",
        "  input_category = None\n",
        "  if re.search('/', hypernym):\n",
        "    input_category = hypernym.rsplit('/', 1)[1]\n",
        "  else:\n",
        "    input_category = hypernym\n",
        "  print(input_category)\n",
        "  dico_input_contents[input_category] = {}\n",
        "  # Format properties for passing as argument to python module\n",
        "  # list_triple_object contains object with 3 attributes: DBsubj, DBprop, DBobj\n",
        "  # list_propObj is used for UI (for triples selection by the user)\n",
        "  # list_obj is used for getting class and gender info later on\n",
        "  ##############################################################################\n",
        "  # WARNING-TODO: I forgot to apply removeReservedCharsFileName to entity_names and replace spaces by underscores\n",
        "  ##############################################################################\n",
        "  for sampled_entity in sorted(dico_hypernym_sample_entities_loaded[hypernym]):\n",
        "    entity_name = None\n",
        "    if re.search('/', sampled_entity):\n",
        "      entity_name = sampled_entity.rsplit('/', 1)[1]\n",
        "    else:\n",
        "      entity_name = '_'.join(sampled_entity.split(' '))\n",
        "\n",
        "    # Get all triples in which the entity is the subject\n",
        "    list_triple_objects, list_propObj, list_obj = get_dbpedia_properties(props_list_path, entity_name, triple_source, ignore_properties, get_triples_where_entity_is_subj, get_triples_where_entity_is_obj, triple_validation)\n",
        "\n",
        "    if len(list_triple_objects) > 0:\n",
        "      print(f'  {entity_name}: found {len(list_triple_objects)} properties.')\n",
        "      dico_input_contents[input_category][entity_name] = list_triple_objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8bEVe9qef3SM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "collapsed": true,
        "outputId": "6d017ac0-ed43-4d75-8198-e065ce17a396"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c8dfdfa3-ad51-4d01-87d2-a1b6fb6fd108\", \"dico_input_contents_DBp.pickle\", 18091310)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Serialise dico_input_contents using pickle and download\n",
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "with open(\"dico_input_contents_DBp.pickle\", \"wb\") as handle:\n",
        "    pickle.dump(dico_input_contents, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Download file\n",
        "files.download('dico_input_contents_DBp.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FynuNJCvC3Du"
      },
      "source": [
        "## Build knowledge graphs #1: WebNLG mirror input configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "ivwEj5vbNXa5"
      },
      "outputs": [],
      "source": [
        "#@title Check which entities have property (sub)sets that match WebNLG inputs. Creates dico_entities_for_triple_configuration.json file.\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "dico_input_contents = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "print_output = False #@param{type:'boolean'}\n",
        "\n",
        "# dico_input_contents_loaded should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have entity_name as key, and a list of Triple Objects as value.\n",
        "# Triple Objects have 3 attributes: DBsubj, DBprop, DBobj\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "# dico_triple_configs_WebNLG should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have strings of properties separated by \"##\" as keys, and integers (occurrence counts in WebNLG train data) as values.\n",
        "dico_triple_configs_WebNLG = None\n",
        "with open(\"/content/Build_KGs_entities/resources/dico_category_tripleConfigs_WebNLG.json\", \"r\") as handle:\n",
        "    dico_triple_configs_WebNLG = json.load(handle)\n",
        "\n",
        "# dico_mapping_categories = {'City':'Cities'}\n",
        "dico_mapping_categories = {'Person':'People', 'City':'Cities'}\n",
        "\n",
        "# Let's extract which entities have the properties that match a WebNLG configuration. The dico will have: { category: { triple_config: [entity1, entity2, etc.] } }\n",
        "dico_entities_for_triple_configuration = {}\n",
        "print('Finding which entities have the properties that match a WebNLG configuration...')\n",
        "for category_label_WebNLG in dico_mapping_categories.keys():\n",
        "  dico_entities_for_triple_configuration[category_label_WebNLG] = {}\n",
        "  for triple_config_WebNLG in dico_triple_configs_WebNLG[category_label_WebNLG].keys():\n",
        "    # Get input configurations (i.e. property sets) extracted from WebNLG\n",
        "    list_properties_WebNLG = triple_config_WebNLG.split('##')\n",
        "    # print(list_properties_WebNLG)\n",
        "    # Get category label used in GREC\n",
        "    category_label_GREC = dico_mapping_categories[category_label_WebNLG]\n",
        "    # For each GREC entity, extract the set of properties found on DBpedia\n",
        "    for entity_name in dico_input_contents_loaded[category_label_GREC].keys():\n",
        "      # Need a list of strings so we can then convert in sets and compare with other set of property labels\n",
        "      list_properties_entity = []\n",
        "      list_triple_objects_entity = dico_input_contents_loaded[category_label_GREC][entity_name]\n",
        "      for triple_object in list_triple_objects_entity:\n",
        "        list_properties_entity.append(triple_object.DBprop)\n",
        "      # Check if any of the WebNLG triple configurations can be built using the properties of each entity\n",
        "      if set(list_properties_WebNLG).issubset(set(list_properties_entity)):\n",
        "        if triple_config_WebNLG not in dico_entities_for_triple_configuration[category_label_WebNLG].keys():\n",
        "          dico_entities_for_triple_configuration[category_label_WebNLG][triple_config_WebNLG] = []\n",
        "        dico_entities_for_triple_configuration[category_label_WebNLG][triple_config_WebNLG].append(entity_name)\n",
        "\n",
        "# Save dico_entities_for_triple_configuration as json\n",
        "with open(\"dico_entities_for_triple_configuration.json\", \"w\") as outfile:\n",
        "    json.dump(dico_entities_for_triple_configuration, outfile)\n",
        "\n",
        "if print_output == True:\n",
        "  for category_label in dico_entities_for_triple_configuration.keys():\n",
        "    print('============')\n",
        "    print(category_label)\n",
        "    print('============')\n",
        "    for triple_config_overlap in dico_entities_for_triple_configuration[category_label].keys():\n",
        "      print('')\n",
        "      print(triple_config_overlap)\n",
        "      print('----------------------------')\n",
        "      for entity_name in dico_entities_for_triple_configuration[category_label][triple_config_overlap]:\n",
        "        print('-', entity_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IWkIclvxbM-i"
      },
      "outputs": [],
      "source": [
        "#@title Save triple sets in XML format: WebNLG size and input config mirroring\n",
        "# Here we're trying to build a new dataset that has the same properties as in WebNLG, and the same property configurations in the outputs.\n",
        "import pickle\n",
        "import json\n",
        "from WikipediaPage_Generator.code.utils import create_xml, clear_folder\n",
        "\n",
        "dico_input_contents = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "dico_input_entities = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "\n",
        "dico_mapping_categories = {'Person':'People', 'City':'Cities'}\n",
        "\n",
        "# The dico has the following form: { category: { triple_config: [entity1, entity2, etc.] } }\n",
        "dico_entities_for_triple_configuration_l = None\n",
        "if dico_input_entities == 'Made_with_this_notebook':\n",
        "  dico_entities_for_triple_configuration_l = json.load(open('/content/dico_entities_for_triple_configuration.json', 'r'))\n",
        "elif dico_input_entities == 'GitHub_GREC_NEs':\n",
        "  dico_entities_for_triple_configuration_l = json.load(open('/content/Build_KGs_entities/resources/dico_entities_for_triple_configuration_GREC_NEs.json', 'r'))\n",
        "\n",
        "# dico_input_contents_loaded should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have entity_name as key, and a list of Triple Objects as value.\n",
        "# Triple Objects have 3 attributes: DBsubj, DBprop, DBobj\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "counter_datapoints = 0\n",
        "# Keep track of how many times an entity is used for an XML, so we can number the inputs corresponding to the same entity (an XML is named after the entity name)\n",
        "entity_counter = {}\n",
        "for category_l in dico_entities_for_triple_configuration_l:\n",
        "  print(category_l)\n",
        "  # Prepare output folder\n",
        "  clear_folder(os.path.join(triple2predArg, category_l))\n",
        "  # if not os.path.exists(os.path.join(triple2predArg, input_category)):\n",
        "  os.makedirs(os.path.join(triple2predArg, category_l))\n",
        "  catregory_grec = dico_mapping_categories[category_l]\n",
        "  for triple_config in dico_entities_for_triple_configuration_l[category_l]:\n",
        "    print('  ', triple_config, len(dico_entities_for_triple_configuration_l[category_l][triple_config]))\n",
        "    property_list_l = triple_config.split('##')\n",
        "    # print(f'{category_l}: {triple_config}: {len(dico_entities_for_triple_configuration_l[category_l][triple_config])}')\n",
        "    for entity_name in dico_entities_for_triple_configuration_l[category_l][triple_config]:\n",
        "      # Make filename by using entity name + number of times that entity is being used\n",
        "      if entity_name not in entity_counter.keys():\n",
        "        entity_counter[entity_name] = 0\n",
        "      else:\n",
        "        entity_counter[entity_name] += 1\n",
        "      filename = entity_name+'_'+str(entity_counter[entity_name])\n",
        "      list_triple_objects = dico_input_contents_loaded[catregory_grec][entity_name]\n",
        "      list_selected_triple_objects = []\n",
        "      for triple_object in list_triple_objects:\n",
        "        found_prop = False\n",
        "        if triple_object.DBprop in property_list_l:\n",
        "          # print(f'      {entity_name}: found {triple_object.DBprop}.')\n",
        "          if found_prop == False:\n",
        "            list_selected_triple_objects.append(triple_object)\n",
        "            found_prop = True\n",
        "      # The function that builds an XML expects a list of list IDs that correspond to selected triples. In this context, we want all triples.\n",
        "      properties_selected = [i for i in range(len(property_list_l))]\n",
        "      # create xml file passing the entity name to use as filename\n",
        "      counter_datapoints += 1\n",
        "      list_triples_text = create_xml(list_selected_triple_objects, properties_selected, category_l, os.path.join(triple2predArg, category_l), entity_name=filename, eid = counter_datapoints)\n",
        "\n",
        "print(f'----------\\n{counter_datapoints} new datapoints were created in total.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhdqevp_7JZO"
      },
      "source": [
        "## Build knowledge graphs #2: WebNLG mirror input size distribution only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mauku5SQDRq7"
      },
      "outputs": [],
      "source": [
        "#@title Pseudo-code\n",
        "\n",
        "# Initialize empty dictionary dico_length_ratio_entity\n",
        "\n",
        "# For each entity in the dataset:\n",
        "#     Initialize two lists:\n",
        "#         subject_triples = []  // Triples where the entity is the subject\n",
        "#         object_triples = []   // Triples where the entity is the object\n",
        "\n",
        "#     For each triple related to the entity:\n",
        "#         If entity is the subject:\n",
        "#             Add triple to subject_triples\n",
        "#         Else if entity is the object:\n",
        "#             Add triple to object_triples\n",
        "\n",
        "#     // Create property-based dictionaries for subjects and objects\n",
        "#     subject_property_dict = Group subject_triples by property\n",
        "#     object_property_dict = Group object_triples by property\n",
        "\n",
        "#     For each desired input length (e.g., 1 to N triples, in our case N=7):\n",
        "#         For each possible subject/object ratio (e.g., 1:2, 2:1, etc.):\n",
        "#             // Ensure at least one triple has the entity as subject (Constraint 2)\n",
        "#             subject_count = number of triples to select as subject\n",
        "#             object_count = input_length - subject_count\n",
        "\n",
        "#             possible_subject_triples = []\n",
        "#             possible_object_triples = []\n",
        "\n",
        "#             For subject_properties in subject_property_dict:\n",
        "#                 Randomly select up to 2 triples per property (Constraint 3)\n",
        "#                 Add to possible_subject_triples\n",
        "\n",
        "#             For object_properties in object_property_dict:\n",
        "#                 Randomly select up to 2 triples per property (Constraint 3)\n",
        "#                 Add to possible_object_triples\n",
        "\n",
        "#             selected_triples = []\n",
        "#             Randomly select subject_count triples from possible_subject_triples\n",
        "#             Add to selected_triples\n",
        "#             Randomly select object_count triples from possible_object_triples\n",
        "#             Add to selected_triples\n",
        "\n",
        "#             Shuffle selected_triples\n",
        "\n",
        "#             Store selected_triples in dico_length_ratio_entity\n",
        "\n",
        "# Sample randomly from dico_length_ratio_entity using WebNLG's input length distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "VGse5Am5TWYd"
      },
      "outputs": [],
      "source": [
        "#@title Save triple sets in XML format: Implementation of the algorithm for the search+triples sampling\n",
        "import pickle\n",
        "import random\n",
        "import json\n",
        "from WikipediaPage_Generator.code.utils import create_xml, clear_folder\n",
        "\n",
        "dico_input_contents = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "suffle_selected_triples = True #@param{type:'boolean'}\n",
        "final_dataset_size = 20 #@param{type:'integer'}\n",
        "seed = 785 #@param{type:'integer'}\n",
        "MAX_TRIPLES_SET_LENGTH = 7\n",
        "MAX_TRIPLES_PER_PROPERTY = 2\n",
        "DEBUG = False#@param{type:'boolean'}\n",
        "\n",
        "random.seed(seed)\n",
        "\n",
        "def group_triples_by_property(triples):\n",
        "  property_dict = {}\n",
        "  for triple in triples:\n",
        "    if triple.DBprop not in property_dict:\n",
        "      property_dict[triple.DBprop] = []\n",
        "    property_dict[triple.DBprop].append(triple)\n",
        "  return property_dict\n",
        "\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "print(f'Triples for {len(dico_input_contents_loaded[\"Cities\"].keys())+len(dico_input_contents_loaded[\"People\"].keys())} entities were found.')\n",
        "\n",
        "all_triples_sets = []\n",
        "for category_name, entities in dico_input_contents_loaded.items():\n",
        "  for entity_name, triples in entities.items():\n",
        "    if DEBUG:\n",
        "      print('\\n')\n",
        "      print(f'Entity name: {entity_name}')\n",
        "      print('', f'# Triples available: {len(triples)}')\n",
        "    # Extract all the triples that have the current entity as subject\n",
        "    subject_triples = [tri for tri in triples if tri.DBsubj == entity_name]\n",
        "    # Extract all the triples that have the current entity as object\n",
        "    object_triples = [tri for tri in triples if tri.DBobj == entity_name]\n",
        "    if DEBUG:\n",
        "      print('  ', f'# Triples with entity as subject: {len(subject_triples)}')\n",
        "      print('    ', [f\"{o.DBsubj} {o.DBprop} {o.DBobj}\" for o in subject_triples[:10]])\n",
        "      print('  ', f'# Triples with entity as object: {len(object_triples)}')\n",
        "      print('    ', [f\"{o.DBsubj} {o.DBprop} {o.DBobj}\" for o in object_triples[:10]])\n",
        "\n",
        "    # Group subject_triples by property to be able to select randomly a maximum of N triples with the same property (set in MAX_TRIPLES_PER_PROPERTY)\n",
        "    subject_property_dict = group_triples_by_property(subject_triples)\n",
        "    # Group object_triples by property\n",
        "    object_property_dict = group_triples_by_property(object_triples)\n",
        "    if DEBUG:\n",
        "      print('  ', f'# Unique properties entity as subj: {len(subject_property_dict)}')\n",
        "      print('  ', f'# Unique properties entity as obj: {len(object_property_dict)}')\n",
        "\n",
        "    # create one triple set for each triple set length and ratio\n",
        "    for triples_len in range(1, MAX_TRIPLES_SET_LENGTH+1):\n",
        "      if DEBUG:\n",
        "        print(\"Triple set length:\", triples_len)\n",
        "\n",
        "      # the subject/object ratio is extracted considering subject count between 1 (possibly change to 0?)\n",
        "      # (at least one triples with the current entity as subject must be in the triples set)\n",
        "      # and the triples set length, while object count is triples set length - subject count\n",
        "      # together they give the subject/object ratio\n",
        "      for subj_count in range(1, triples_len+1):\n",
        "        obj_count = triples_len - subj_count\n",
        "        if DEBUG:\n",
        "          print(f\"  Subject count: {subj_count}, Object count: {obj_count}\")\n",
        "\n",
        "        # extracted the possible triples with the entity as subject\n",
        "        # for each property, select up to 2 triples, this gives us the complete\n",
        "        # set of triples with entity as subject from which randomly select the\n",
        "        # triples for the final triples set\n",
        "        possible_subject_triples = []\n",
        "        for prop, prop_triples in subject_property_dict.items():\n",
        "          possible_subject_triples.extend(random.sample(prop_triples, min(MAX_TRIPLES_PER_PROPERTY, len(prop_triples))))\n",
        "        # if DEBUG:\n",
        "        #   print('   ', [f\"{o.DBsubj} {o.DBprop} {o.DBobj}\" for o in possible_subject_triples])\n",
        "\n",
        "        # extracted the possible triples with the entity as object\n",
        "        # for each property, select up to 2 triples, this gives us the complete\n",
        "        # set of triples with entity as object from which randomly select the\n",
        "        # triples for the final triples set\n",
        "        possible_object_triples = []\n",
        "        for prop, prop_triples in object_property_dict.items():\n",
        "          possible_object_triples.extend(random.sample(prop_triples, min(MAX_TRIPLES_PER_PROPERTY, len(prop_triples))))\n",
        "\n",
        "        # check that we have enough triples to select\n",
        "        if len(possible_subject_triples) >= subj_count and len(possible_object_triples) >= obj_count:\n",
        "          selected_triples = []\n",
        "          # select subj_count triples where the entity is subject\n",
        "          selected_triples.extend(random.sample(possible_subject_triples, subj_count))\n",
        "          # select obj_count triples where the entity is object\n",
        "          selected_triples.extend(random.sample(possible_object_triples, obj_count))\n",
        "          # shuffle the selected triples\n",
        "          if suffle_selected_triples:\n",
        "            random.shuffle(selected_triples)\n",
        "          all_triples_sets.append({\n",
        "              'triples': selected_triples,\n",
        "              'subj_count': subj_count,\n",
        "              'obj_count': obj_count,\n",
        "              'triples_len': triples_len,\n",
        "              'entity_name': entity_name,\n",
        "              'category_name': category_name\n",
        "          })\n",
        "          if DEBUG:\n",
        "            print('   ', [f\"{o.DBsubj} {o.DBprop} {o.DBobj}\" for o in selected_triples])\n",
        "        else:\n",
        "          if DEBUG:\n",
        "            print('   ', 'Cannot make an input with the subj/obj ratio.')\n",
        "\n",
        "# sample all_triples_sets to reflect the same triples_len as WebNLG and specific\n",
        "# TODO replace with automatic extraction of real WebNLG distribution\n",
        "# IMPORTANT: Express distribution in %\n",
        "distr = {1: 20.8, 2: 19.6, 3: 19.6, 4: 17.2, 5: 12, 6: 6.4, 7: 4.4}\n",
        "total_prob = int(sum(distr.values()))\n",
        "# print(total_prob)\n",
        "assert total_prob == 100, \"Total probability should be 100%\"\n",
        "\n",
        "# If we are missing one sample in the total, find which number to increase by one. E.g. we want 20 samples and get 19 because several numbers are rounded down.\n",
        "# We look for the number that is the closest to the number above to round it up. e.g. we have 1.2, 3.4 and 3.1 samples, which round to 1, 3 and 3, we want to round 3.4 up to 4.\n",
        "# Keep here all float numbers to be sampled\n",
        "num_to_sample_list = [final_dataset_size*value/100 for key, value in distr.items()]\n",
        "# Keep here all rounded numbers to be sampled\n",
        "rounded_num_to_sample_list = [round(x) for x in num_to_sample_list]\n",
        "# If the total of rounded numbers to sample does not equal the actual number to sample, correct the float closest to the above number\n",
        "position_of_num_to_increase = 0\n",
        "highest_difference = 0\n",
        "if sum(rounded_num_to_sample_list) + 1 == final_dataset_size:\n",
        "  count = 0\n",
        "  while count < len(num_to_sample_list):\n",
        "    rounded_number = rounded_num_to_sample_list[count]\n",
        "    float_number = num_to_sample_list[count]\n",
        "    difference = float_number - rounded_number\n",
        "    if difference > highest_difference:\n",
        "      highest_difference = difference\n",
        "      position_of_num_to_increase = count\n",
        "    count += 1\n",
        "  # Now update the list with rounded numbers to sample\n",
        "  rounded_num_to_sample_list[position_of_num_to_increase] += 1\n",
        "  if DEBUG:\n",
        "    print(num_to_sample_list)\n",
        "    print(rounded_num_to_sample_list)\n",
        "    print(f'Position of number to increase: {position_of_num_to_increase} (diff = {highest_difference})')\n",
        "assert sum(rounded_num_to_sample_list) == final_dataset_size, 'Total number of samples does not match final_dataset_size.'\n",
        "\n",
        "sampled_triple_sets = []\n",
        "for count, (triples_len, prob) in enumerate(distr.items()):\n",
        "  # Select any triple set that has the expected size (allows for multiple triple sets per entity)\n",
        "  triples_of_len = [tri for tri in all_triples_sets if tri['triples_len'] == triples_len]\n",
        "  # num_to_sample = round(final_dataset_size * prob / 100)\n",
        "  num_to_sample = rounded_num_to_sample_list[count]\n",
        "  if num_to_sample <= len(triples_of_len):\n",
        "    sampled_triple_sets.extend(random.sample(triples_of_len, num_to_sample))\n",
        "  else:\n",
        "    print(f'!!! Could not select triples sets of size {triples_len} (not enough triple sets).')\n",
        "  print(f'Length: {triples_len}')\n",
        "  print(f'  # Total triple sets of current size: {len(triples_of_len)}')\n",
        "  print(f'  # Selected triple sets of current size: {num_to_sample}')\n",
        "  print(f'  # Total Selected triples at this point: {len(sampled_triple_sets)}')\n",
        "\n",
        "print(f'# Selected triple sets: {len(sampled_triple_sets)}')\n",
        "print(sampled_triple_sets[:10])\n",
        "\n",
        "## Number of triples for each category\n",
        "# print(f'# Triple sets People: {len([tri for tri in sampled_triples if tri[\"category_name\"]==\"People\"])}')\n",
        "# print(f'# Triple sets Cities:{len([tri for tri in sampled_triples if tri[\"category_name\"]==\"Cities\"])}')\n",
        "## Total number of triple sets\n",
        "# print(f'# Triple sets before sampling: {len(all_triples_sets)}')\n",
        "# print(all_triples_sets[:10])\n",
        "\n",
        "# Save datapoints in individual XML files\n",
        "counter_datapoints = 0\n",
        "entity_counter = {}\n",
        "folder_name = ''\n",
        "# print(folder_name)\n",
        "clear_folder(triple2predArg)\n",
        "os.makedirs(os.path.join(triple2predArg, 'People'))\n",
        "os.makedirs(os.path.join(triple2predArg, 'Cities'))\n",
        "for sampled_triple_set in sampled_triple_sets:\n",
        "  counter_datapoints += 1\n",
        "  # print(counter_datapoints)\n",
        "  list_triple_objects = sampled_triple_set['triples']\n",
        "  properties_selected = [i for i in range(len(sampled_triple_set['triples']))]\n",
        "  input_category = sampled_triple_set['category_name']\n",
        "  folder_name = input_category\n",
        "  entity_name = sampled_triple_set['entity_name']\n",
        "  if entity_name not in entity_counter.keys():\n",
        "    entity_counter[entity_name] = 0\n",
        "  else:\n",
        "    entity_counter[entity_name] += 1\n",
        "  filename = entity_name+'_'+str(entity_counter[entity_name])\n",
        "  # create xml file passing the entity name to use as filename\n",
        "  list_triples_text = create_xml(list_triple_objects, properties_selected, input_category, os.path.join(triple2predArg, folder_name), entity_name=filename, eid = counter_datapoints)\n",
        "\n",
        "# print(sampled_triple_sets[:10])\n",
        "# TODO save created dataset preprocessing the triples (we cannot save the object as it is)\n",
        "# for i, tri in enumerate(sampled_triple_sets):\n",
        "#   tri['triples'] = [f\"{o.DBsubj} | {o.DBprop} | {o.DBobj}\" for o in tri['triples']]\n",
        "# with open('/content/sampled_triples.json', 'w') as f:\n",
        "#   json.dump(sampled_triple_sets, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "5gSdmLEM7isu"
      },
      "outputs": [],
      "source": [
        "#@title Check triples extracted from DBpedia\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# dico_input_contents_loaded should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have entity_name as key, and a list of Triple Objects as value.\n",
        "# Triple Objects have 3 attributes: DBsubj, DBprop, DBobj\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "dico_properties = {}\n",
        "dico_different_properties = {}\n",
        "dico_entity_as_subj = {}\n",
        "dico_entity_as_obj = {}\n",
        "for category in dico_input_contents_loaded.keys():\n",
        "  # print(category)\n",
        "  for entity_name in dico_input_contents_loaded[category].keys():\n",
        "    all_properties = []\n",
        "    different_properties = []\n",
        "    subj_of_properties = []\n",
        "    obj_of_properties = []\n",
        "    # print('  ', entity_name)\n",
        "    for triple_object in dico_input_contents_loaded[category][entity_name]:\n",
        "      all_properties.append(triple_object.DBprop)\n",
        "      if triple_object.DBprop not in different_properties:\n",
        "        different_properties.append(triple_object.DBprop)\n",
        "      if triple_object.DBsubj == entity_name:\n",
        "        subj_of_properties.append(triple_object.DBprop)\n",
        "      if triple_object.DBobj == entity_name:\n",
        "        obj_of_properties.append(triple_object.DBprop)\n",
        "    dico_properties[entity_name] = len(all_properties)\n",
        "    dico_different_properties[entity_name] = len(different_properties)\n",
        "    dico_entity_as_subj[entity_name] = len(subj_of_properties)\n",
        "    dico_entity_as_obj[entity_name] = len(obj_of_properties)\n",
        "    # print(f'    {len(different_properties)} different properties')\n",
        "    # print(f'    {len(subj_of_properties)} properties with {entity_name} as subject')\n",
        "    # print(f'    {len(obj_of_properties)} properties with {entity_name} as object')\n",
        "\n",
        "dico_properties_sorted = {k: v for k, v in sorted(dico_properties.items(), key=lambda item: item[1], reverse=True)}\n",
        "dico_different_properties_sorted = {k: v for k, v in sorted(dico_different_properties.items(), key=lambda item: item[1], reverse=True)}\n",
        "dico_entity_as_subj_sorted = {k: v for k, v in sorted(dico_entity_as_subj.items(), key=lambda item: item[1], reverse=True)}\n",
        "dico_entity_as_obj_sorted = {k: v for k, v in sorted(dico_entity_as_obj.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "# Plot dico_properties dictionary using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(dico_properties_sorted.keys(), dico_properties_sorted.values())\n",
        "plt.xlabel('Entity Name')\n",
        "plt.ylabel('Number of Properties')\n",
        "plt.show()\n",
        "\n",
        "# Plot dico_different_properties dictionary using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(dico_different_properties_sorted.keys(), dico_different_properties_sorted.values())\n",
        "plt.xlabel('Entity Name')\n",
        "plt.ylabel('Number of Different Properties')\n",
        "plt.show()\n",
        "\n",
        "# Plot dico_entity_as_subj_sorted dictionary using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(dico_entity_as_subj_sorted.keys(), dico_entity_as_subj_sorted.values())\n",
        "plt.xlabel('Entity Name')\n",
        "plt.ylabel('Number of Properties with Entity as Subject')\n",
        "plt.show()\n",
        "\n",
        "# Plot dico_entity_as_obj_sorted dictionary using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(dico_entity_as_obj_sorted.keys(), dico_entity_as_obj_sorted.values())\n",
        "plt.xlabel('Entity Name')\n",
        "plt.ylabel('Number of Properties with Entity as Object')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "1JeN6DPmKnGC"
      },
      "outputs": [],
      "source": [
        "#@title Check dataset\n",
        "from colored import Fore, Back, Style\n",
        "\n",
        "sizes = {}\n",
        "property_count_per_datapoint = {}\n",
        "subjObj_ratios = {}\n",
        "category_count = {}\n",
        "for i, striple_set in enumerate(sampled_triple_sets):\n",
        "  print(f'Datapoint {i+1}:')\n",
        "  # Check that there is no error with the triple set size\n",
        "  assert striple_set['subj_count'] + striple_set['obj_count'] == striple_set['triples_len'], 'subj_count + obj_count should be equal to triples_len.'\n",
        "  print(f'{Fore.green}{Back.white}  subj_count and obj_count match triples_len{Style.reset}')\n",
        "  assert striple_set['triples_len'] == len(striple_set['triples']), 'triples_len should be equal to the number of triples in the triple set.'\n",
        "  print(f'{Fore.green}{Back.white}  triples_len and number of triples match{Style.reset}')\n",
        "\n",
        "  # Collect and check property count for each datapoint\n",
        "  if str(i) not in property_count_per_datapoint.keys():\n",
        "    property_count_per_datapoint[str(i)] = {}\n",
        "  for triple in striple_set['triples']:\n",
        "    if triple.DBprop not in property_count_per_datapoint[str(i)]:\n",
        "      property_count_per_datapoint[str(i)][triple.DBprop] = 0\n",
        "    property_count_per_datapoint[str(i)][triple.DBprop] += 1\n",
        "  assert property_count_per_datapoint[str(i)][triple.DBprop] <= 2, 'No more than 2 instances of a property per triple set.'\n",
        "  print(f'{Fore.green}{Back.white}  No more than 2 instances of the same property in the triple set{Style.reset}')\n",
        "\n",
        "  # Collect all triple sizes\n",
        "  if striple_set['triples_len'] not in sizes.keys():\n",
        "    sizes[striple_set['triples_len']] = 0\n",
        "  sizes[striple_set['triples_len']] += 1\n",
        "\n",
        "  # Collect category count\n",
        "  if striple_set['category_name'] not in category_count.keys():\n",
        "    category_count[striple_set['category_name']] = {}\n",
        "  if striple_set['entity_name'] not in category_count[striple_set['category_name']].keys():\n",
        "    category_count[striple_set['category_name']][striple_set['entity_name']] = 0\n",
        "  category_count[striple_set['category_name']][striple_set['entity_name']] += 1\n",
        "  # Sort entity names by frequency\n",
        "  category_count[striple_set['category_name']] = {k: v for k, v in sorted(category_count[striple_set['category_name']].items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "  # Collect configuration count of triples (i.e. for each size, what is the subj/obj ratio count)\n",
        "  if striple_set['triples_len'] not in subjObj_ratios.keys():\n",
        "    subjObj_ratios[striple_set['triples_len']] = {}\n",
        "  subj_obj_ratio = str(striple_set['subj_count'])+'/'+str(striple_set['obj_count'])\n",
        "  if subj_obj_ratio not in subjObj_ratios[striple_set['triples_len']].keys():\n",
        "    subjObj_ratios[striple_set['triples_len']][subj_obj_ratio] = 0\n",
        "  subjObj_ratios[striple_set['triples_len']][subj_obj_ratio] += 1\n",
        "\n",
        "print('')\n",
        "# Check number of datapoints for each size and in total\n",
        "assert sum(sizes.values()) == final_dataset_size\n",
        "print(f'{Fore.green}{Back.white}Total number of datapoints: {sum(sizes.values())}{Style.reset}')\n",
        "print(f'{Fore.green}{Back.white}Triple size distribution: {sizes}{Style.reset}')\n",
        "print('')\n",
        "print(f'Subject/Object ratios:    {subjObj_ratios}')\n",
        "# print sum of people\n",
        "print(f\"Category count:           'People': {sum(category_count['People'].values())} ({len(category_count['People'])} distinct), 'Cities': {sum(category_count['Cities'].values())} ({len(category_count['Cities'])} distinct)\")\n",
        "print(f'People instances:         {category_count[\"People\"]}')\n",
        "print(f'Cities instances:         {category_count[\"Cities\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixeaiGixDGMB"
      },
      "source": [
        "## Build knowledge graphs #3: free distribution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Optional: examine pickle\n",
        "# with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "#   dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "# print(dico_input_contents_loaded.keys())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LL4-GMgYfqSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "hxGzSaD3Czzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d163230-cae1-4b7e-e09d-4beff1b29878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "geography\n",
            "  Abu_Dhabi: pre-selected 69/527 properties.\n",
            "    69 included in dataset\n",
            "  Africa: pre-selected 80/305 properties.\n",
            "    80 included in dataset\n",
            "  Alps: pre-selected 32/148 properties.\n",
            "    32 included in dataset\n",
            "  Altiplano: pre-selected 8/8 properties.\n",
            "    8 included in dataset\n",
            "  American_Samoa: pre-selected 68/256 properties.\n",
            "    68 included in dataset\n",
            "  Antarctic_Peninsula: pre-selected 16/57 properties.\n",
            "    16 included in dataset\n",
            "  Arabian_Peninsula: pre-selected 37/123 properties.\n",
            "    37 included in dataset\n",
            "  Arctic_Circle: pre-selected 2/2 properties.\n",
            "  Ashgabat: pre-selected 64/315 properties.\n",
            "    64 included in dataset\n",
            "  Atacama_Desert: pre-selected 10/17 properties.\n",
            "    10 included in dataset\n",
            "  Axel_Heiberg_Island: pre-selected 9/12 properties.\n",
            "    9 included in dataset\n",
            "  Baku: pre-selected 106/2123 properties.\n",
            "    106 included in dataset\n",
            "  Balkan_Mountains: pre-selected 14/21 properties.\n",
            "    14 included in dataset\n",
            "  Bass_Strait: pre-selected 18/162 properties.\n",
            "    18 included in dataset\n",
            "  Bavaria: pre-selected 141/2556 properties.\n",
            "    141 included in dataset\n",
            "  Black_Sea: pre-selected 53/199 properties.\n",
            "    53 included in dataset\n",
            "  Bosnia_and_Herzegovina: pre-selected 163/6344 properties.\n",
            "    163 included in dataset\n",
            "  Brasília: pre-selected 76/596 properties.\n",
            "    76 included in dataset\n",
            "  Brazilian_Highlands: pre-selected 3/3 properties.\n",
            "  Brazzaville: pre-selected 56/225 properties.\n",
            "    56 included in dataset\n",
            "  Bridgetown: pre-selected 45/196 properties.\n",
            "    45 included in dataset\n",
            "  Brunei: pre-selected 131/988 properties.\n",
            "    131 included in dataset\n",
            "  Bryce_Canyon_National_Park: pre-selected 12/25 properties.\n",
            "    12 included in dataset\n",
            "  Burkina_Faso: pre-selected 113/3344 properties.\n",
            "    113 included in dataset\n",
            "  Cape_Town: pre-selected 131/2139 properties.\n",
            "    131 included in dataset\n",
            "  Castries: pre-selected 43/100 properties.\n",
            "    43 included in dataset\n",
            "  Caucasus_Mountains: pre-selected 17/56 properties.\n",
            "    17 included in dataset\n",
            "  Central_African_Republic: pre-selected 89/820 properties.\n",
            "    89 included in dataset\n",
            "  Chongqing: pre-selected 95/635 properties.\n",
            "    95 included in dataset\n",
            "  City_of_Brussels: pre-selected 34/289 properties.\n",
            "    34 included in dataset\n",
            "  Columbia_Plateau: pre-selected 6/6 properties.\n",
            "  Columbia_River: pre-selected 36/152 properties.\n",
            "    36 included in dataset\n",
            "  Copenhagen: pre-selected 162/4313 properties.\n",
            "    162 included in dataset\n",
            "  Costa_Rica: pre-selected 124/1692 properties.\n",
            "    124 included in dataset\n",
            "  Country: pre-selected 4/4 properties.\n",
            "  Crimea: pre-selected 60/333 properties.\n",
            "    60 included in dataset\n",
            "  Croatia: pre-selected 179/7836 properties.\n",
            "    179 included in dataset\n",
            "  Cuba: pre-selected 148/2934 properties.\n",
            "    148 included in dataset\n",
            "  Cyprus: pre-selected 160/1979 properties.\n",
            "    160 included in dataset\n",
            "  Dardanelles: pre-selected 14/24 properties.\n",
            "    14 included in dataset\n",
            "  Daugava: pre-selected 19/27 properties.\n",
            "    19 included in dataset\n",
            "  Dnieper: pre-selected 41/82 properties.\n",
            "    41 included in dataset\n",
            "  Drakensberg: pre-selected 14/26 properties.\n",
            "    14 included in dataset\n",
            "  Dubai: pre-selected 119/1125 properties.\n",
            "    119 included in dataset\n",
            "  Durban: pre-selected 111/1044 properties.\n",
            "    111 included in dataset\n",
            "  Düsseldorf: pre-selected 87/815 properties.\n",
            "    87 included in dataset\n",
            "  Equator: pre-selected 1/1 properties.\n",
            "  Ethiopia: pre-selected 160/2230 properties.\n",
            "    160 included in dataset\n",
            "  Federated_States_of_Micronesia: pre-selected 70/196 properties.\n",
            "    70 included in dataset\n",
            "  Genoa: pre-selected 118/1476 properties.\n",
            "    118 included in dataset\n",
            "  Geoid: pre-selected 1/1 properties.\n",
            "  Georgetown,_Guyana: pre-selected 52/464 properties.\n",
            "    52 included in dataset\n",
            "  Ghana: pre-selected 172/4285 properties.\n",
            "    172 included in dataset\n",
            "  Gobi_Desert: pre-selected 4/4 properties.\n",
            "  Godavari_River: pre-selected 25/46 properties.\n",
            "    25 included in dataset\n",
            "  Grenada: pre-selected 73/357 properties.\n",
            "    73 included in dataset\n",
            "  Guangzhou: pre-selected 127/864 properties.\n",
            "    127 included in dataset\n",
            "  Gulf_of_Alaska: pre-selected 16/18 properties.\n",
            "    16 included in dataset\n",
            "  Gulf_of_Mexico: pre-selected 44/197 properties.\n",
            "    44 included in dataset\n",
            "  Haiti: pre-selected 119/1227 properties.\n",
            "    119 included in dataset\n",
            "  Hamburg: pre-selected 188/3212 properties.\n",
            "    188 included in dataset\n",
            "  Ho_Chi_Minh_City: pre-selected 128/735 properties.\n",
            "    128 included in dataset\n",
            "  Hudson_Strait: pre-selected 7/56 properties.\n",
            "  Indian_Ocean: pre-selected 72/557 properties.\n",
            "    72 included in dataset\n",
            "  Islamabad: pre-selected 110/755 properties.\n",
            "    110 included in dataset\n",
            "  Italy: pre-selected 33/9599 properties.\n",
            "    33 included in dataset\n",
            "  Java_Sea: pre-selected 34/79 properties.\n",
            "    34 included in dataset\n",
            "  Jutland: pre-selected 26/70 properties.\n",
            "    26 included in dataset\n",
            "  Kabul: pre-selected 94/647 properties.\n",
            "    94 included in dataset\n",
            "  Kalahari_Desert: pre-selected 4/4 properties.\n",
            "  Kingstown: pre-selected 30/65 properties.\n",
            "    30 included in dataset\n",
            "  Kunlun_Mountains: pre-selected 11/15 properties.\n",
            "    11 included in dataset\n",
            "  Kyiv: pre-selected 170/2633 properties.\n",
            "    170 included in dataset\n",
            "  Lake_Michigan: pre-selected 45/160 properties.\n",
            "    45 included in dataset\n",
            "  Lake_Nicaragua: pre-selected 20/26 properties.\n",
            "    20 included in dataset\n",
            "  Lesser_Antilles: pre-selected 25/59 properties.\n",
            "    25 included in dataset\n",
            "  Lima: pre-selected 130/2318 properties.\n",
            "    130 included in dataset\n",
            "  Lithuania: pre-selected 165/3538 properties.\n",
            "    165 included in dataset\n",
            "  Ljubljana: pre-selected 87/1331 properties.\n",
            "    87 included in dataset\n",
            "  Lomé: pre-selected 50/189 properties.\n",
            "    50 included in dataset\n",
            "  London: pre-selected 26/9932 properties.\n",
            "    26 included in dataset\n",
            "  Longitude: pre-selected 1/1 properties.\n",
            "  Luanda: pre-selected 78/613 properties.\n",
            "    78 included in dataset\n",
            "  Lusaka: pre-selected 74/425 properties.\n",
            "    74 included in dataset\n",
            "  Mali: pre-selected 126/1821 properties.\n",
            "    126 included in dataset\n",
            "  Malé: pre-selected 69/358 properties.\n",
            "    69 included in dataset\n",
            "  Manila: pre-selected 163/2369 properties.\n",
            "    163 included in dataset\n",
            "  Maputo: pre-selected 58/275 properties.\n",
            "    58 included in dataset\n",
            "  Marshall_Islands: pre-selected 72/198 properties.\n",
            "    72 included in dataset\n",
            "  Melbourne: pre-selected 145/6556 properties.\n",
            "    145 included in dataset\n",
            "  Mercator_projection: pre-selected 1/1 properties.\n",
            "  Mississippi_River: pre-selected 50/284 properties.\n",
            "    50 included in dataset\n",
            "  Monrovia: pre-selected 74/379 properties.\n",
            "    74 included in dataset\n",
            "  Mount_Kilimanjaro: pre-selected 15/22 properties.\n",
            "    15 included in dataset\n",
            "  Mount_Kosciuszko: pre-selected 11/13 properties.\n",
            "    11 included in dataset\n",
            "  Mount_Olympus: pre-selected 12/12 properties.\n",
            "    12 included in dataset\n",
            "  Mount_Vesuvius: pre-selected 8/9 properties.\n",
            "    8 included in dataset\n",
            "  Munich: pre-selected 147/3173 properties.\n",
            "    147 included in dataset\n",
            "  Murray_River: pre-selected 25/35 properties.\n",
            "    25 included in dataset\n",
            "  Najd: pre-selected 18/35 properties.\n",
            "    18 included in dataset\n",
            "  New_Delhi: pre-selected 131/2375 properties.\n",
            "    131 included in dataset\n",
            "  New_Siberian_Islands: pre-selected 12/15 properties.\n",
            "    12 included in dataset\n",
            "  Ngorongoro_Conservation_Area: pre-selected 3/3 properties.\n",
            "  Niger: pre-selected 100/790 properties.\n",
            "    100 included in dataset\n",
            "  Nigeria: pre-selected 222/7607 properties.\n",
            "    199 included in dataset\n",
            "  North: pre-selected 1/1 properties.\n",
            "  North_Korea: pre-selected 138/1254 properties.\n",
            "    138 included in dataset\n",
            "  North_Sea: pre-selected 53/270 properties.\n",
            "    53 included in dataset\n",
            "  Norway: pre-selected 72/9923 properties.\n",
            "    72 included in dataset\n",
            "  Nuremberg: pre-selected 69/580 properties.\n",
            "    69 included in dataset\n",
            "  Okapi_Wildlife_Reserve: pre-selected 3/3 properties.\n",
            "  Ontario: pre-selected 16/9885 properties.\n",
            "    16 included in dataset\n",
            "  Ottawa: pre-selected 145/3249 properties.\n",
            "    145 included in dataset\n",
            "  Pacific_Coast_Ranges: pre-selected 14/59 properties.\n",
            "    14 included in dataset\n",
            "  Palau: pre-selected 72/186 properties.\n",
            "    72 included in dataset\n",
            "  Panama: pre-selected 130/1830 properties.\n",
            "    130 included in dataset\n",
            "  Papua_New_Guinea: pre-selected 117/2235 properties.\n",
            "    117 included in dataset\n",
            "  Paris: pre-selected 28/9986 properties.\n",
            "    28 included in dataset\n",
            "  Patagonian_Desert: pre-selected 1/1 properties.\n",
            "  Phoenix,_Arizona: pre-selected 127/1770 properties.\n",
            "    127 included in dataset\n",
            "  Port_Vila: pre-selected 37/77 properties.\n",
            "    37 included in dataset\n",
            "  Pyrenees: pre-selected 28/183 properties.\n",
            "    28 included in dataset\n",
            "  Quebec: pre-selected 163/9930 properties.\n",
            "    163 included in dataset\n",
            "  Rabat: pre-selected 56/427 properties.\n",
            "    56 included in dataset\n",
            "  Recife: pre-selected 52/465 properties.\n",
            "    52 included in dataset\n",
            "  Republic_of_Ireland: pre-selected 159/6110 properties.\n",
            "    159 included in dataset\n",
            "  Ross_Sea: pre-selected 11/11 properties.\n",
            "    11 included in dataset\n",
            "  Saimaa: pre-selected 11/11 properties.\n",
            "    11 included in dataset\n",
            "  Salonga_National_Park: pre-selected 3/3 properties.\n",
            "  San_Francisco: pre-selected 224/6236 properties.\n",
            "    199 included in dataset\n",
            "  San_Juan,_Puerto_Rico: pre-selected 102/1390 properties.\n",
            "    102 included in dataset\n",
            "  Sapporo: pre-selected 65/381 properties.\n",
            "    65 included in dataset\n",
            "  Sardinia: pre-selected 80/674 properties.\n",
            "    80 included in dataset\n",
            "  Saudi_Arabia: pre-selected 159/3445 properties.\n",
            "    159 included in dataset\n",
            "  Scotia_Sea: pre-selected 5/5 properties.\n",
            "  Scotland: pre-selected 117/9866 properties.\n",
            "    117 included in dataset\n",
            "  Sea_of_Azov: pre-selected 29/35 properties.\n",
            "    29 included in dataset\n",
            "  Sea_of_Japan: pre-selected 23/102 properties.\n",
            "    23 included in dataset\n",
            "  Seattle: pre-selected 174/4115 properties.\n",
            "    174 included in dataset\n",
            "  Seoul: pre-selected 156/4576 properties.\n",
            "    156 included in dataset\n",
            "  Serengeti_National_Park: pre-selected 6/6 properties.\n",
            "  Sian_Kaʼan_Biosphere_Reserve: pre-selected 3/4 properties.\n",
            "  Sicily: pre-selected 114/1297 properties.\n",
            "    114 included in dataset\n",
            "  Sikhote-Alin: pre-selected 5/9 properties.\n",
            "  Slovenia: pre-selected 148/8507 properties.\n",
            "    148 included in dataset\n",
            "  Sofia: pre-selected 126/2174 properties.\n",
            "    126 included in dataset\n",
            "  Somalia: pre-selected 138/1241 properties.\n",
            "    138 included in dataset\n",
            "  South_America: pre-selected 52/168 properties.\n",
            "    52 included in dataset\n",
            "  Southeast_Asia: pre-selected 52/531 properties.\n",
            "    52 included in dataset\n",
            "  Southern_Alps: pre-selected 12/138 properties.\n",
            "    12 included in dataset\n",
            "  St._John's,_Antigua_and_Barbuda: pre-selected 44/98 properties.\n",
            "    44 included in dataset\n",
            "  St._Louis: pre-selected 165/4231 properties.\n",
            "    165 included in dataset\n",
            "  Stockholm: pre-selected 150/5629 properties.\n",
            "    150 included in dataset\n",
            "  Strait_of_Gibraltar: pre-selected 16/28 properties.\n",
            "    16 included in dataset\n",
            "  Straits_of_Florida: pre-selected 4/4 properties.\n",
            "  Sunda_Strait: pre-selected 16/16 properties.\n",
            "    16 included in dataset\n",
            "  Surabaya: pre-selected 107/465 properties.\n",
            "    107 included in dataset\n",
            "  Suriname: pre-selected 100/836 properties.\n",
            "    100 included in dataset\n",
            "  Sweden: pre-selected 83/9866 properties.\n",
            "    83 included in dataset\n",
            "  Syr_Darya: pre-selected 23/30 properties.\n",
            "    23 included in dataset\n",
            "  São_Francisco_River: pre-selected 18/45 properties.\n",
            "    18 included in dataset\n",
            "  Taiwan: pre-selected 192/6800 properties.\n",
            "    192 included in dataset\n",
            "  Tasmania: pre-selected 91/1229 properties.\n",
            "    91 included in dataset\n",
            "  Tbilisi: pre-selected 107/1754 properties.\n",
            "    107 included in dataset\n",
            "  Tegucigalpa: pre-selected 63/275 properties.\n",
            "    63 included in dataset\n",
            "  Texas: pre-selected 203/9641 properties.\n",
            "    199 included in dataset\n",
            "  The_Hague: pre-selected 101/1675 properties.\n",
            "    101 included in dataset\n",
            "  Tian_Shan: pre-selected 18/38 properties.\n",
            "    18 included in dataset\n",
            "  Tirana: pre-selected 109/1227 properties.\n",
            "    109 included in dataset\n",
            "  Tocantins_River: pre-selected 12/32 properties.\n",
            "    12 included in dataset\n",
            "  Togo: pre-selected 97/1022 properties.\n",
            "    97 included in dataset\n",
            "  Torres_Strait: pre-selected 14/19 properties.\n",
            "    14 included in dataset\n",
            "  Tuvalu: pre-selected 69/149 properties.\n",
            "    69 included in dataset\n",
            "  United_Kingdom: pre-selected 103/9322 properties.\n",
            "    103 included in dataset\n",
            "  Uruguay: pre-selected 137/2678 properties.\n",
            "    137 included in dataset\n",
            "  Uzbekistan: pre-selected 140/1738 properties.\n",
            "    140 included in dataset\n",
            "  Valparaíso: pre-selected 64/378 properties.\n",
            "    64 included in dataset\n",
            "  Vanuatu: pre-selected 87/550 properties.\n",
            "    87 included in dataset\n",
            "  Vatican_City: pre-selected 72/648 properties.\n",
            "    72 included in dataset\n",
            "  Venezuela: pre-selected 147/4023 properties.\n",
            "    147 included in dataset\n",
            "  Victoria,_Seychelles: pre-selected 32/61 properties.\n",
            "    32 included in dataset\n",
            "  Vietnam: pre-selected 181/4830 properties.\n",
            "    181 included in dataset\n",
            "  Vladivostok: pre-selected 73/221 properties.\n",
            "    73 included in dataset\n",
            "  West_Siberian_Plain: pre-selected 6/76 properties.\n",
            "  White_Nile: pre-selected 18/23 properties.\n",
            "    18 included in dataset\n",
            "  Yosemite_National_Park: pre-selected 19/138 properties.\n",
            "    19 included in dataset\n",
            "  Zealand: pre-selected 32/75 properties.\n",
            "    32 included in dataset\n",
            "  Zurich: pre-selected 121/1683 properties.\n",
            "    121 included in dataset\n",
            "history\n",
            "  1755_Lisbon_earthquake: pre-selected 2/2 properties.\n",
            "  2004_Indian_Ocean_earthquake_and_tsunami: pre-selected 10/15 properties.\n",
            "    10 included in dataset\n",
            "  2010_Haiti_earthquake: pre-selected 2/2 properties.\n",
            "  Achaemenid_Empire: pre-selected 32/48 properties.\n",
            "    32 included in dataset\n",
            "  American_frontier: pre-selected 17/17 properties.\n",
            "    17 included in dataset\n",
            "  Andean_civilizations: pre-selected 3/3 properties.\n",
            "  Angolan_Civil_War: pre-selected 29/158 properties.\n",
            "    29 included in dataset\n",
            "  Arab_Revolt: pre-selected 18/30 properties.\n",
            "    18 included in dataset\n",
            "  Archaeogenetics: pre-selected 1/1 properties.\n",
            "  Art_history: pre-selected 37/214 properties.\n",
            "    37 included in dataset\n",
            "  Austrian_Empire: pre-selected 58/1425 properties.\n",
            "    58 included in dataset\n",
            "  Ayutthaya_Kingdom: pre-selected 30/35 properties.\n",
            "    30 included in dataset\n",
            "  Babylonia: pre-selected 25/37 properties.\n",
            "    25 included in dataset\n",
            "  Balkan_Wars: pre-selected 21/418 properties.\n",
            "    21 included in dataset\n",
            "  Belgian_Revolution: pre-selected 11/24 properties.\n",
            "    11 included in dataset\n",
            "  Biography: pre-selected 31/747 properties.\n",
            "    31 included in dataset\n",
            "  Bourgeoisie: pre-selected 2/2 properties.\n",
            "  Boxer_Rebellion: pre-selected 19/425 properties.\n",
            "    19 included in dataset\n",
            "  Burgundians: pre-selected 5/5 properties.\n",
            "  COVID-19_pandemic: pre-selected 20/20 properties.\n",
            "    20 included in dataset\n",
            "  Carolingian_Empire: pre-selected 13/15 properties.\n",
            "    13 included in dataset\n",
            "  Castle: pre-selected 10/842 properties.\n",
            "    10 included in dataset\n",
            "  Chernobyl_disaster: pre-selected 9/9 properties.\n",
            "    9 included in dataset\n",
            "  Chronology: pre-selected 6/8 properties.\n",
            "  Classical_antiquity: pre-selected 9/12 properties.\n",
            "    9 included in dataset\n",
            "  Cold_War: pre-selected 17/701 properties.\n",
            "    17 included in dataset\n",
            "  Confederate_States_of_America: pre-selected 65/1255 properties.\n",
            "    65 included in dataset\n",
            "  Crusader_states: pre-selected 3/3 properties.\n",
            "  Crusades: pre-selected 20/142 properties.\n",
            "    20 included in dataset\n",
            "  Cultural_Revolution: pre-selected 11/11 properties.\n",
            "    11 included in dataset\n",
            "  Dacia: pre-selected 15/15 properties.\n",
            "    15 included in dataset\n",
            "  Deccan_sultanates: pre-selected 4/4 properties.\n",
            "  Delian_League: pre-selected 4/4 properties.\n",
            "  Eastern_Bloc: pre-selected 9/9 properties.\n",
            "    9 included in dataset\n",
            "  Empire_of_Japan: pre-selected 89/3146 properties.\n",
            "    89 included in dataset\n",
            "  Etruscan_civilization: pre-selected 5/5 properties.\n",
            "  Fall_of_the_Western_Roman_Empire: pre-selected 6/35 properties.\n",
            "  Fertile_Crescent: pre-selected 5/5 properties.\n",
            "  First_Indochina_War: pre-selected 31/385 properties.\n",
            "    31 included in dataset\n",
            "  Franks: pre-selected 5/5 properties.\n",
            "  French_Indochina: pre-selected 45/476 properties.\n",
            "    45 included in dataset\n",
            "  French_Revolutionary_Wars: pre-selected 39/1431 properties.\n",
            "    39 included in dataset\n",
            "  French_colonial_empire: pre-selected 40/44 properties.\n",
            "    40 included in dataset\n",
            "  Golden_Horde: pre-selected 17/19 properties.\n",
            "    17 included in dataset\n",
            "  Goryeo: pre-selected 22/39 properties.\n",
            "    22 included in dataset\n",
            "  Great_Famine_(Ireland): pre-selected 3/3 properties.\n",
            "  Great_Moravia: pre-selected 9/10 properties.\n",
            "    9 included in dataset\n",
            "  Gupta_Empire: pre-selected 13/13 properties.\n",
            "    13 included in dataset\n",
            "  Heraldry: pre-selected 13/16 properties.\n",
            "    13 included in dataset\n",
            "  History: pre-selected 54/1243 properties.\n",
            "    54 included in dataset\n",
            "  History_of_Afghanistan: pre-selected 7/7 properties.\n",
            "  History_of_Antarctica: pre-selected 3/3 properties.\n",
            "  History_of_Argentina: pre-selected 1/1 properties.\n",
            "  History_of_Australia: pre-selected 5/7 properties.\n",
            "  History_of_Austria: pre-selected 2/2 properties.\n",
            "  History_of_Bangladesh: pre-selected 1/1 properties.\n",
            "  History_of_Belgium: pre-selected 1/1 properties.\n",
            "  History_of_Chile: pre-selected 8/12 properties.\n",
            "    8 included in dataset\n",
            "  History_of_Christianity: pre-selected 14/16 properties.\n",
            "    14 included in dataset\n",
            "  History_of_Denmark: pre-selected 8/13 properties.\n",
            "    8 included in dataset\n",
            "  History_of_France: pre-selected 9/11 properties.\n",
            "    9 included in dataset\n",
            "  History_of_India: pre-selected 11/13 properties.\n",
            "    11 included in dataset\n",
            "  History_of_Islam: pre-selected 15/43 properties.\n",
            "    15 included in dataset\n",
            "  History_of_Israel: pre-selected 3/3 properties.\n",
            "  History_of_Korea: pre-selected 4/4 properties.\n",
            "  History_of_Lebanon: pre-selected 1/1 properties.\n",
            "  History_of_Lithuania: pre-selected 4/4 properties.\n",
            "  History_of_Mongolia: pre-selected 1/1 properties.\n",
            "  History_of_Paraguay: pre-selected 3/3 properties.\n",
            "  History_of_Russia: pre-selected 10/20 properties.\n",
            "    10 included in dataset\n",
            "  History_of_Serbia: pre-selected 1/1 properties.\n",
            "  History_of_Singapore: pre-selected 2/2 properties.\n",
            "  History_of_Sweden: pre-selected 2/2 properties.\n",
            "  History_of_Turkey: pre-selected 1/1 properties.\n",
            "  History_of_Ukraine: pre-selected 6/6 properties.\n",
            "  History_of_Wales: pre-selected 6/6 properties.\n",
            "  History_of_art: pre-selected 22/39 properties.\n",
            "    22 included in dataset\n",
            "  History_of_aviation: pre-selected 6/6 properties.\n",
            "  History_of_chemistry: pre-selected 5/9 properties.\n",
            "  History_of_mathematics: pre-selected 13/37 properties.\n",
            "    13 included in dataset\n",
            "  History_of_technology: pre-selected 8/12 properties.\n",
            "    8 included in dataset\n",
            "  History_of_the_Middle_East: pre-selected 2/2 properties.\n",
            "  History_of_the_Netherlands: pre-selected 3/3 properties.\n",
            "  History_of_the_People's_Republic_of_China: pre-selected 1/1 properties.\n",
            "  History_of_the_Philippines: pre-selected 5/5 properties.\n",
            "  Hittites: pre-selected 13/14 properties.\n",
            "    13 included in dataset\n",
            "  House_of_Medici: pre-selected 3/3 properties.\n",
            "  House_of_Romanov: pre-selected 4/4 properties.\n",
            "  Human_evolution: pre-selected 12/17 properties.\n",
            "    12 included in dataset\n",
            "  Huns: pre-selected 7/7 properties.\n",
            "  Inca_Empire: pre-selected 21/24 properties.\n",
            "    21 included in dataset\n",
            "  Indian_independence_movement: pre-selected 23/120 properties.\n",
            "    23 included in dataset\n",
            "  Indo-Scythians: pre-selected 4/4 properties.\n",
            "  Investiture_Controversy: pre-selected 2/2 properties.\n",
            "  Iron_Curtain: pre-selected 2/2 properties.\n",
            "  Islamic_Golden_Age: pre-selected 5/19 properties.\n",
            "  Khanate_of_Kazan: pre-selected 10/12 properties.\n",
            "    10 included in dataset\n",
            "  Khitan_people: pre-selected 9/10 properties.\n",
            "    9 included in dataset\n",
            "  Kingdom_of_Armenia_(antiquity): pre-selected 18/40 properties.\n",
            "    18 included in dataset\n",
            "  Macedonian_Wars: pre-selected 5/6 properties.\n",
            "  Magna_Graecia: pre-selected 14/14 properties.\n",
            "    14 included in dataset\n",
            "  Mahdist_War: pre-selected 32/272 properties.\n",
            "    32 included in dataset\n",
            "  Majapahit: pre-selected 16/16 properties.\n",
            "    16 included in dataset\n",
            "  Meiji_Restoration: pre-selected 4/4 properties.\n",
            "  Memphis,_Egypt: pre-selected 7/16 properties.\n",
            "  Mensheviks: pre-selected 11/17 properties.\n",
            "    11 included in dataset\n",
            "  Mexican_Revolution: pre-selected 41/216 properties.\n",
            "    41 included in dataset\n",
            "  Minoan_civilization: pre-selected 3/3 properties.\n",
            "  Mitochondrial_Eve: pre-selected 3/3 properties.\n",
            "  Mughal_Empire: pre-selected 70/364 properties.\n",
            "    70 included in dataset\n",
            "  Mycenaean_Greece: pre-selected 1/1 properties.\n",
            "  Nazi_Germany: pre-selected 117/5356 properties.\n",
            "    117 included in dataset\n",
            "  Nazi_concentration_camps: pre-selected 14/48 properties.\n",
            "    14 included in dataset\n",
            "  Neanderthal: pre-selected 1/1 properties.\n",
            "  Neustria: pre-selected 17/17 properties.\n",
            "    17 included in dataset\n",
            "  Normans: pre-selected 24/41 properties.\n",
            "    24 included in dataset\n",
            "  North_Yemen_Civil_War: pre-selected 22/58 properties.\n",
            "    22 included in dataset\n",
            "  Ostrogoths: pre-selected 3/3 properties.\n",
            "  Palmyra: pre-selected 14/15 properties.\n",
            "    14 included in dataset\n",
            "  Paraguayan_War: pre-selected 34/229 properties.\n",
            "    34 included in dataset\n",
            "  Paris_Commune: pre-selected 31/42 properties.\n",
            "    31 included in dataset\n",
            "  Phrygia: pre-selected 12/15 properties.\n",
            "    12 included in dataset\n",
            "  Pompeii: pre-selected 9/12 properties.\n",
            "    9 included in dataset\n",
            "  Prussia: pre-selected 57/539 properties.\n",
            "    57 included in dataset\n",
            "  Ptolemaic_Kingdom: pre-selected 23/29 properties.\n",
            "    23 included in dataset\n",
            "  Pyu_city-states: pre-selected 5/9 properties.\n",
            "  Revolutions_of_1848: pre-selected 11/17 properties.\n",
            "    11 included in dataset\n",
            "  Roman–Persian_Wars: pre-selected 16/115 properties.\n",
            "    16 included in dataset\n",
            "  Russian_Civil_War: pre-selected 22/1019 properties.\n",
            "    22 included in dataset\n",
            "  Russo-Polish_War_(1654–1667): pre-selected 25/86 properties.\n",
            "    25 included in dataset\n",
            "  Scythia: pre-selected 1/1 properties.\n",
            "  Second_Bulgarian_Empire: pre-selected 14/16 properties.\n",
            "    14 included in dataset\n",
            "  Sturmabteilung: pre-selected 10/16 properties.\n",
            "    10 included in dataset\n",
            "  Susa: pre-selected 12/12 properties.\n",
            "    12 included in dataset\n",
            "  The_Holocaust: pre-selected 31/114 properties.\n",
            "    31 included in dataset\n",
            "  The_Troubles: pre-selected 30/364 properties.\n",
            "    30 included in dataset\n",
            "  Thirty_Years'_War: pre-selected 25/248 properties.\n",
            "    25 included in dataset\n",
            "  Three_Kingdoms_of_Korea: pre-selected 1/1 properties.\n",
            "  Treaty_of_Paris_(1814): pre-selected 1/1 properties.\n",
            "  Treaty_of_Versailles: pre-selected 3/3 properties.\n",
            "  Trojan_War: pre-selected 5/5 properties.\n",
            "  Troy: pre-selected 13/14 properties.\n",
            "    13 included in dataset\n",
            "  Tyre,_Lebanon: pre-selected 35/67 properties.\n",
            "    35 included in dataset\n",
            "  Ugarit: pre-selected 5/5 properties.\n",
            "  Unification_of_Germany: pre-selected 6/6 properties.\n",
            "  Valley_of_the_Kings: pre-selected 2/3 properties.\n",
            "  War_in_Afghanistan_(2001–2021): pre-selected 15/1768 properties.\n",
            "    15 included in dataset\n",
            "  War_of_1812: pre-selected 34/859 properties.\n",
            "    34 included in dataset\n",
            "  Winter_War: pre-selected 34/445 properties.\n",
            "    34 included in dataset\n",
            "  World_War_I: pre-selected 23/9931 properties.\n",
            "    23 included in dataset\n",
            "people\n",
            "  Aaron_Sorkin: pre-selected 24/42 properties.\n",
            "    24 included in dataset\n",
            "  Abraham_Maslow: pre-selected 12/12 properties.\n",
            "    12 included in dataset\n",
            "  Adam_Smith: pre-selected 27/72 properties.\n",
            "    27 included in dataset\n",
            "  Al-Ghazali: pre-selected 10/16 properties.\n",
            "    10 included in dataset\n",
            "  Al_Capone: pre-selected 20/20 properties.\n",
            "    20 included in dataset\n",
            "  Aldous_Huxley: pre-selected 28/53 properties.\n",
            "    28 included in dataset\n",
            "  Aleksandr_Solzhenitsyn: pre-selected 18/28 properties.\n",
            "    18 included in dataset\n",
            "  Alexander_Dubček: pre-selected 14/18 properties.\n",
            "    14 included in dataset\n",
            "  Alexander_Nevsky: pre-selected 14/14 properties.\n",
            "    14 included in dataset\n",
            "  Alexander_Pushkin: pre-selected 20/51 properties.\n",
            "    20 included in dataset\n",
            "  Alexander_von_Humboldt: pre-selected 24/32 properties.\n",
            "    24 included in dataset\n",
            "  Alfred_the_Great: pre-selected 19/28 properties.\n",
            "    19 included in dataset\n",
            "  Alfredo_Di_Stéfano: pre-selected 22/44 properties.\n",
            "    22 included in dataset\n",
            "  Alfredo_Stroessner: pre-selected 13/14 properties.\n",
            "    13 included in dataset\n",
            "  Ali: pre-selected 34/91 properties.\n",
            "    34 included in dataset\n",
            "  Alvin_Ailey: pre-selected 13/13 properties.\n",
            "    13 included in dataset\n",
            "  Ammianus_Marcellinus: pre-selected 6/8 properties.\n",
            "  Andrea_Bocelli: pre-selected 33/52 properties.\n",
            "    33 included in dataset\n",
            "  Andreas_Vesalius: pre-selected 17/19 properties.\n",
            "    17 included in dataset\n",
            "  André_Gide: pre-selected 13/15 properties.\n",
            "    13 included in dataset\n",
            "  Anja_Andersen: pre-selected 3/4 properties.\n",
            "  Anna_Pavlova: pre-selected 1/1 properties.\n",
            "  Antonio_Gramsci: pre-selected 29/41 properties.\n",
            "    29 included in dataset\n",
            "  Archimedes: pre-selected 16/23 properties.\n",
            "    16 included in dataset\n",
            "  Archytas: pre-selected 8/9 properties.\n",
            "    8 included in dataset\n",
            "  Aretha_Franklin: pre-selected 50/158 properties.\n",
            "    50 included in dataset\n",
            "  Aristophanes: pre-selected 12/20 properties.\n",
            "    12 included in dataset\n",
            "  Armen_Dzhigarkhanyan: pre-selected 15/43 properties.\n",
            "    15 included in dataset\n",
            "  Arnold_Schoenberg: pre-selected 8/8 properties.\n",
            "    8 included in dataset\n",
            "  Arnold_Sommerfeld: pre-selected 33/89 properties.\n",
            "    33 included in dataset\n",
            "  Arthur_Conan_Doyle: pre-selected 32/135 properties.\n",
            "    32 included in dataset\n",
            "  Arthur_Miller: pre-selected 33/52 properties.\n",
            "    33 included in dataset\n",
            "  Arun_Shourie: pre-selected 20/24 properties.\n",
            "    20 included in dataset\n",
            "  Attila: pre-selected 18/22 properties.\n",
            "    18 included in dataset\n",
            "  Auguste_and_Louis_Lumière: pre-selected 26/32 properties.\n",
            "    26 included in dataset\n",
            "  Augustin-Jean_Fresnel: pre-selected 8/9 properties.\n",
            "    8 included in dataset\n",
            "  Augustine_of_Hippo: pre-selected 38/89 properties.\n",
            "    38 included in dataset\n",
            "  Avicenna: pre-selected 23/29 properties.\n",
            "    23 included in dataset\n",
            "  Axel_Paulsen: pre-selected 3/3 properties.\n",
            "  Ayrton_Senna: pre-selected 41/192 properties.\n",
            "    41 included in dataset\n",
            "  B._R._Ambedkar: pre-selected 43/50 properties.\n",
            "    43 included in dataset\n",
            "  Baal_Shem_Tov: pre-selected 1/1 properties.\n",
            "  Babe_Ruth: pre-selected 15/16 properties.\n",
            "    15 included in dataset\n",
            "  Babur: pre-selected 33/65 properties.\n",
            "    33 included in dataset\n",
            "  Barack_Obama: pre-selected 66/2019 properties.\n",
            "    66 included in dataset\n",
            "  Barbara_W._Tuchman: pre-selected 14/14 properties.\n",
            "    14 included in dataset\n",
            "  Basava: pre-selected 1/1 properties.\n",
            "  Ben_Ainslie: pre-selected 7/8 properties.\n",
            "  Ben_Hogan: pre-selected 9/17 properties.\n",
            "    9 included in dataset\n",
            "  Benito_Juárez: pre-selected 29/46 properties.\n",
            "    29 included in dataset\n",
            "  Benito_Mussolini: pre-selected 71/167 properties.\n",
            "    71 included in dataset\n",
            "  Benjamin_Disraeli: pre-selected 37/142 properties.\n",
            "    37 included in dataset\n",
            "  Benjamin_Franklin: pre-selected 49/61 properties.\n",
            "    49 included in dataset\n",
            "  Bernard_Hinault: pre-selected 4/6 properties.\n",
            "  Bertolt_Brecht: pre-selected 33/61 properties.\n",
            "    33 included in dataset\n",
            "  Bill_Gates: pre-selected 32/37 properties.\n",
            "    32 included in dataset\n",
            "  Billy_Wilder: pre-selected 19/96 properties.\n",
            "    19 included in dataset\n",
            "  Billy_the_Kid: pre-selected 17/19 properties.\n",
            "    17 included in dataset\n",
            "  Blaise_Pascal: pre-selected 19/21 properties.\n",
            "    19 included in dataset\n",
            "  Bob_Dylan: pre-selected 72/709 properties.\n",
            "    72 included in dataset\n",
            "  Bob_Fosse: pre-selected 26/30 properties.\n",
            "    26 included in dataset\n",
            "  Bob_Marley: pre-selected 56/132 properties.\n",
            "    56 included in dataset\n",
            "  Bohdan_Khmelnytsky: pre-selected 12/29 properties.\n",
            "    12 included in dataset\n",
            "  Bonaventura_Cavalieri: pre-selected 12/15 properties.\n",
            "    12 included in dataset\n",
            "  Bonnie_Blair: pre-selected 10/13 properties.\n",
            "    10 included in dataset\n",
            "  Boris_Christoff: pre-selected 12/12 properties.\n",
            "    12 included in dataset\n",
            "  Brigitte_Bardot: pre-selected 23/59 properties.\n",
            "    23 included in dataset\n",
            "  Brontë_family: pre-selected 6/7 properties.\n",
            "  Bruce_Lee: pre-selected 41/59 properties.\n",
            "    41 included in dataset\n",
            "  Bruno_Latour: pre-selected 26/32 properties.\n",
            "    26 included in dataset\n",
            "  Béla_Bartók: pre-selected 6/7 properties.\n",
            "  C._V._Raman: pre-selected 27/34 properties.\n",
            "    27 included in dataset\n",
            "  C._Wright_Mills: pre-selected 5/6 properties.\n",
            "  Cai_Lun: pre-selected 6/8 properties.\n",
            "  Camillo_Benso,_Count_of_Cavour: pre-selected 15/17 properties.\n",
            "    15 included in dataset\n",
            "  Caravaggio: pre-selected 19/91 properties.\n",
            "    19 included in dataset\n",
            "  Cardinal_Mazarin: pre-selected 14/18 properties.\n",
            "    14 included in dataset\n",
            "  Carl_Gustav_Jacob_Jacobi: pre-selected 19/34 properties.\n",
            "    19 included in dataset\n",
            "  Carl_Sagan: pre-selected 44/64 properties.\n",
            "    44 included in dataset\n",
            "  Carlos_Saura: pre-selected 23/83 properties.\n",
            "    23 included in dataset\n",
            "  Carlos_Slim: pre-selected 28/29 properties.\n",
            "    28 included in dataset\n",
            "  Cary_Grant: pre-selected 23/85 properties.\n",
            "    23 included in dataset\n",
            "  Casimir_Funk: pre-selected 10/10 properties.\n",
            "    10 included in dataset\n",
            "  Catherine_the_Great: pre-selected 25/31 properties.\n",
            "    25 included in dataset\n",
            "  Cecil_Rhodes: pre-selected 26/32 properties.\n",
            "    26 included in dataset\n",
            "  Chanakya: pre-selected 12/16 properties.\n",
            "    12 included in dataset\n",
            "  Charles_Babbage: pre-selected 17/17 properties.\n",
            "    17 included in dataset\n",
            "  Charles_de_Gaulle: pre-selected 51/102 properties.\n",
            "    51 included in dataset\n",
            "  Chinua_Achebe: pre-selected 13/20 properties.\n",
            "    13 included in dataset\n",
            "  Christian_Michelsen: pre-selected 23/54 properties.\n",
            "    23 included in dataset\n",
            "  Christine_de_Pizan: pre-selected 6/8 properties.\n",
            "  Christopher_Wren: pre-selected 18/67 properties.\n",
            "    18 included in dataset\n",
            "  Claude_Debussy: pre-selected 7/7 properties.\n",
            "  Clement_Attlee: pre-selected 43/220 properties.\n",
            "    43 included in dataset\n",
            "  Clint_Eastwood: pre-selected 48/175 properties.\n",
            "    48 included in dataset\n",
            "  Clovis_I: pre-selected 24/36 properties.\n",
            "    24 included in dataset\n",
            "  Cole_Porter: pre-selected 22/115 properties.\n",
            "    22 included in dataset\n",
            "  Croesus: pre-selected 9/9 properties.\n",
            "    9 included in dataset\n",
            "  Dan_Bricklin: pre-selected 10/11 properties.\n",
            "    10 included in dataset\n",
            "  Daniel_Defoe: pre-selected 14/24 properties.\n",
            "    14 included in dataset\n",
            "  Daniel_Ortega: pre-selected 27/35 properties.\n",
            "    27 included in dataset\n",
            "  Dante_Alighieri: pre-selected 34/40 properties.\n",
            "    34 included in dataset\n",
            "  David_Beckham: pre-selected 22/42 properties.\n",
            "    22 included in dataset\n",
            "  David_Copperfield: pre-selected 14/18 properties.\n",
            "    14 included in dataset\n",
            "  David_Coverdale: pre-selected 22/37 properties.\n",
            "    22 included in dataset\n",
            "  David_Hume: pre-selected 30/79 properties.\n",
            "    30 included in dataset\n",
            "  David_Lean: pre-selected 28/77 properties.\n",
            "    28 included in dataset\n",
            "  David_McCullough: pre-selected 20/31 properties.\n",
            "    20 included in dataset\n",
            "  Dean_Martin: pre-selected 32/110 properties.\n",
            "    32 included in dataset\n",
            "  Diego_Velázquez: pre-selected 15/71 properties.\n",
            "    15 included in dataset\n",
            "  Don_Bradman: pre-selected 5/7 properties.\n",
            "  Donald_Knuth: pre-selected 28/40 properties.\n",
            "    28 included in dataset\n",
            "  Donald_Trump: pre-selected 87/2392 properties.\n",
            "    87 included in dataset\n",
            "  Dōgen: pre-selected 11/13 properties.\n",
            "    11 included in dataset\n",
            "  Eddy_Merckx: pre-selected 6/8 properties.\n",
            "  Edgar_Degas: pre-selected 16/37 properties.\n",
            "    16 included in dataset\n",
            "  Edmund_Burke: pre-selected 27/38 properties.\n",
            "    27 included in dataset\n",
            "  Edmund_Hillary: pre-selected 15/15 properties.\n",
            "    15 included in dataset\n",
            "  Edmund_Kean: pre-selected 9/11 properties.\n",
            "    9 included in dataset\n",
            "  Edvard_Grieg: pre-selected 20/23 properties.\n",
            "    20 included in dataset\n",
            "  Edward_I_of_England: pre-selected 26/56 properties.\n",
            "    26 included in dataset\n",
            "  Edward_R._Murrow: pre-selected 30/31 properties.\n",
            "    30 included in dataset\n",
            "  Edward_Said: pre-selected 20/21 properties.\n",
            "    20 included in dataset\n",
            "  Edward_Thorndike: pre-selected 5/5 properties.\n",
            "  Eino_Leino: pre-selected 9/12 properties.\n",
            "    9 included in dataset\n",
            "  Elias_Howe: pre-selected 8/9 properties.\n",
            "    8 included in dataset\n",
            "  Elton_John: pre-selected 61/384 properties.\n",
            "    61 included in dataset\n",
            "  Eminem: pre-selected 44/449 properties.\n",
            "    44 included in dataset\n",
            "  Emma_Goldman: pre-selected 20/22 properties.\n",
            "    20 included in dataset\n",
            "  Emperor_Meiji: pre-selected 25/32 properties.\n",
            "    25 included in dataset\n",
            "  Epicurus: pre-selected 16/20 properties.\n",
            "    16 included in dataset\n",
            "  Ernst_Cassirer: pre-selected 19/25 properties.\n",
            "    19 included in dataset\n",
            "  Erving_Goffman: pre-selected 25/28 properties.\n",
            "    25 included in dataset\n",
            "  Erwin_Panofsky: pre-selected 9/9 properties.\n",
            "    9 included in dataset\n",
            "  Euclid: pre-selected 9/16 properties.\n",
            "    9 included in dataset\n",
            "  Eugène_Delacroix: pre-selected 17/42 properties.\n",
            "    17 included in dataset\n",
            "  Ferdinand_II_of_Aragon: pre-selected 19/32 properties.\n",
            "    19 included in dataset\n",
            "  Fernandel: pre-selected 20/76 properties.\n",
            "    20 included in dataset\n",
            "  Fidel_Castro: pre-selected 53/84 properties.\n",
            "    53 included in dataset\n",
            "  Francis_Drake: pre-selected 11/21 properties.\n",
            "    11 included in dataset\n",
            "  Frank_Capra: pre-selected 36/119 properties.\n",
            "    36 included in dataset\n",
            "  Frank_Gehry: pre-selected 8/34 properties.\n",
            "    8 included in dataset\n",
            "  Frank_Lloyd: pre-selected 29/144 properties.\n",
            "    29 included in dataset\n",
            "  Frank_Zappa: pre-selected 62/254 properties.\n",
            "    62 included in dataset\n",
            "  Franklin_D._Roosevelt: pre-selected 61/536 properties.\n",
            "    61 included in dataset\n",
            "  Franz_Joseph_I_of_Austria: pre-selected 28/43 properties.\n",
            "    28 included in dataset\n",
            "  Franz_Kafka: pre-selected 24/73 properties.\n",
            "    24 included in dataset\n",
            "  François_Truffaut: pre-selected 33/86 properties.\n",
            "    33 included in dataset\n",
            "  Friedrich_Engels: pre-selected 31/38 properties.\n",
            "    31 included in dataset\n",
            "  Friedrich_Schiller: pre-selected 25/31 properties.\n",
            "    25 included in dataset\n",
            "  Friedrich_Schleiermacher: pre-selected 23/32 properties.\n",
            "    23 included in dataset\n",
            "  Fritz_Lang: pre-selected 30/86 properties.\n",
            "    30 included in dataset\n",
            "  Gabriel_García_Márquez: pre-selected 26/48 properties.\n",
            "    26 included in dataset\n",
            "  Galen: pre-selected 8/9 properties.\n",
            "    8 included in dataset\n",
            "  Garry_Kasparov: pre-selected 10/12 properties.\n",
            "    10 included in dataset\n",
            "  Gary_Player: pre-selected 12/32 properties.\n",
            "    12 included in dataset\n",
            "  George_Boole: pre-selected 14/15 properties.\n",
            "    14 included in dataset\n",
            "  George_Cukor: pre-selected 19/72 properties.\n",
            "    19 included in dataset\n",
            "  George_W._Bush: pre-selected 60/1571 properties.\n",
            "    60 included in dataset\n",
            "  George_Westinghouse: pre-selected 13/14 properties.\n",
            "    13 included in dataset\n",
            "  Gian_Lorenzo_Bernini: pre-selected 21/97 properties.\n",
            "    21 included in dataset\n",
            "  Girolamo_Savonarola: pre-selected 11/13 properties.\n",
            "    11 included in dataset\n",
            "  Glenn_Miller: pre-selected 28/44 properties.\n",
            "    28 included in dataset\n",
            "  Gloria_Swanson: pre-selected 25/88 properties.\n",
            "    25 included in dataset\n",
            "  Gotthold_Ephraim_Lessing: pre-selected 21/28 properties.\n",
            "    21 included in dataset\n",
            "  Graham_Greene: pre-selected 30/71 properties.\n",
            "    30 included in dataset\n",
            "  Gregor_Mendel: pre-selected 10/14 properties.\n",
            "    10 included in dataset\n",
            "  Gregory_of_Nazianzus: pre-selected 9/13 properties.\n",
            "    9 included in dataset\n",
            "  Grigori_Perelman: pre-selected 8/9 properties.\n",
            "    8 included in dataset\n",
            "  Guillaume_Apollinaire: pre-selected 10/10 properties.\n",
            "    10 included in dataset\n",
            "  Gustav_Fechner: pre-selected 9/13 properties.\n",
            "    9 included in dataset\n",
            "  Gustave_Flaubert: pre-selected 14/20 properties.\n",
            "    14 included in dataset\n",
            "  Hannah_Arendt: pre-selected 32/42 properties.\n",
            "    32 included in dataset\n",
            "  Hannibal: pre-selected 13/45 properties.\n",
            "    13 included in dataset\n",
            "  Haruki_Murakami: pre-selected 17/46 properties.\n",
            "    17 included in dataset\n",
            "  Hastings_Banda: pre-selected 15/19 properties.\n",
            "    15 included in dataset\n",
            "  Hector_Berlioz: pre-selected 4/4 properties.\n",
            "  Heinrich_Heine: pre-selected 16/17 properties.\n",
            "    16 included in dataset\n",
            "  Henri_Poincaré: pre-selected 28/32 properties.\n",
            "    28 included in dataset\n",
            "  Henry_Cavendish: pre-selected 10/12 properties.\n",
            "    10 included in dataset\n",
            "  Henry_David_Thoreau: pre-selected 21/29 properties.\n",
            "    21 included in dataset\n",
            "  Henry_IV_of_France: pre-selected 24/49 properties.\n",
            "    24 included in dataset\n",
            "  Henry_Morton_Stanley: pre-selected 9/10 properties.\n",
            "    9 included in dataset\n",
            "  Heraclitus: pre-selected 12/16 properties.\n",
            "    12 included in dataset\n",
            "  Herbert_Spencer: pre-selected 21/29 properties.\n",
            "    21 included in dataset\n",
            "  Herman_Melville: pre-selected 28/45 properties.\n",
            "    28 included in dataset\n",
            "  Hermann_Heinrich_Gossen: pre-selected 6/10 properties.\n",
            "  Hermann_Maier: pre-selected 14/18 properties.\n",
            "    14 included in dataset\n",
            "  Hermann_von_Helmholtz: pre-selected 22/69 properties.\n",
            "    22 included in dataset\n",
            "  Hernán_Cortés: pre-selected 31/38 properties.\n",
            "    31 included in dataset\n",
            "  Heydar_Aliyev: pre-selected 38/93 properties.\n",
            "    38 included in dataset\n",
            "  Hirohito: pre-selected 26/35 properties.\n",
            "    26 included in dataset\n",
            "  Hu_Jintao: pre-selected 35/52 properties.\n",
            "    35 included in dataset\n",
            "  Hugo_Chávez: pre-selected 42/87 properties.\n",
            "    42 included in dataset\n",
            "  Huineng: pre-selected 10/13 properties.\n",
            "    10 included in dataset\n",
            "  Huldrych_Zwingli: pre-selected 1/1 properties.\n",
            "  Humayun: pre-selected 27/42 properties.\n",
            "    27 included in dataset\n",
            "  Hypatia: pre-selected 10/14 properties.\n",
            "    10 included in dataset\n",
            "  Ignacy_Łukasiewicz: pre-selected 9/11 properties.\n",
            "    9 included in dataset\n",
            "  Ilf_and_Petrov: pre-selected 10/12 properties.\n",
            "    10 included in dataset\n",
            "  Irving_Berlin: pre-selected 44/132 properties.\n",
            "    44 included in dataset\n",
            "  Isaac_Barrow: pre-selected 9/9 properties.\n",
            "    9 included in dataset\n",
            "  Isaac_Newton: pre-selected 34/54 properties.\n",
            "    34 included in dataset\n",
            "  Isaiah_Berlin: pre-selected 27/34 properties.\n",
            "    27 included in dataset\n",
            "  Ivan_Bunin: pre-selected 8/19 properties.\n",
            "    8 included in dataset\n",
            "  Ivan_III_of_Russia: pre-selected 25/34 properties.\n",
            "    25 included in dataset\n",
            "  Ivan_Mazepa: pre-selected 15/24 properties.\n",
            "    15 included in dataset\n",
            "  J._M._Coetzee: pre-selected 10/28 properties.\n",
            "    10 included in dataset\n",
            "  J._Paul_Getty: pre-selected 11/11 properties.\n",
            "    11 included in dataset\n",
            "  J._R._R._Tolkien: pre-selected 50/142 properties.\n",
            "    50 included in dataset\n",
            "  Jack_Dempsey: pre-selected 9/10 properties.\n",
            "    9 included in dataset\n",
            "  Jack_Kerouac: pre-selected 25/49 properties.\n",
            "    25 included in dataset\n",
            "  Jack_Nicholson: pre-selected 31/84 properties.\n",
            "    31 included in dataset\n",
            "  Jack_Nicklaus: pre-selected 8/32 properties.\n",
            "    8 included in dataset\n",
            "  Jacques_Derrida: pre-selected 27/67 properties.\n",
            "    27 included in dataset\n",
            "  James_Buchanan_Duke: pre-selected 20/21 properties.\n",
            "    20 included in dataset\n",
            "  James_Cagney: pre-selected 26/81 properties.\n",
            "    26 included in dataset\n",
            "  James_Joyce: pre-selected 29/54 properties.\n",
            "    29 included in dataset\n",
            "  James_Prescott_Joule: pre-selected 7/9 properties.\n",
            "  James_VI_and_I: pre-selected 32/47 properties.\n",
            "    32 included in dataset\n",
            "  Jan-Ove_Waldner: pre-selected 5/6 properties.\n",
            "  Jan_van_Eyck: pre-selected 11/20 properties.\n",
            "    11 included in dataset\n",
            "  Jane_Goodall: pre-selected 18/21 properties.\n",
            "    18 included in dataset\n",
            "  Jaroslav_Hašek: pre-selected 14/16 properties.\n",
            "    14 included in dataset\n",
            "  Jean-Claude_Killy: pre-selected 16/19 properties.\n",
            "    16 included in dataset\n",
            "  Jean-Paul_Sartre: pre-selected 37/83 properties.\n",
            "    37 included in dataset\n",
            "  Jean_le_Rond_d'Alembert: pre-selected 3/3 properties.\n",
            "  Jerome: pre-selected 20/23 properties.\n",
            "    20 included in dataset\n",
            "  Joan_of_Arc: pre-selected 19/30 properties.\n",
            "    19 included in dataset\n",
            "  Joe_Hisaishi: pre-selected 18/84 properties.\n",
            "    18 included in dataset\n",
            "  John,_King_of_England: pre-selected 21/38 properties.\n",
            "    21 included in dataset\n",
            "  John_Adams: pre-selected 43/79 properties.\n",
            "    43 included in dataset\n",
            "  John_B._Watson: pre-selected 13/13 properties.\n",
            "    13 included in dataset\n",
            "  John_Bardeen: pre-selected 26/30 properties.\n",
            "    26 included in dataset\n",
            "  John_Calvin: pre-selected 14/15 properties.\n",
            "    14 included in dataset\n",
            "  John_D._Rockefeller: pre-selected 21/23 properties.\n",
            "    21 included in dataset\n",
            "  John_Dalton: pre-selected 8/9 properties.\n",
            "    8 included in dataset\n",
            "  John_Dewey: pre-selected 35/55 properties.\n",
            "    35 included in dataset\n",
            "  John_F._Kennedy: pre-selected 57/386 properties.\n",
            "    57 included in dataset\n",
            "  John_Jacob_Astor: pre-selected 19/21 properties.\n",
            "    19 included in dataset\n",
            "  John_Lee_Hooker: pre-selected 27/53 properties.\n",
            "    27 included in dataset\n",
            "  John_Logie_Baird: pre-selected 11/11 properties.\n",
            "    11 included in dataset\n",
            "  John_Milton: pre-selected 23/26 properties.\n",
            "    23 included in dataset\n",
            "  John_Stuart_Mill: pre-selected 20/44 properties.\n",
            "    20 included in dataset\n",
            "  John_Updike: pre-selected 17/61 properties.\n",
            "    17 included in dataset\n",
            "  John_the_Apostle: pre-selected 7/7 properties.\n",
            "  Jon_Lord: pre-selected 35/55 properties.\n",
            "    35 included in dataset\n",
            "  Jorge_Luis_Borges: pre-selected 24/70 properties.\n",
            "    24 included in dataset\n",
            "  Joseph_Goebbels: pre-selected 36/48 properties.\n",
            "    36 included in dataset\n",
            "  Joseph_Priestley: pre-selected 22/27 properties.\n",
            "    22 included in dataset\n",
            "  Joseph_Smith: pre-selected 23/28 properties.\n",
            "    23 included in dataset\n",
            "  Joseph_Stalin: pre-selected 63/150 properties.\n",
            "    63 included in dataset\n",
            "  Juan_Manuel_Fangio: pre-selected 28/188 properties.\n",
            "    28 included in dataset\n",
            "  Justus_von_Liebig: pre-selected 21/33 properties.\n",
            "    21 included in dataset\n",
            "  Józef_Piłsudski: pre-selected 39/64 properties.\n",
            "    39 included in dataset\n",
            "  Kahlil_Gibran: pre-selected 19/23 properties.\n",
            "    19 included in dataset\n",
            "  Karch_Kiraly: pre-selected 7/9 properties.\n",
            "  Karen_Blixen: pre-selected 21/25 properties.\n",
            "    21 included in dataset\n",
            "  Kazimir_Malevich: pre-selected 15/24 properties.\n",
            "    15 included in dataset\n",
            "  Keith_Richards: pre-selected 47/78 properties.\n",
            "    47 included in dataset\n",
            "  Kipchoge_Keino: pre-selected 8/10 properties.\n",
            "    8 included in dataset\n",
            "  Klemens_von_Metternich: pre-selected 11/14 properties.\n",
            "    11 included in dataset\n",
            "  Kurt_Gödel: pre-selected 23/41 properties.\n",
            "    23 included in dataset\n",
            "  Kūkai: pre-selected 6/7 properties.\n",
            "  Larisa_Latynina: pre-selected 15/21 properties.\n",
            "    15 included in dataset\n",
            "  Larry_Bird: pre-selected 21/43 properties.\n",
            "    21 included in dataset\n",
            "  Larry_Page: pre-selected 11/11 properties.\n",
            "    11 included in dataset\n",
            "  Lars_von_Trier: pre-selected 29/63 properties.\n",
            "    29 included in dataset\n",
            "  Lata_Mangeshkar: pre-selected 35/55 properties.\n",
            "    35 included in dataset\n",
            "  Laurence_Olivier: pre-selected 37/95 properties.\n",
            "    37 included in dataset\n",
            "  Le_Corbusier: pre-selected 17/55 properties.\n",
            "    17 included in dataset\n",
            "  Leo_III_the_Isaurian: pre-selected 15/15 properties.\n",
            "    15 included in dataset\n",
            "  Leon_Festinger: pre-selected 8/8 properties.\n",
            "    8 included in dataset\n",
            "  Lewis_Carroll: pre-selected 38/82 properties.\n",
            "    38 included in dataset\n",
            "  Li_Ning: pre-selected 8/11 properties.\n",
            "    8 included in dataset\n",
            "  Lin_Dan: pre-selected 2/2 properties.\n",
            "  Lise_Meitner: pre-selected 28/31 properties.\n",
            "    28 included in dataset\n",
            "  Louis_de_Funès: pre-selected 26/88 properties.\n",
            "    26 included in dataset\n",
            "  Lu_Xun: pre-selected 16/20 properties.\n",
            "    16 included in dataset\n",
            "  Luciano_Pavarotti: pre-selected 19/21 properties.\n",
            "    19 included in dataset\n",
            "  Lucrezia_Borgia: pre-selected 11/14 properties.\n",
            "    11 included in dataset\n",
            "  Ludmilla_Tourischeva: pre-selected 15/18 properties.\n",
            "    15 included in dataset\n",
            "  Ludwig_Feuerbach: pre-selected 20/24 properties.\n",
            "    20 included in dataset\n",
            "  Ludwig_Mies_van_der_Rohe: pre-selected 9/29 properties.\n",
            "    9 included in dataset\n",
            "  Ludwig_Wittgenstein: pre-selected 32/80 properties.\n",
            "    32 included in dataset\n",
            "  Léon_Foucault: pre-selected 14/21 properties.\n",
            "    14 included in dataset\n",
            "  Mack_Sennett: pre-selected 32/311 properties.\n",
            "    32 included in dataset\n",
            "  Mae_West: pre-selected 22/36 properties.\n",
            "    22 included in dataset\n",
            "  Mao_Zedong: pre-selected 53/83 properties.\n",
            "    53 included in dataset\n",
            "  Marcel_Duchamp: pre-selected 24/28 properties.\n",
            "    24 included in dataset\n",
            "  Marcello_Mastroianni: pre-selected 21/131 properties.\n",
            "    21 included in dataset\n",
            "  Marco_Polo: pre-selected 12/14 properties.\n",
            "    12 included in dataset\n",
            "  Margaret_Thatcher: pre-selected 58/577 properties.\n",
            "    58 included in dataset\n",
            "  Maria_Theresa: pre-selected 20/34 properties.\n",
            "    20 included in dataset\n",
            "  Mariah_Carey: pre-selected 46/304 properties.\n",
            "    46 included in dataset\n",
            "  Marian_Anderson: pre-selected 10/10 properties.\n",
            "    10 included in dataset\n",
            "  Marie_Antoinette: pre-selected 17/19 properties.\n",
            "    17 included in dataset\n",
            "  Marie_Curie: pre-selected 29/35 properties.\n",
            "    29 included in dataset\n",
            "  Mario_Puzo: pre-selected 26/65 properties.\n",
            "    26 included in dataset\n",
            "  Mario_Vargas_Llosa: pre-selected 24/39 properties.\n",
            "    24 included in dataset\n",
            "  Mark_Spitz: pre-selected 13/15 properties.\n",
            "    13 included in dataset\n",
            "  Marlon_Brando: pre-selected 29/74 properties.\n",
            "    29 included in dataset\n",
            "  Martin_Buber: pre-selected 8/8 properties.\n",
            "    8 included in dataset\n",
            "  Martin_Scorsese: pre-selected 47/142 properties.\n",
            "    47 included in dataset\n",
            "  Martina_Navratilova: pre-selected 28/349 properties.\n",
            "    28 included in dataset\n",
            "  Marx_Brothers: pre-selected 7/7 properties.\n",
            "  Mary_Cassatt: pre-selected 21/29 properties.\n",
            "    21 included in dataset\n",
            "  Mary_Pickford: pre-selected 42/160 properties.\n",
            "    42 included in dataset\n",
            "  Masaccio: pre-selected 16/20 properties.\n",
            "    16 included in dataset\n",
            "  Matti_Nykänen: pre-selected 10/11 properties.\n",
            "    10 included in dataset\n",
            "  Max_Planck: pre-selected 35/54 properties.\n",
            "    35 included in dataset\n",
            "  Max_Steiner: pre-selected 12/313 properties.\n",
            "    12 included in dataset\n",
            "  Maximilian_I,_Holy_Roman_Emperor: pre-selected 19/24 properties.\n",
            "    19 included in dataset\n",
            "  Mehmed_IV: pre-selected 22/34 properties.\n",
            "    22 included in dataset\n",
            "  Menander: pre-selected 11/12 properties.\n",
            "    11 included in dataset\n",
            "  Meryl_Streep: pre-selected 25/96 properties.\n",
            "    25 included in dataset\n",
            "  Michael_Servetus: pre-selected 2/2 properties.\n",
            "  Michel_Foucault: pre-selected 32/87 properties.\n",
            "    32 included in dataset\n",
            "  Mikhail_Bakhtin: pre-selected 20/30 properties.\n",
            "    20 included in dataset\n",
            "  Miklós_Rózsa: pre-selected 14/108 properties.\n",
            "    14 included in dataset\n",
            "  Miles_Davis: pre-selected 42/161 properties.\n",
            "    42 included in dataset\n",
            "  Mobutu_Sese_Seko: pre-selected 28/74 properties.\n",
            "    28 included in dataset\n",
            "  Mohammed_bin_Salman: pre-selected 42/46 properties.\n",
            "    42 included in dataset\n",
            "  Mother_Jones: pre-selected 11/13 properties.\n",
            "    11 included in dataset\n",
            "  Muhammad_Ali: pre-selected 46/59 properties.\n",
            "    46 included in dataset\n",
            "  Muhammad_Ali_Jinnah: pre-selected 43/50 properties.\n",
            "    43 included in dataset\n",
            "  Muhammad_Ali_of_Egypt: pre-selected 20/46 properties.\n",
            "    20 included in dataset\n",
            "  Muslim_ibn_al-Hajjaj: pre-selected 4/6 properties.\n",
            "  Narendra_Modi: pre-selected 47/575 properties.\n",
            "    47 included in dataset\n",
            "  Neil_Armstrong: pre-selected 15/15 properties.\n",
            "    15 included in dataset\n",
            "  Nelson_Mandela: pre-selected 57/171 properties.\n",
            "    57 included in dataset\n",
            "  Ngũgĩ_wa_Thiong'o: pre-selected 14/22 properties.\n",
            "    14 included in dataset\n",
            "  Nicholas_of_Cusa: pre-selected 21/25 properties.\n",
            "    21 included in dataset\n",
            "  Nikita_Khrushchev: pre-selected 31/71 properties.\n",
            "    31 included in dataset\n",
            "  Niklas_Luhmann: pre-selected 17/19 properties.\n",
            "    17 included in dataset\n",
            "  Niklaus_Wirth: pre-selected 30/38 properties.\n",
            "    30 included in dataset\n",
            "  Nikolai_Rimsky-Korsakov: pre-selected 9/13 properties.\n",
            "    9 included in dataset\n",
            "  Nikolay_Karamzin: pre-selected 8/11 properties.\n",
            "    8 included in dataset\n",
            "  Nizami_Ganjavi: pre-selected 10/11 properties.\n",
            "    10 included in dataset\n",
            "  Noam_Chomsky: pre-selected 22/64 properties.\n",
            "    22 included in dataset\n",
            "  Orhan_Pamuk: pre-selected 7/14 properties.\n",
            "  Origen: pre-selected 17/22 properties.\n",
            "    17 included in dataset\n",
            "  Osama_bin_Laden: pre-selected 21/38 properties.\n",
            "    21 included in dataset\n",
            "  Osborne_Reynolds: pre-selected 15/16 properties.\n",
            "    15 included in dataset\n",
            "  Otto_Hahn: pre-selected 19/30 properties.\n",
            "    19 included in dataset\n",
            "  Ovid: pre-selected 10/14 properties.\n",
            "    10 included in dataset\n",
            "  Pablo_Escobar: pre-selected 13/13 properties.\n",
            "    13 included in dataset\n",
            "  Paracelsus: pre-selected 19/29 properties.\n",
            "    19 included in dataset\n",
            "  Paramahansa_Yogananda: pre-selected 6/6 properties.\n",
            "  Paul_Celan: pre-selected 14/22 properties.\n",
            "    14 included in dataset\n",
            "  Paul_Gauguin: pre-selected 11/50 properties.\n",
            "    11 included in dataset\n",
            "  Paul_Samuelson: pre-selected 16/20 properties.\n",
            "    16 included in dataset\n",
            "  Pearl_S._Buck: pre-selected 16/26 properties.\n",
            "    16 included in dataset\n",
            "  Pedro_Almodóvar: pre-selected 21/69 properties.\n",
            "    21 included in dataset\n",
            "  Pete_Sampras: pre-selected 15/76 properties.\n",
            "    15 included in dataset\n",
            "  Peter_Debye: pre-selected 20/21 properties.\n",
            "    20 included in dataset\n",
            "  Peter_Paul_Rubens: pre-selected 24/97 properties.\n",
            "    24 included in dataset\n",
            "  Phidias: pre-selected 1/1 properties.\n",
            "  Philip_V_of_Spain: pre-selected 27/41 properties.\n",
            "    27 included in dataset\n",
            "  Pierre_Bourdieu: pre-selected 19/25 properties.\n",
            "    19 included in dataset\n",
            "  Pierre_de_Coubertin: pre-selected 14/15 properties.\n",
            "    14 included in dataset\n",
            "  Piet_Mondrian: pre-selected 18/24 properties.\n",
            "    18 included in dataset\n",
            "  Pink_Floyd: pre-selected 46/262 properties.\n",
            "    46 included in dataset\n",
            "  Pope_Boniface_VIII: pre-selected 11/13 properties.\n",
            "    11 included in dataset\n",
            "  Pope_Clement_VII: pre-selected 12/13 properties.\n",
            "    12 included in dataset\n",
            "  Pope_Gregory_VII: pre-selected 15/20 properties.\n",
            "    15 included in dataset\n",
            "  Porfirio_Díaz: pre-selected 27/65 properties.\n",
            "    27 included in dataset\n",
            "  Prince_Eugene_of_Savoy: pre-selected 8/34 properties.\n",
            "    8 included in dataset\n",
            "  Rainer_Maria_Rilke: pre-selected 20/26 properties.\n",
            "    20 included in dataset\n",
            "  Rainer_Werner_Fassbinder: pre-selected 32/80 properties.\n",
            "    32 included in dataset\n",
            "  Raja_Ravi_Varma: pre-selected 14/20 properties.\n",
            "    14 included in dataset\n",
            "  Ramakrishna: pre-selected 7/7 properties.\n",
            "  Ramanuja: pre-selected 2/2 properties.\n",
            "  Rashi: pre-selected 14/18 properties.\n",
            "    14 included in dataset\n",
            "  Raymond_Cattell: pre-selected 15/16 properties.\n",
            "    15 included in dataset\n",
            "  Richard_Feynman: pre-selected 29/44 properties.\n",
            "    29 included in dataset\n",
            "  Richard_Neutra: pre-selected 10/23 properties.\n",
            "    10 included in dataset\n",
            "  Richard_Strauss: pre-selected 13/14 properties.\n",
            "    13 included in dataset\n",
            "  Robert_Andrews_Millikan: pre-selected 9/16 properties.\n",
            "    9 included in dataset\n",
            "  Robert_Burns_Woodward: pre-selected 29/46 properties.\n",
            "    29 included in dataset\n",
            "  Robert_Fulton: pre-selected 21/23 properties.\n",
            "    21 included in dataset\n",
            "  Robert_Jarvik: pre-selected 15/15 properties.\n",
            "    15 included in dataset\n",
            "  Robert_Louis_Stevenson: pre-selected 35/69 properties.\n",
            "    35 included in dataset\n",
            "  Robert_Walpole: pre-selected 36/54 properties.\n",
            "    36 included in dataset\n",
            "  Robert_Wise: pre-selected 24/74 properties.\n",
            "    24 included in dataset\n",
            "  Rod_Laver: pre-selected 16/89 properties.\n",
            "    16 included in dataset\n",
            "  Roger_Federer: pre-selected 16/121 properties.\n",
            "    16 included in dataset\n",
            "  Ronnie_James_Dio: pre-selected 25/37 properties.\n",
            "    25 included in dataset\n",
            "  Rosa_Parks: pre-selected 17/18 properties.\n",
            "    17 included in dataset\n",
            "  Roy_J._Plunkett: pre-selected 8/8 properties.\n",
            "    8 included in dataset\n",
            "  Rumi: pre-selected 19/23 properties.\n",
            "    19 included in dataset\n",
            "  Ryūnosuke_Akutagawa: pre-selected 15/23 properties.\n",
            "    15 included in dataset\n",
            "  Sam_Walton: pre-selected 28/28 properties.\n",
            "    28 included in dataset\n",
            "  Samuel_Beckett: pre-selected 31/51 properties.\n",
            "    31 included in dataset\n",
            "  Samuel_Johnson: pre-selected 18/18 properties.\n",
            "    18 included in dataset\n",
            "  Samuel_Morse: pre-selected 17/17 properties.\n",
            "    17 included in dataset\n",
            "  Santiago_Ramón_y_Cajal: pre-selected 14/31 properties.\n",
            "    14 included in dataset\n",
            "  Satyajit_Ray: pre-selected 66/219 properties.\n",
            "    66 included in dataset\n",
            "  Scipio_Africanus: pre-selected 17/26 properties.\n",
            "    17 included in dataset\n",
            "  Scott_Moir: pre-selected 22/28 properties.\n",
            "    22 included in dataset\n",
            "  Sebastian_Coe: pre-selected 22/24 properties.\n",
            "    22 included in dataset\n",
            "  Seki_Takakazu: pre-selected 5/7 properties.\n",
            "  Selim_I: pre-selected 23/42 properties.\n",
            "    23 included in dataset\n",
            "  Semiramis: pre-selected 1/1 properties.\n",
            "  Seneca_the_Younger: pre-selected 18/39 properties.\n",
            "    18 included in dataset\n",
            "  Sergei_Belov: pre-selected 16/31 properties.\n",
            "    16 included in dataset\n",
            "  Sergei_Eisenstein: pre-selected 27/40 properties.\n",
            "    27 included in dataset\n",
            "  Sergey_Brin: pre-selected 23/25 properties.\n",
            "    23 included in dataset\n",
            "  Sitting_Bull: pre-selected 20/31 properties.\n",
            "    20 included in dataset\n",
            "  Slobodan_Milošević: pre-selected 26/46 properties.\n",
            "    26 included in dataset\n",
            "  Srinivasa_Ramanujan: pre-selected 16/33 properties.\n",
            "    16 included in dataset\n",
            "  Steffi_Graf: pre-selected 24/128 properties.\n",
            "    24 included in dataset\n",
            "  Stephanie_Kwolek: pre-selected 10/17 properties.\n",
            "    10 included in dataset\n",
            "  Stephen_I_of_Hungary: pre-selected 18/20 properties.\n",
            "    18 included in dataset\n",
            "  Stephen_King: pre-selected 45/326 properties.\n",
            "    45 included in dataset\n",
            "  Stevie_Ray_Vaughan: pre-selected 32/45 properties.\n",
            "    32 included in dataset\n",
            "  Suharto: pre-selected 33/300 properties.\n",
            "    33 included in dataset\n",
            "  Sukarno: pre-selected 42/341 properties.\n",
            "    42 included in dataset\n",
            "  T._E._Lawrence: pre-selected 16/22 properties.\n",
            "    16 included in dataset\n",
            "  Tacitus: pre-selected 5/5 properties.\n",
            "  Tamara_Karsavina: pre-selected 10/11 properties.\n",
            "    10 included in dataset\n",
            "  Theodor_Mommsen: pre-selected 13/18 properties.\n",
            "    13 included in dataset\n",
            "  Thomas_Aquinas: pre-selected 37/86 properties.\n",
            "    37 included in dataset\n",
            "  Thomas_Kuhn: pre-selected 25/28 properties.\n",
            "    25 included in dataset\n",
            "  Tina_Turner: pre-selected 51/130 properties.\n",
            "    51 included in dataset\n",
            "  Tokugawa_Ieyasu: pre-selected 36/73 properties.\n",
            "    36 included in dataset\n",
            "  Tomás_de_Torquemada: pre-selected 7/10 properties.\n",
            "  Truman_Capote: pre-selected 29/45 properties.\n",
            "    29 included in dataset\n",
            "  Tycho_Brahe: pre-selected 15/18 properties.\n",
            "    15 included in dataset\n",
            "  Ulysses_S._Grant: pre-selected 50/244 properties.\n",
            "    50 included in dataset\n",
            "  Ursula_K._Le_Guin: pre-selected 13/60 properties.\n",
            "    13 included in dataset\n",
            "  Ursula_von_der_Leyen: pre-selected 32/100 properties.\n",
            "    32 included in dataset\n",
            "  Victor_Francis_Hess: pre-selected 13/14 properties.\n",
            "    13 included in dataset\n",
            "  Victor_Vasarely: pre-selected 9/10 properties.\n",
            "    9 included in dataset\n",
            "  Virgil: pre-selected 14/17 properties.\n",
            "    14 included in dataset\n",
            "  Virginia_Woolf: pre-selected 25/52 properties.\n",
            "    25 included in dataset\n",
            "  Vitruvius: pre-selected 5/5 properties.\n",
            "  Vladimir_Arnold: pre-selected 20/41 properties.\n",
            "    20 included in dataset\n",
            "  Vladimir_Vysotsky: pre-selected 26/34 properties.\n",
            "    26 included in dataset\n",
            "  W._B._Yeats: pre-selected 20/29 properties.\n",
            "    20 included in dataset\n",
            "  Walt_Disney: pre-selected 52/652 properties.\n",
            "    52 included in dataset\n",
            "  Walt_Whitman: pre-selected 16/18 properties.\n",
            "    16 included in dataset\n",
            "  Wernher_von_Braun: pre-selected 27/30 properties.\n",
            "    27 included in dataset\n",
            "  Wilhelm_Eduard_Weber: pre-selected 21/25 properties.\n",
            "    21 included in dataset\n",
            "  Wilhelm_Röntgen: pre-selected 23/32 properties.\n",
            "    23 included in dataset\n",
            "  Wilhelm_Wundt: pre-selected 24/37 properties.\n",
            "    24 included in dataset\n",
            "  William_Harvey: pre-selected 13/15 properties.\n",
            "    13 included in dataset\n",
            "  William_III_of_England: pre-selected 20/38 properties.\n",
            "    20 included in dataset\n",
            "  William_Le_Baron_Jenney: pre-selected 13/19 properties.\n",
            "    13 included in dataset\n",
            "  William_Pitt_the_Younger: pre-selected 35/126 properties.\n",
            "    35 included in dataset\n",
            "  William_Wyler: pre-selected 30/79 properties.\n",
            "    30 included in dataset\n",
            "  Wilma_Rudolph: pre-selected 11/13 properties.\n",
            "    11 included in dataset\n",
            "  Woody_Allen: pre-selected 35/192 properties.\n",
            "    35 included in dataset\n",
            "  Wright_brothers: pre-selected 10/11 properties.\n",
            "    10 included in dataset\n",
            "  Yan_Fu: pre-selected 7/11 properties.\n",
            "  Yo-Yo_Ma: pre-selected 18/19 properties.\n",
            "    18 included in dataset\n",
            "  Yuzuru_Hanyu: pre-selected 13/15 properties.\n",
            "    13 included in dataset\n",
            "  Zeno_of_Elea: pre-selected 10/11 properties.\n",
            "    10 included in dataset\n",
            "  Zheng_He: pre-selected 4/4 properties.\n",
            "----------\n",
            "669 new datapoints were created in total (between 8 and 199 triples in input; up to 3to6 instances of the same property in input).\n",
            "Average number of triples per input: 37.44394618834081\n"
          ]
        }
      ],
      "source": [
        "#@title Save triple set in XML format: Large DBpedia dataset\n",
        "from WikipediaPage_Generator.code.utils import get_first_n_instances_of_props, create_xml, clear_folder\n",
        "from WikipediaPage_Generator.code.queryDBpediaProps import CheckedTriple, get_resource_types, get_dbo_property_range_or_domain, get_superclasses, sql_query\n",
        "# from WikipediaPage_Generator.code.utils import create_xml, clear_folder\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Used for long-input D2T experiments: C:\\Users\\sfmil\\Desktop\\ADAPT-2025-2026\\MyPapers\\2025-06_INLG-longText-D2T\\files\\triple_sets_full\\dico_input_contents_DBp.pickle\n",
        "dico_input_contents = 'Made_with_this_notebook'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "# Map some categories that are found in WebNLG to the name used in WebNLG\n",
        "dico_mapping_categories = {'People':'Person', 'Cities':'City'}\n",
        "\n",
        "# Specifies the maximum number of occurrences of each property in the triple set. The lower and upper bounds are used to vary the number of instances for each entity.\n",
        " # e.g. one may not always want at most 3 occurrences of the same property with an entity, but rather say between 3 and 6 occurrences so the text looks a bit less mechanical.\n",
        "max_num_of_instances_of_prop_desired_lowerBound = \"3\" #@param[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "max_num_of_instances_of_prop_desired_upperBound = \"6\" #@param[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "# Specifies the minimun and maximum size of input desired\n",
        "min_input_size = \"8\" #@param[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100]\n",
        "max_input_size = \"199\" #@param[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100, 150, 199, 200, 250, 300, 500, 1000]\n",
        "# List here properties that cannot have 2 values (to filter bad stuff from i.e. infobox)\n",
        "# properties_that_can_happen_once_only = ['budget', 'gross', 'imdbId', 'length', 'runtime']\n",
        "properties_that_can_happen_once_only = json.loads(open('/content/WikipediaPage_Generator/resources/list_props_that_can_happen_once_only.json', 'r').read())\n",
        "# Select to run the validation of domains and ranges if needed and not done before. Doing it here instead of when collecting triple sets is more efficient be\n",
        "triple_validation = False #@param{type:\"boolean\"}\n",
        "# Either select along with triple_validation, or run on already validated triples.\n",
        "filter_out_unvalidated_triples = False #@param{type:\"boolean\"}\n",
        "# (INLG-2025)\n",
        "# entities_for_dev = ['Abu_Dhabi', 'Accra', 'Islamabad', 'Lagos', 'Aneurin_Bevan', 'Anthony_Giddens', 'Antoine_Lavoisier', 'Antonio_Negri']\n",
        "# (ACL 2026)\n",
        "entities_for_dev = ['Amsterdam', 'Berlin', 'Bucharest', 'Kuala_Lumpur', 'Madrid', 'Moscow', 'Mumbai', 'Singapore', 'Washington,_D.C.', 'Zagreb']\n",
        "\n",
        "if triple_validation and not filter_out_unvalidated_triples:\n",
        "  print('You selected triple_validation but not the filtering of triples, so validation will be bypassed because not used.')\n",
        "# Input data structure should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have entity_name as key, and a list of Triple Objects as value.\n",
        "# Triple Objects have 3 attributes: DBsubj, DBprop, DBobj\n",
        "counter_datapoints = 0\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "total_num_selected_props = 0\n",
        "longest_input_size = 0\n",
        "for input_category in dico_input_contents_loaded.keys():\n",
        "  webnlg_input_category = input_category\n",
        "  if input_category in dico_mapping_categories.keys():\n",
        "    webnlg_input_category = dico_mapping_categories[input_category]\n",
        "  print(webnlg_input_category)\n",
        "  folder_name = f'{webnlg_input_category}_{max_num_of_instances_of_prop_desired_lowerBound}to{max_num_of_instances_of_prop_desired_upperBound}'\n",
        "  clear_folder(os.path.join(triple2predArg, folder_name))\n",
        "  # if not os.path.exists(os.path.join(triple2predArg, input_category)):\n",
        "  os.makedirs(os.path.join(triple2predArg, folder_name))\n",
        "  for entity_name in dico_input_contents_loaded[input_category].keys():\n",
        "    # Comment this \"if\" to get v1 data; use \"if entity_name in entities_for_dev\" to get DEV data.\n",
        "    if entity_name not in entities_for_dev:\n",
        "      list_triple_objects = dico_input_contents_loaded[input_category][entity_name]\n",
        "      # Generate list of indices of all properties that can be part of an input (index in the list of Triple objects that contains all retrieved triples)\n",
        "      candidate_properties = get_first_n_instances_of_props(list_triple_objects, f'{max_num_of_instances_of_prop_desired_lowerBound}-{max_num_of_instances_of_prop_desired_upperBound}', properties_that_can_happen_once_only)\n",
        "      print(f'  {entity_name}: pre-selected {len(candidate_properties)}/{len(dico_input_contents_loaded[input_category][entity_name])} properties.')\n",
        "\n",
        "      filtered_list_triple_objects = []\n",
        "      # Check whether there is (at least) one intersection between the sets of expected and actual values\n",
        "      if filter_out_unvalidated_triples:\n",
        "        # Add triple validation information to candidate triples\n",
        "        for x in range(len(list_triple_objects)):\n",
        "          if x in candidate_properties:\n",
        "            triple_obj = list_triple_objects[x]\n",
        "            if triple_validation:\n",
        "              triple_obj.expected_ranges = get_dbo_property_range_or_domain(triple_obj.DBprop, 'range')\n",
        "              triple_obj.actual_ranges = get_resource_types(triple_obj.DBobj)\n",
        "              triple_obj.expected_domain = get_dbo_property_range_or_domain(triple_obj.DBprop, 'domain')\n",
        "              triple_obj.actual_domain = get_resource_types(triple_obj.DBsubj)\n",
        "            print('    ', x, triple_obj.DBprop)\n",
        "            # If there is a problem with the range validation, remove triple index from candidates\n",
        "            if len(triple_obj.expected_ranges) > 0:\n",
        "              if not set(triple_obj.expected_ranges) & set(triple_obj.actual_ranges):\n",
        "                print(f'      Range issue: exp. {triple_obj.expected_ranges} - act. {triple_obj.actual_ranges}')\n",
        "                candidate_properties.remove(x)\n",
        "            # If there is a problem with the domain validation, remove triple index from candidates\n",
        "            elif len(triple_obj.expected_domain) > 0:\n",
        "              if not set(triple_obj.expected_domain) & set(triple_obj.actual_domain):\n",
        "                print(f'      Domain issue: exp. {triple_obj.expected_domain} - act. {triple_obj.actual_domain}')\n",
        "                candidate_properties.remove(x)\n",
        "      else:\n",
        "        filtered_list_triple_objects = list_triple_objects\n",
        "\n",
        "      if len(candidate_properties) >= int(min_input_size):\n",
        "        if len(candidate_properties) > longest_input_size and len(candidate_properties) <= int(max_input_size):\n",
        "          longest_input_size = len(candidate_properties)\n",
        "          # print(f'!!! Updated longest input: {longest_input_size} - {entity_name}')\n",
        "        elif len(candidate_properties) > longest_input_size and len(candidate_properties) > int(max_input_size):\n",
        "          longest_input_size = int(max_input_size)\n",
        "          # print(f'!!! Updated longest input: {longest_input_size} - {entity_name}')\n",
        "        # create xml file passing the entity name to use as filename\n",
        "        print(f'    {len(candidate_properties[:int(max_input_size)])} included in dataset')\n",
        "        total_num_selected_props += len(candidate_properties[:int(max_input_size)])\n",
        "        counter_datapoints += 1\n",
        "        list_triples_text = create_xml(list_triple_objects, candidate_properties[:int(max_input_size)], webnlg_input_category, os.path.join(triple2predArg, folder_name), entity_name=entity_name, eid = counter_datapoints)\n",
        "\n",
        "print(f'----------\\n{counter_datapoints} new datapoints were created in total (between {int(min_input_size)} and {longest_input_size} triples in input; up to {max_num_of_instances_of_prop_desired_lowerBound}to{max_num_of_instances_of_prop_desired_upperBound} instances of the same property in input).')\n",
        "print(f'Average number of triples per input: {total_num_selected_props/counter_datapoints}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrZKd0bCDP39"
      },
      "source": [
        "## Process output XMLs (group, etc.)\n",
        "Stratified sampling info 100 inputs:\n",
        "1:20.8 - 2:19.6 - 3:19.6 - 4:17.2 - 5:12 - 6:6.4 - 7:4.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4AXBt6r7QcH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac7aad8-36b4-460e-9d8e-e2d11c9f5ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created XML file!\n"
          ]
        }
      ],
      "source": [
        "#@title Put all XMLs in the same file for sampling\n",
        "import glob\n",
        "import codecs\n",
        "import json\n",
        "\n",
        "XML_folder = 'XML'#@param['XML', 'XML_Split']\n",
        "\n",
        "paths_folders_categories = None\n",
        "out_filename = None\n",
        "out_entities_list = None\n",
        "\n",
        "############# In case code is applied to unsplit files generated with the cells above\n",
        "if XML_folder == 'XML':\n",
        "  paths_folders_categories = glob.glob('/content/XML/*')\n",
        "  out_filename = f'/content/XML/D2T-1-FA_same{max_num_of_instances_of_prop_desired_lowerBound}to{max_num_of_instances_of_prop_desired_upperBound}_min{min_input_size}_max{max_input_size}.xml'\n",
        "  out_entities_list = f'/content/XML/list_entities_same{max_num_of_instances_of_prop_desired_lowerBound}to{max_num_of_instances_of_prop_desired_upperBound}_min{min_input_size}_max{max_input_size}.json'\n",
        "############# In case code is applied to split files\n",
        "elif XML_folder == 'XML_Split':\n",
        "  paths_folders_categories = glob.glob('/content/XML_Split/*')\n",
        "  out_filename = f'/content/XML_Split/D2T-1-FA_same3_min8_max{max_num_triples}_SPLIT.xml'\n",
        "  out_entities_list = f'/content/XML_Split/list_entities_same3_min8_max100_SPLIT-{max_num_triples}.json'\n",
        "\n",
        "with codecs.open(out_filename, 'w', 'utf-8') as f:\n",
        "  list_entities = []\n",
        "  f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
        "  f.write('<benchmark>\\n')\n",
        "  f.write('  <entries>\\n')\n",
        "  for path_folder_category in sorted(paths_folders_categories):\n",
        "    list_XMLS_for_category = glob.glob(os.path.join(path_folder_category, '*.xml'))\n",
        "    for XML in sorted(list_XMLS_for_category):\n",
        "      filename = os.path.basename(XML).rsplit('.', 1)[0]\n",
        "      list_entities.append(filename)\n",
        "      with codecs.open(XML, 'r', 'utf-8') as file:\n",
        "        XML_lines = file.readlines()\n",
        "        for line in XML_lines:\n",
        "          if not line.startswith('<') and not line.startswith('  <'):\n",
        "            f.write(line)\n",
        "  f.write('  </entries>\\n')\n",
        "  f.write('</benchmark>\\n')\n",
        "  # Save entity list as json\n",
        "  # print(list_entities)\n",
        "  with codecs.open(out_entities_list, 'w', 'utf-8') as f:\n",
        "    json.dump(list_entities, f)\n",
        "print('Created XML file!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "6Gmn1qKRr6wj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77d9a8d4-355d-4eff-b60f-350e20142bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/XML/ (stored 0%)\n",
            "  adding: content/XML/people_3to6/ (stored 0%)\n",
            "  adding: content/XML/people_3to6/André-Marie_Ampère.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Maxim_Gorky.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Darryl_F._Zanuck.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Washington_Irving.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Abu_Bakr.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Leon_Festinger.xml (deflated 73%)\n",
            "  adding: content/XML/people_3to6/Antonio_Vivaldi.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Michelangelo_Antonioni.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Henry_Miller.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Lorenzo_de_Medici.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Vladimir_Vysotsky.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Graham_Greene.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Rafael_Nadal.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Maurice_Béjart.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Chuck_Berry.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Isadora_Duncan.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Mily_Balakirev.xml (deflated 74%)\n",
            "  adding: content/XML/people_3to6/Thomas_Hardy.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Leo_III_the_Isaurian.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Pierre_Curie.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Aristotle.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Sachin_Tendulkar.xml (deflated 74%)\n",
            "  adding: content/XML/people_3to6/Michel_de_Montaigne.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Seleucus_I_Nicator.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Seymour_Cray.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Stendhal.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/B._F._Skinner.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Charles_Hermite.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Richard_Wagner.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Humayun.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Sigismund,_Holy_Roman_Emperor.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Henryk_Sienkiewicz.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Frederick_Soddy.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/J._Paul_Getty.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Huineng.xml (deflated 74%)\n",
            "  adding: content/XML/people_3to6/G._H._Hardy.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Jean_Marais.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Hernán_Cortés.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Diego_Maradona.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Augustin-Jean_Fresnel.xml (deflated 74%)\n",
            "  adding: content/XML/people_3to6/Henry_the_Fowler.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Robert_Mugabe.xml (deflated 85%)\n",
            "  adding: content/XML/people_3to6/Averroes.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/David_Coverdale.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Ken_Kesey.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Babe_Didrikson_Zaharias.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Jürgen_Habermas.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Robin_Williams.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Ashoka.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Alexander_Grothendieck.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/George_Washington.xml (deflated 85%)\n",
            "  adding: content/XML/people_3to6/Robert_Fulton.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Ursula_von_der_Leyen.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Boris_Pasternak.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/B._R._Ambedkar.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Matsuo_Bashō.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/William_Vickrey.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Nolan_Bushnell.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/George_Bernard_Shaw.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Amitabh_Bachchan.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Ray_Bradbury.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Richard_Nixon.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Józef_Piłsudski.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Leonid_Brezhnev.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Joseph_Conrad.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Alexander_von_Humboldt.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Alexander_Pushkin.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Martin_Scorsese.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Robert_Walpole.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Luciano_Pavarotti.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Alessandro_Volta.xml (deflated 74%)\n",
            "  adding: content/XML/people_3to6/Francesco_Borromini.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Frédéric_Chopin.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/John_B._Watson.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Pierre_de_Coubertin.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/John_Jacob_Astor.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Jacobus_Henricus_van_t_Hoff.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Amedeo_Avogadro.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Ambrose.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Jean_Sibelius.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/William_Blake.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Shirley_Temple.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Jerry_Rice.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/John_the_Baptist.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/The_Rolling_Stones.xml (deflated 86%)\n",
            "  adding: content/XML/people_3to6/Masaccio.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Aurangzeb.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Charles_Algernon_Parsons.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Karl_Popper.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Aldous_Huxley.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/William_Herschel.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Boethius.xml (deflated 72%)\n",
            "  adding: content/XML/people_3to6/Alvin_Ailey.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Juan_Antonio_Samaranch.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Athanasius_of_Alexandria.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Tokugawa_Ieyasu.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Gustav_Holst.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Edward_Teller.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/John_Calvin.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Stevie_Wonder.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Gregory_of_Nyssa.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Olga_Korbut.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Paul_Gauguin.xml (deflated 73%)\n",
            "  adding: content/XML/people_3to6/Niklas_Luhmann.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Dhyan_Chand.xml (deflated 73%)\n",
            "  adding: content/XML/people_3to6/Alan_Moore.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Gregory_of_Nazianzus.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Joseph_Fourier.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Elvis_Presley.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/John_III_Sobieski.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/George_Carlin.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Edmund_Hillary.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Peter_Kropotkin.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Maria_Montessori.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Samuel_Johnson.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Erik_Bruhn.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/James_K._Polk.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Giovanni_Bellini.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Suzanne_Lenglen.xml (deflated 86%)\n",
            "  adding: content/XML/people_3to6/Wilhelm_Röntgen.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Oda_Nobunaga.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Bonaventura_Cavalieri.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Edward_I_of_England.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Arthur_Miller.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Hank_Aaron.xml (deflated 71%)\n",
            "  adding: content/XML/people_3to6/Francis_Bacon.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Jules_Verne.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/George_Westinghouse.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Kenzaburō_Ōe.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Bede.xml (deflated 74%)\n",
            "  adding: content/XML/people_3to6/Alexander_the_Great.xml (deflated 86%)\n",
            "  adding: content/XML/people_3to6/Nikita_Mikhalkov.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Helen_Keller.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Georg_Cantor.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Hans_Christian_Ørsted.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Paul_Klee.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Gaspard_Monge.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Nikolai_Lobachevsky.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Hayao_Miyazaki.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Osborne_Reynolds.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Woody_Allen.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Tristan_Tzara.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Edmund_Burke.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/El_Greco.xml (deflated 72%)\n",
            "  adding: content/XML/people_3to6/Roy_J._Plunkett.xml (deflated 72%)\n",
            "  adding: content/XML/people_3to6/J._P._Morgan.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Friedrich_Engels.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/David_McClelland.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Meyer_Guggenheim.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/James_Clerk_Maxwell.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Arminius.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Montesquieu.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Larisa_Latynina.xml (deflated 85%)\n",
            "  adding: content/XML/people_3to6/Laurence_Olivier.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Johannes_Rydberg.xml (deflated 72%)\n",
            "  adding: content/XML/people_3to6/Richard_Feynman.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Pope_Gregory_I.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Alfredo_Stroessner.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Confucius.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Rosa_Luxemburg.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Arthur_Wellesley,_1st_Duke_of_Wellington.xml (deflated 88%)\n",
            "  adding: content/XML/people_3to6/France_Prešeren.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Nelson_Mandela.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Mohammad_Reza_Pahlavi.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Pink_Floyd.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Pliny_the_Elder.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Pol_Pot.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Marius_Petipa.xml (deflated 73%)\n",
            "  adding: content/XML/people_3to6/Henry_Fonda.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Sandro_Botticelli.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Adam_Smith.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/John_Wayne.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Eusébio.xml (deflated 74%)\n",
            "  adding: content/XML/people_3to6/Alan_Parker.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Carlos_Saura.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Michelle_Kwan.xml (deflated 72%)\n",
            "  adding: content/XML/people_3to6/Joni_Mitchell.xml (deflated 85%)\n",
            "  adding: content/XML/people_3to6/Emperor_Meiji.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Augustine_of_Hippo.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Vivien_Leigh.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Metallica.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Pope_Julius_II.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Hendrik_Lorentz.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Michel_Foucault.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Archytas.xml (deflated 71%)\n",
            "  adding: content/XML/people_3to6/Bill_Gates.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Lech_Wałęsa.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Lata_Mangeshkar.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Erich_Maria_Remarque.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Miloš_Forman.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Pyotr_Ilyich_Tchaikovsky.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/John_Logie_Baird.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Rashi.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Léon_Foucault.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Élie_Cartan.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Báb.xml (deflated 71%)\n",
            "  adding: content/XML/people_3to6/John_D._Rockefeller.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Constantine_the_Great.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Horace.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Georg_Wilhelm_Friedrich_Hegel.xml (deflated 85%)\n",
            "  adding: content/XML/people_3to6/Wole_Soyinka.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/David_Hilbert.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Oscar_Niemeyer.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Ernest_Lawrence.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Ernst_Cassirer.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Alexander_Graham_Bell.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Robert_Burns_Woodward.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/George_III.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Cary_Grant.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Nizami_Ganjavi.xml (deflated 74%)\n",
            "  adding: content/XML/people_3to6/Titian.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Ludwig_Mies_van_der_Rohe.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Lothair_I.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Luís_de_Camões.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Heraclitus.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Helmut_Kohl.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Lev_Landau.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Peter_Debye.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Ravi_Shankar.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Irving_Thalberg.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Wang_Yangming.xml (deflated 72%)\n",
            "  adding: content/XML/people_3to6/Pitirim_Sorokin.xml (deflated 71%)\n",
            "  adding: content/XML/people_3to6/Warren_Buffett.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/James_Cook.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Casimir_Funk.xml (deflated 73%)\n",
            "  adding: content/XML/people_3to6/Maximilian_Schell.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Sergei_Eisenstein.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Kublai_Khan.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Tara_Lipinski.xml (deflated 72%)\n",
            "  adding: content/XML/people_3to6/Ludwig_van_Beethoven.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Dwight_D._Eisenhower.xml (deflated 85%)\n",
            "  adding: content/XML/people_3to6/Garrincha.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Qin_Shi_Huang.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Florence_Nightingale.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Philip_K._Dick.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Umm_Kulthum.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Howard_Hughes.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Mary_Baker_Eddy.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Rosa_Parks.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Slobodan_Milošević.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Vasil_Levski.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Pearl_S._Buck.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Andrea_Bocelli.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Mikhail_Bakhtin.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Adriano_Celentano.xml (deflated 86%)\n",
            "  adding: content/XML/people_3to6/Johannes_Gutenberg.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/William_the_Silent.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Ingemar_Johansson.xml (deflated 71%)\n",
            "  adding: content/XML/people_3to6/Wilt_Chamberlain.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Mohammed_bin_Salman.xml (deflated 85%)\n",
            "  adding: content/XML/people_3to6/Julius_Caesar.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Caravaggio.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Osama_bin_Laden.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Aleksey_Nikolayevich_Tolstoy.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Babur.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/C._V._Raman.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Robert_Hooke.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Sulla.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Hirohito.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Domitian.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Dean_Martin.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Vangelis.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Alfred_Adler.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Ghalib.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Joan_of_Arc.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Louis_Armstrong.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Raj_Kapoor.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Nikolai_Rimsky-Korsakov.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Richard_Neutra.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Wallace_Carothers.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Greta_Garbo.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Giuseppe_Verdi.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Golda_Meir.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Aisha.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Oprah_Winfrey.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Nikolay_Nekrasov.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/T._S._Eliot.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Charles_IV,_Holy_Roman_Emperor.xml (deflated 85%)\n",
            "  adding: content/XML/people_3to6/Giovanni_Boccaccio.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Woodrow_Wilson.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Jean-Victor_Poncelet.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Nicolae_Ceaușescu.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Idi_Amin.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Tom_Hanks.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Barack_Obama.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Francis_Ford_Coppola.xml (deflated 86%)\n",
            "  adding: content/XML/people_3to6/Hiroshige.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Audrey_Hepburn.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Adolf_Hitler.xml (deflated 85%)\n",
            "  adding: content/XML/people_3to6/Gerd_Müller.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Douglas_MacArthur.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Celine_Dion.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Leland_Stanford.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Richard_Branson.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Elizabeth_II.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/António_de_Oliveira_Salazar.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Bertrand_Russell.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Hermann_Maier.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Mehmed_IV.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Ivan_Mazepa.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Jack_Nicholson.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Shaka.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Johannes_Brahms.xml (deflated 74%)\n",
            "  adding: content/XML/people_3to6/William_I,_German_Emperor.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Gregor_Mendel.xml (deflated 73%)\n",
            "  adding: content/XML/people_3to6/Eugene_Wigner.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Clement_Attlee.xml (deflated 85%)\n",
            "  adding: content/XML/people_3to6/Dietrich_Bonhoeffer.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Vasco_da_Gama.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Michel_Fokine.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Jesse_Owens.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Ilf_and_Petrov.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Salvador_Dalí.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Christopher_Wren.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Horatio_Nelson,_1st_Viscount_Nelson.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Nikola_Tesla.xml (deflated 73%)\n",
            "  adding: content/XML/people_3to6/Edward_R._Murrow.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Ronnie_James_Dio.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Ignatius_of_Loyola.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Keir_Starmer.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Karl_Jaspers.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Charles_V,_Holy_Roman_Emperor.xml (deflated 86%)\n",
            "  adding: content/XML/people_3to6/Guru_Gobind_Singh.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Carl_Friedrich_Gauss.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Jan_van_Eyck.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Mario_Puzo.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Marcello_Mastroianni.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Grigori_Perelman.xml (deflated 73%)\n",
            "  adding: content/XML/people_3to6/Miklós_Rózsa.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Judith_Butler.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Vincent_van_Gogh.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Kurt_Vonnegut.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Ivan_III_of_Russia.xml (deflated 85%)\n",
            "  adding: content/XML/people_3to6/Paul_Dirac.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Eric_Clapton.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Ivan_the_Terrible.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/John_Steinbeck.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Andrea_del_Sarto.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Jerry_Goldsmith.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Arthur_C._Clarke.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Emperor_Gaozu_of_Han.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Stevie_Ray_Vaughan.xml (deflated 85%)\n",
            "  adding: content/XML/people_3to6/Luigi_Pirandello.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Mark_Zuckerberg.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Winston_Churchill.xml (deflated 86%)\n",
            "  adding: content/XML/people_3to6/Louis_de_Funès.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/James_Madison.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Sándor_Petőfi.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Gordie_Howe.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Ernest_Hemingway.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Johan_Cruyff.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Albert_Bandura.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Friedrich_Schiller.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Joseph_II,_Holy_Roman_Emperor.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Joe_Hisaishi.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Inigo_Jones.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Sinclair_Lewis.xml (deflated 73%)\n",
            "  adding: content/XML/people_3to6/Joan_Miró.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Arnold_Schoenberg.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Pope_Leo_I.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Tom_Brokaw.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Abel_Tasman.xml (deflated 74%)\n",
            "  adding: content/XML/people_3to6/Muhammad_Ali_Jinnah.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Dashiell_Hammett.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Alice_Walker.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Camillo_Benso,_Count_of_Cavour.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Lope_de_Vega.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Sebastian_Coe.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Leonardo_da_Vinci.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Quentin_Tarantino.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Leopold_II_of_Belgium.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Catherine_de_Medici.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Brigitte_Bardot.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Queen_Victoria.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Ang_Lee.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Epicurus.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Carlos_Slim.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Antonio_Gramsci.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Max_Weber.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Vitaly_Scherbo.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/John_Donne.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Muhammad_al-Bukhari.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Josiah_Willard_Gibbs.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Anwar_Sadat.xml (deflated 83%)\n",
            "  adding: content/XML/people_3to6/Black_Sabbath.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Claudette_Colbert.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Andre_Agassi.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/John_Chrysostom.xml (deflated 73%)\n",
            "  adding: content/XML/people_3to6/Jean-Luc_Godard.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Mihai_Eminescu.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/William_Harvey.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Ismail_Kadare.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Simón_Bolívar.xml (deflated 84%)\n",
            "  adding: content/XML/people_3to6/Tiger_Woods.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Pedro_Álvares_Cabral.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/George_Boole.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Søren_Kierkegaard.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Lyndon_B._Johnson.xml (deflated 85%)\n",
            "  adding: content/XML/people_3to6/Gustav_Fechner.xml (deflated 72%)\n",
            "  adding: content/XML/people_3to6/Thomas_Cranmer.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Aristophanes.xml (deflated 75%)\n",
            "  adding: content/XML/people_3to6/Andrew_Lloyd_Webber.xml (deflated 87%)\n",
            "  adding: content/XML/people_3to6/Euclid.xml (deflated 70%)\n",
            "  adding: content/XML/people_3to6/Steven_Chu.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Wolfgang_Amadeus_Mozart.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Aleksandr_Solzhenitsyn.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Shah_Jahan.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/Robert_Frost.xml (deflated 77%)\n",
            "  adding: content/XML/people_3to6/Carl_von_Clausewitz.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Zinedine_Zidane.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Joe_Cocker.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Emmy_Noether.xml (deflated 81%)\n",
            "  adding: content/XML/people_3to6/Barbara_Stanwyck.xml (deflated 79%)\n",
            "  adding: content/XML/people_3to6/François_Mansart.xml (deflated 80%)\n",
            "  adding: content/XML/people_3to6/Nikos_Kazantzakis.xml (deflated 78%)\n",
            "  adding: content/XML/people_3to6/Isaac_Singer.xml (deflated 76%)\n",
            "  adding: content/XML/people_3to6/Hermann_Weyl.xml (deflated 82%)\n",
            "  adding: content/XML/people_3to6/Emma_Goldman.xml (deflated 79%)\n",
            "  adding: content/XML/geography_3to6/ (stored 0%)\n",
            "  adding: content/XML/geography_3to6/San_Juan,_Puerto_Rico.xml (deflated 86%)\n",
            "  adding: content/XML/geography_3to6/Czech_Republic.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Uruguay_River.xml (deflated 81%)\n",
            "  adding: content/XML/geography_3to6/Minsk.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Vancouver_Island.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Oceania.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Atlantic_Ocean.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Bay_of_Biscay.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Sakhalin.xml (deflated 79%)\n",
            "  adding: content/XML/geography_3to6/Torres_Strait.xml (deflated 77%)\n",
            "  adding: content/XML/geography_3to6/Switzerland.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Brazil.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Athens.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Hungary.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Arabian_Sea.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Angola.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Atlanta.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Wales.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Ramallah.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Porto-Novo.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Bogotá.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Mogadishu.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Armenia.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Maseru.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Mount_Kenya.xml (deflated 71%)\n",
            "  adding: content/XML/geography_3to6/Sea_of_Azov.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Nagoya.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Irish_Sea.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Kathmandu.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Kraków.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Beaufort_Sea.xml (deflated 81%)\n",
            "  adding: content/XML/geography_3to6/Wuhan.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Kosovo.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Amur.xml (deflated 76%)\n",
            "  adding: content/XML/geography_3to6/La_Paz.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Gulf_of_Alaska.xml (deflated 80%)\n",
            "  adding: content/XML/geography_3to6/Ireland.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Burkina_Faso.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Khartoum.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Venice.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/North_Korea.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Yellow_River.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Liechtenstein.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Ethiopia.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Saint_Vincent_and_the_Grenadines.xml (deflated 87%)\n",
            "  adding: content/XML/geography_3to6/Ural_Mountains.xml (deflated 78%)\n",
            "  adding: content/XML/geography_3to6/Qatar.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Apennine_Mountains.xml (deflated 79%)\n",
            "  adding: content/XML/geography_3to6/Brunei.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Dominica.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Rio_Grande.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Gabon.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Ljubljana.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Genoa.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Mauritius.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Krishna_River.xml (deflated 81%)\n",
            "  adding: content/XML/geography_3to6/Iraq.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Great_Smoky_Mountains_National_Park.xml (deflated 76%)\n",
            "  adding: content/XML/geography_3to6/Chad.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Sea_of_Okhotsk.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Algeria.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Laptev_Sea.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Kuwait.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Lake_Balkhash.xml (deflated 79%)\n",
            "  adding: content/XML/geography_3to6/Roseau.xml (deflated 81%)\n",
            "  adding: content/XML/geography_3to6/Bosnia_and_Herzegovina.xml (deflated 88%)\n",
            "  adding: content/XML/geography_3to6/Oman.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Portugal.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Hanover.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Belgrade.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/San_Salvador.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Lyon.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Greenland_Sea.xml (deflated 78%)\n",
            "  adding: content/XML/geography_3to6/Addis_Ababa.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Rome.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Lahore.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Vancouver.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Slovakia.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Georgetown,_Guyana.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Dresden.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Cape_Town.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Malabo.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Transantarctic_Mountains.xml (deflated 78%)\n",
            "  adding: content/XML/geography_3to6/Guyana.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Honolulu.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Chukchi_Sea.xml (deflated 79%)\n",
            "  adding: content/XML/geography_3to6/Rio_de_Janeiro.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Baltic_Sea.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Paris.xml (deflated 80%)\n",
            "  adding: content/XML/geography_3to6/Rio_Negro_(Amazon).xml (deflated 77%)\n",
            "  adding: content/XML/geography_3to6/Balkans.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Mauritania.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Sarajevo.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Managua.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Castries.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Nigeria.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Caspian_Sea.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/The_Gambia.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/North_China_Plain.xml (deflated 75%)\n",
            "  adding: content/XML/geography_3to6/Laos.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Siberia.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Kingston,_Jamaica.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Borneo.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Molucca_Sea.xml (deflated 70%)\n",
            "  adding: content/XML/geography_3to6/Valletta.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Los_Angeles.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/New_Orleans.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Sydney.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Zanzibar_Archipelago.xml (deflated 79%)\n",
            "  adding: content/XML/geography_3to6/Malay_Peninsula.xml (deflated 81%)\n",
            "  adding: content/XML/geography_3to6/Ligurian_Sea.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/K2.xml (deflated 74%)\n",
            "  adding: content/XML/geography_3to6/Godavari_River.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Sudan.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Grenada.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Bermuda.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Comoros.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Denver.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Pretoria.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Mount_Kilimanjaro.xml (deflated 80%)\n",
            "  adding: content/XML/geography_3to6/Warsaw.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Central_African_Republic.xml (deflated 87%)\n",
            "  adding: content/XML/geography_3to6/Budapest.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Sahara.xml (deflated 74%)\n",
            "  adding: content/XML/geography_3to6/Cape_Verde.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Thessaloniki.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Rabat.xml (deflated 81%)\n",
            "  adding: content/XML/geography_3to6/South_China_Sea.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Bulgaria.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Kolyma_(river).xml (deflated 79%)\n",
            "  adding: content/XML/geography_3to6/Seoul.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Osaka.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Toulouse.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Nile.xml (deflated 82%)\n",
            "  adding: content/XML/geography_3to6/Aïr_Mountains.xml (deflated 76%)\n",
            "  adding: content/XML/geography_3to6/Lake_Superior.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Lake_Van.xml (deflated 79%)\n",
            "  adding: content/XML/geography_3to6/Great_Lakes.xml (deflated 80%)\n",
            "  adding: content/XML/geography_3to6/Poyang_Lake.xml (deflated 79%)\n",
            "  adding: content/XML/geography_3to6/Yamuna.xml (deflated 77%)\n",
            "  adding: content/XML/geography_3to6/Detroit.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Vanuatu.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Baffin_Bay.xml (deflated 76%)\n",
            "  adding: content/XML/geography_3to6/Adriatic_Sea.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Quebec.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/City_of_Brussels.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Colorado_River.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/El_Salvador.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Labrador_Sea.xml (deflated 77%)\n",
            "  adding: content/XML/geography_3to6/Atlas_Mountains.xml (deflated 80%)\n",
            "  adding: content/XML/geography_3to6/Afghanistan.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Black_Sea.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/San_Francisco.xml (deflated 85%)\n",
            "  adding: content/XML/geography_3to6/Pacific_Coast_Ranges.xml (deflated 80%)\n",
            "  adding: content/XML/geography_3to6/Paraná_River.xml (deflated 81%)\n",
            "  adding: content/XML/geography_3to6/Europe.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Yangtze.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Djibouti.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Riyadh.xml (deflated 83%)\n",
            "  adding: content/XML/geography_3to6/Drakensberg.xml (deflated 76%)\n",
            "  adding: content/XML/geography_3to6/Easter_Island.xml (deflated 81%)\n",
            "  adding: content/XML/geography_3to6/Java_Sea.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Lake_Maracaibo.xml (deflated 79%)\n",
            "  adding: content/XML/geography_3to6/Atlas.xml (deflated 74%)\n",
            "  adding: content/XML/geography_3to6/Iberian_Peninsula.xml (deflated 84%)\n",
            "  adding: content/XML/geography_3to6/Sumatra.xml (deflated 83%)\n",
            "  adding: content/XML/history_3to6/ (stored 0%)\n",
            "  adding: content/XML/history_3to6/Duchy_of_Burgundy.xml (deflated 80%)\n",
            "  adding: content/XML/history_3to6/Pompeii.xml (deflated 74%)\n",
            "  adding: content/XML/history_3to6/History_of_Iceland.xml (deflated 79%)\n",
            "  adding: content/XML/history_3to6/French_Revolution.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/Vladimir-Suzdal.xml (deflated 72%)\n",
            "  adding: content/XML/history_3to6/COVID-19_pandemic.xml (deflated 78%)\n",
            "  adding: content/XML/history_3to6/Womens_suffrage.xml (deflated 78%)\n",
            "  adding: content/XML/history_3to6/Wars_of_the_Roses.xml (deflated 83%)\n",
            "  adding: content/XML/history_3to6/Kingdom_of_France.xml (deflated 85%)\n",
            "  adding: content/XML/history_3to6/Fall_of_Constantinople.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/History_of_France.xml (deflated 76%)\n",
            "  adding: content/XML/history_3to6/History_of_China.xml (deflated 83%)\n",
            "  adding: content/XML/history_3to6/Graf.xml (deflated 69%)\n",
            "  adding: content/XML/history_3to6/Reconquista.xml (deflated 79%)\n",
            "  adding: content/XML/history_3to6/Kingdom_of_Bohemia.xml (deflated 83%)\n",
            "  adding: content/XML/history_3to6/Syrian_Wars.xml (deflated 80%)\n",
            "  adding: content/XML/history_3to6/Nazi_Germany.xml (deflated 85%)\n",
            "  adding: content/XML/history_3to6/French_and_Indian_War.xml (deflated 82%)\n",
            "  adding: content/XML/history_3to6/Umayyad_Caliphate.xml (deflated 83%)\n",
            "  adding: content/XML/history_3to6/Turkish_War_of_Independence.xml (deflated 85%)\n",
            "  adding: content/XML/history_3to6/Roman_Empire.xml (deflated 85%)\n",
            "  adding: content/XML/history_3to6/North_Yemen_Civil_War.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/Ancient_Rome.xml (deflated 84%)\n",
            "  adding: content/XML/history_3to6/Dutch_East_India_Company.xml (deflated 84%)\n",
            "  adding: content/XML/history_3to6/Wounded_Knee_Massacre.xml (deflated 80%)\n",
            "  adding: content/XML/history_3to6/History_of_India.xml (deflated 76%)\n",
            "  adding: content/XML/history_3to6/Macedonia_(ancient_kingdom).xml (deflated 84%)\n",
            "  adding: content/XML/history_3to6/Battle_of_Britain.xml (deflated 80%)\n",
            "  adding: content/XML/history_3to6/Early_modern_period.xml (deflated 73%)\n",
            "  adding: content/XML/history_3to6/Second_Spanish_Republic.xml (deflated 84%)\n",
            "  adding: content/XML/history_3to6/Mesopotamia.xml (deflated 83%)\n",
            "  adding: content/XML/history_3to6/Kievan_Rus.xml (deflated 80%)\n",
            "  adding: content/XML/history_3to6/Paraguayan_War.xml (deflated 83%)\n",
            "  adding: content/XML/history_3to6/Spanish_colonization_of_the_Americas.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/Sokoto_Caliphate.xml (deflated 79%)\n",
            "  adding: content/XML/history_3to6/Timbuktu.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/Ptolemaic_Kingdom.xml (deflated 83%)\n",
            "  adding: content/XML/history_3to6/Novgorod_Republic.xml (deflated 73%)\n",
            "  adding: content/XML/history_3to6/Gulf_War.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/Solidarity_(Polish_trade_union).xml (deflated 80%)\n",
            "  adding: content/XML/history_3to6/Abolitionism.xml (deflated 77%)\n",
            "  adding: content/XML/history_3to6/Crimean_War.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/Crimean_Khanate.xml (deflated 77%)\n",
            "  adding: content/XML/history_3to6/Vikings.xml (deflated 76%)\n",
            "  adding: content/XML/history_3to6/Atlantic_slave_trade.xml (deflated 75%)\n",
            "  adding: content/XML/history_3to6/Eighty_Years_War.xml (deflated 82%)\n",
            "  adding: content/XML/history_3to6/World_War_II.xml (deflated 79%)\n",
            "  adding: content/XML/history_3to6/Spanish_conquest_of_the_Aztec_Empire.xml (deflated 85%)\n",
            "  adding: content/XML/history_3to6/Seleucid_Empire.xml (deflated 80%)\n",
            "  adding: content/XML/history_3to6/Papal_States.xml (deflated 84%)\n",
            "  adding: content/XML/history_3to6/Tsardom_of_Russia.xml (deflated 84%)\n",
            "  adding: content/XML/history_3to6/History_of_Germany.xml (deflated 76%)\n",
            "  adding: content/XML/history_3to6/Sultanate_of_Rum.xml (deflated 82%)\n",
            "  adding: content/XML/history_3to6/Cold_War.xml (deflated 78%)\n",
            "  adding: content/XML/history_3to6/Second_Balkan_War.xml (deflated 83%)\n",
            "  adding: content/XML/history_3to6/Counter-Reformation.xml (deflated 75%)\n",
            "  adding: content/XML/history_3to6/Partition_of_India.xml (deflated 75%)\n",
            "  adding: content/XML/history_3to6/Iraq_War.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/Golden_Horde.xml (deflated 79%)\n",
            "  adding: content/XML/history_3to6/Russo-Japanese_War.xml (deflated 83%)\n",
            "  adding: content/XML/history_3to6/Ottoman_Empire.xml (deflated 85%)\n",
            "  adding: content/XML/history_3to6/Sparta.xml (deflated 79%)\n",
            "  adding: content/XML/history_3to6/Thirty_Years_War.xml (deflated 80%)\n",
            "  adding: content/XML/history_3to6/Arab_Revolt.xml (deflated 79%)\n",
            "  adding: content/XML/history_3to6/Susa.xml (deflated 73%)\n",
            "  adding: content/XML/history_3to6/Iran–Iraq_War.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/Hundred_Years_War.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/The_Holocaust.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/Balkan_Wars.xml (deflated 80%)\n",
            "  adding: content/XML/history_3to6/Indonesian_National_Revolution.xml (deflated 85%)\n",
            "  adding: content/XML/history_3to6/Angolan_Civil_War.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/History_of_Europe.xml (deflated 76%)\n",
            "  adding: content/XML/history_3to6/Livonian_War.xml (deflated 80%)\n",
            "  adding: content/XML/history_3to6/History_of_astronomy.xml (deflated 77%)\n",
            "  adding: content/XML/history_3to6/Hellenistic_period.xml (deflated 73%)\n",
            "  adding: content/XML/history_3to6/Eastern_Bloc.xml (deflated 70%)\n",
            "  adding: content/XML/history_3to6/Decolonization.xml (deflated 75%)\n",
            "  adding: content/XML/history_3to6/Beer_Hall_Putsch.xml (deflated 80%)\n",
            "  adding: content/XML/history_3to6/Mexican_Revolution.xml (deflated 83%)\n",
            "  adding: content/XML/history_3to6/First_Boer_War.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/Dacia.xml (deflated 75%)\n",
            "  adding: content/XML/history_3to6/Feudalism.xml (deflated 74%)\n",
            "  adding: content/XML/history_3to6/Babylonia.xml (deflated 83%)\n",
            "  adding: content/XML/history_3to6/German_reunification.xml (deflated 82%)\n",
            "  adding: content/XML/history_3to6/The_Troubles.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/Prussia.xml (deflated 82%)\n",
            "  adding: content/XML/history_3to6/Knight.xml (deflated 77%)\n",
            "  adding: content/XML/history_3to6/Xiongnu.xml (deflated 71%)\n",
            "  adding: content/XML/history_3to6/French_Wars_of_Religion.xml (deflated 81%)\n",
            "  adding: content/XML/history_3to6/Joseon.xml (deflated 82%)\n",
            "  adding: content/XML/history_3to6/Ancient_history.xml (deflated 77%)\n",
            "  adding: content/XML/history_3to6/American_Civil_War.xml (deflated 85%)\n",
            "  adding: content/XML/list_entities_same3to6_min8_max199.json (deflated 54%)\n",
            "  adding: content/XML/D2T-1-FA_same3to6_min8_max199.xml (deflated 87%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0ac96bb5-29cc-48c3-a327-f2422f900e19\", \"XMLs_Made_with_this_notebook_same3to6_min8_max199.zip\", 1335010)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Zip and download XML folder\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "XML_folder = 'XML'#@param['XML', 'XML_Split']\n",
        "\n",
        "zip_name_xml = None\n",
        "folder_to_zip = None\n",
        "if XML_folder == 'XML':\n",
        "  zip_name_xml = f'/content/XMLs_{dico_input_contents}_same{max_num_of_instances_of_prop_desired_lowerBound}to{max_num_of_instances_of_prop_desired_upperBound}_min{min_input_size}_max{max_input_size}.zip'\n",
        "  folder_to_zip = '/content/XML'\n",
        "elif XML_folder == 'XML_Split':\n",
        "  zip_name_xml = f'v4_XMLs_GREC_NEs_Updated_same{max_num_of_instances_of_prop_desired_lowerBound}to{max_num_of_instances_of_prop_desired_upperBound}_min{min_input_size}_max{max_input_size}_SPLIT-{max_num_triples}.zip'\n",
        "  folder_to_zip = '/content/XML_Split'\n",
        "\n",
        "if os.path.exists(zip_name_xml):\n",
        "  os.remove(zip_name_xml)\n",
        "\n",
        "! zip -r {zip_name_xml} {folder_to_zip}\n",
        "files.download(zip_name_xml)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Split each triple set in an XML file into several smaller triple sets\n",
        "from WikipediaPage_Generator.code.utils import TripleSet, Triple_withID, create_xml, clear_folder, balanced_split_with_max, extract_info_from_WebNLG_XML, sort_WebNLG_XMLs, split_XMLs\n",
        "import os\n",
        "\n",
        "##### INPUT VARIABLES\n",
        "path_xml = \"/content/v4_long-inputs_GREC_same3_min8_max100.xml\"#@param{type:\"string\"}\n",
        "path_DBprops_count = \"/content/WikipediaPage_Generator/resources/dico_count_occurrences_dbp_props.json\"#@param{type:\"string\"}\n",
        "# Take into account a few triples can be added to a group in case boundaries between groups are changed (to keep same properties in the same group)\n",
        "max_num_triples = 22#@param{type:\"integer\"}\n",
        "path_save_XMLs = \"/content/XML_Split\"#@param{type:\"string\"}\n",
        "debug = False#@param{type:\"boolean\"}\n",
        "\n",
        "if os.path.exists(path_xml):\n",
        "  split_XMLs(path_xml, path_DBprops_count, max_num_triples, path_save_XMLs, DEBUG = debug)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FxqXr9a8lWbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "Pj7mpBYpMoVi"
      },
      "outputs": [],
      "source": [
        "#@title Compare Semantic accuracy pickle file and Long-input D2T pickle file\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "dico_input_contents_SemEx_loaded = None\n",
        "with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "  dico_input_contents_SemEx_loaded = pickle.load(handle)\n",
        "\n",
        "dico_input_contents_LongIn_loaded = None\n",
        "with open(\"/content/dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "  dico_input_contents_LongIn_loaded = pickle.load(handle)\n",
        "\n",
        "def showContentsDico(dico_input_contents_loaded):\n",
        "  dico_contents = {}\n",
        "  for category in dico_input_contents_loaded.keys():\n",
        "    print(category)\n",
        "    if category not in dico_contents.keys():\n",
        "      dico_contents[category] = {}\n",
        "    for entity_name in dico_input_contents_loaded[category].keys():\n",
        "      print(f'  {entity_name} ({len(dico_input_contents_loaded[category][entity_name])} properties)')\n",
        "      if entity_name not in dico_contents[category].keys():\n",
        "        dico_contents[category][entity_name] = []\n",
        "      for triple_object in dico_input_contents_loaded[category][entity_name]:\n",
        "        # print('    ', triple_object.DBsubj, triple_object.DBprop, triple_object.DBobj)\n",
        "        triple = f'{triple_object.DBsubj} {triple_object.DBprop} {triple_object.DBobj}'\n",
        "        dico_contents[category][entity_name].append(triple)\n",
        "  return dico_contents\n",
        "\n",
        "dico_SemEx = showContentsDico(dico_input_contents_SemEx_loaded)\n",
        "dico_LongIn = showContentsDico(dico_input_contents_LongIn_loaded)\n",
        "\n",
        "# Save dicos as jsons\n",
        "with open('dico_input_contents_DBp_GREC_NEs_SemEx.json', 'w') as f:\n",
        "  json.dump(dico_SemEx, f)\n",
        "with open('dico_input_contents_DBp_GREC_NEs_LongIn.json', 'w') as f:\n",
        "  json.dump(dico_LongIn, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWAn5XF1WDOw"
      },
      "source": [
        "# More stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBIoHi7FWEpQ",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Test Functions\n",
        "\n",
        "# To split large XML inputs that can trigger memory issues in FORGe\n",
        "# 1 Check what the most frequent entity is (as subject or object)\n",
        "# 2 order all triples in which this entity is subject by frequency of occurrence of the property in DBpedia\n",
        "# 3 order all triples in which this entity is object by frequency of occurrence of the property in DBpedia\n",
        "  # If the entity is the object of a property that ends with a preposition, put it at the bottom in the list.\n",
        "# 4 Repeat 2 and 3 for the next most frequent entities as subject.\n",
        "# 5 Create an XML with at most n entities. Do not cut between two same properties.\n",
        "\n",
        "##### INSTALLS\n",
        "# ! pip install xmltodict\n",
        "##### IMPORTS\n",
        "from WikipediaPage_Generator.code.utils import create_xml, clear_folder\n",
        "import xmltodict\n",
        "import json\n",
        "import codecs\n",
        "import os\n",
        "from colored import Fore, Back, Style\n",
        "##### INPUT VARIABLES\n",
        "# path_xml = \"/content/v3_long-inputs_GREC_same3_min8_max100.xml\"\n",
        "path_xml = \"/content/v3_long-inputs_GREC_same3_min8_just1.xml\"\n",
        "path_DBprops_count = \"/content/WikipediaPage_Generator/resources/dico_count_occurrences_dbp_props.json\"\n",
        "# Take into account a few triples can be added to a group in case boundaries between groups are changed (to keep same properties in the same group)\n",
        "max_num_triples = 22\n",
        "path_save_XMLs = \"/content/XML_Split\"\n",
        "debug = False\n",
        "\n",
        "##### FUNCTIONS\n",
        "class TripleSet:\n",
        "  def __init__(self, triples_list, category, eid, shape, shape_type):\n",
        "    self.triples = triples_list\n",
        "    self.category = category\n",
        "    self.eid = eid\n",
        "    self.size = len(triples_list)\n",
        "    self.shape = shape\n",
        "    self.shape_type = shape_type\n",
        "    self.entities_by_frequency = []\n",
        "    # The main entity is the one that has the most occurrences as subject or object (if several have the same num of occurrences, \"max\" returns the first one)\n",
        "    entity_counter_dico = {}\n",
        "    for triple in self.triples:\n",
        "      if triple.DBsubj not in entity_counter_dico.keys():\n",
        "        entity_counter_dico[triple.DBsubj] = 1\n",
        "      else:\n",
        "        entity_counter_dico[triple.DBsubj] += 1\n",
        "      if triple.DBobj not in entity_counter_dico.keys():\n",
        "        entity_counter_dico[triple.DBobj] = 1\n",
        "      else:\n",
        "        entity_counter_dico[triple.DBobj] += 1\n",
        "    self.entities_by_frequency = sorted(entity_counter_dico, key=entity_counter_dico.get, reverse=True)\n",
        "    # print(entity_counter_dico)\n",
        "    # print(self.entities_by_frequency)\n",
        "\n",
        "class Triple_withID:\n",
        "  def __init__(self, prop, subj_value, obj_value, triple_id):\n",
        "    self.DBprop = prop\n",
        "    self.DBsubj = subj_value\n",
        "    self.DBobj = obj_value\n",
        "    self.id = triple_id\n",
        "\n",
        "def balanced_split_with_max(N1, N2):\n",
        "  \"\"\"\n",
        "  Function that splits as evenly as possible a number N1 into smaller numbers each as close as possible to another number N2 without being larger than N2.\n",
        "  For example, if N1 == 50 and N2 == 20, the output is 17, 17 16.\n",
        "  \"\"\"\n",
        "  assert N1 >= N2, \"N1 must be greater than or equal to N2\"\n",
        "  assert N2 >= 1, \"N1 must be at least 1\"\n",
        "  # Start with the minimal number of parts needed to respect the max constraint\n",
        "  for k in range(N1 // N2, N1 + 1):\n",
        "    # print(f'k: {k}')\n",
        "    base = N1 // k\n",
        "    remainder = N1 % k\n",
        "    # print(f'N1//N2, N1+1: {N1//N2}, {N1+1}')\n",
        "    # print(f'base: {base}')\n",
        "    # print(f'remainder: {remainder}')\n",
        "    # print()\n",
        "    # The largest part will be base + 1 (if remainder > 0)\n",
        "    if base + (1 if remainder > 0 else 0) <= N2:\n",
        "      result = [base + 1] * remainder + [base] * (k - remainder)\n",
        "      return result\n",
        "\n",
        "def extract_info_from_WebNLG_XML (path_input_XML):\n",
        "  \"\"\"\n",
        "  path_input_XML: Path to an XML file that contains triple sets, e.g. as provided in the WebNLG shared tasks.\n",
        "  returns a list of TripleSet objects. Each object contains as attributes: triples (a list of Triple objects), category, eid, size, shape, main_entity\n",
        "  \"\"\"\n",
        "  with codecs.open(path_input_XML, 'r', 'utf-8') as file:\n",
        "    XML_file = file.read()\n",
        "    XML_dict = xmltodict.parse(XML_file)\n",
        "    print(f'    Reading file {path_input_XML}..')\n",
        "    # triple_sets_list will be a list of objects of class TripleSet\n",
        "    total_number_of_triples = 0\n",
        "    triple_sets_list = []\n",
        "    if isinstance(XML_dict['benchmark']['entries']['entry'], list):\n",
        "      print(f\"      There are {len(XML_dict['benchmark']['entries']['entry'])} inputs in the original XML file.\")\n",
        "      for entry in XML_dict['benchmark']['entries']['entry']:\n",
        "        category = entry['@category']\n",
        "        eid = entry['@eid']\n",
        "        size = entry['@size']\n",
        "        shape = entry['@shape']\n",
        "        shape_type = entry['@shape-type']\n",
        "        # mtriples_list will be a list of objects of class Triple\n",
        "        mtriples_list = []\n",
        "        # Get modified triples\n",
        "        if isinstance(entry['modifiedtripleset']['mtriple'], list):\n",
        "          for triple_id, mtriple in enumerate(entry['modifiedtripleset']['mtriple']):\n",
        "            triple_object = Triple_withID(mtriple.split(' | ')[1], mtriple.split(' | ')[0], mtriple.split(' | ')[2], triple_id)\n",
        "            mtriples_list.append(triple_object)\n",
        "        else:\n",
        "          mtriples_list.append(entry['modifiedtripleset']['mtriple'])\n",
        "        assert int(size) == len(mtriples_list), f'Error: found size {size} but found {len(mtriples_list)} triples.'\n",
        "        total_number_of_triples += len(mtriples_list)\n",
        "        # Create object of class TripleSet\n",
        "        tripleSet_object = TripleSet(mtriples_list, category, eid, shape, shape_type)\n",
        "        triple_sets_list.append(tripleSet_object)\n",
        "    else:\n",
        "      print(f\"      There is 1 input in the original XML file.\")\n",
        "      category = XML_dict['benchmark']['entries']['entry']['@category']\n",
        "      eid = XML_dict['benchmark']['entries']['entry']['@eid']\n",
        "      size = XML_dict['benchmark']['entries']['entry']['@size']\n",
        "      shape = XML_dict['benchmark']['entries']['entry']['@shape']\n",
        "      shape_type = XML_dict['benchmark']['entries']['entry']['@shape-type']\n",
        "      # Block repeated from above\n",
        "      # mtriples_list will be a list of objects of class Triple\n",
        "      mtriples_list = []\n",
        "      # Get modified triples\n",
        "      if isinstance(XML_dict['benchmark']['entries']['entry']['modifiedtripleset']['mtriple'], list):\n",
        "        for triple_id, mtriple in enumerate(XML_dict['benchmark']['entries']['entry']['modifiedtripleset']['mtriple']):\n",
        "          triple_object = Triple_withID(mtriple.split(' | ')[1], mtriple.split(' | ')[0], mtriple.split(' | ')[2], triple_id)\n",
        "          mtriples_list.append(triple_object)\n",
        "      else:\n",
        "        mtriples_list.append(XML_dict['benchmark']['entries']['entry']['modifiedtripleset']['mtriple'])\n",
        "      assert int(size) == len(mtriples_list), f'Error: found size {size} but found {len(mtriples_list)} triples.'\n",
        "      total_number_of_triples += len(mtriples_list)\n",
        "      # Create object of class TripleSet\n",
        "      tripleSet_object = TripleSet(mtriples_list, category, eid, shape, shape_type)\n",
        "      triple_sets_list.append(tripleSet_object)\n",
        "\n",
        "    print(f\"      There are {total_number_of_triples} input triples in the original XML file.\")\n",
        "\n",
        "  return triple_sets_list\n",
        "\n",
        "def sort_WebNLG_XMLs (path_input_XML, path_DBprops_count):\n",
        "  \"\"\"\n",
        "  path_input_XML: Path to an XML file that contains triple sets, e.g. as provided in the WebNLG shared tasks. The code expects that all triples mention the same entity, as subject or object.\n",
        "  path_DBprops_count: Path to a json file that contains DBpedia properties as keys (e.g. \"http://dbpedia.org/ontology/birthPlace\") and number of occurrences on DBpedia as values (e.g 1486579).\n",
        "  This function returns a list of TripleSets objects. TripleSet.triples contains Triple objects; in each triple set, Triple objects are sorted by \"importance\" (i.e. sorted by frequency of entity in the triple set, and by frequency of property on DBpedia)\n",
        "  \"\"\"\n",
        "  print('  Sorting triples sets by frequency of entity in the triple set, and by frequency of respective properties on DBpedia...')\n",
        "  dico_count_occurrences_dbp_props = json.loads(codecs.open(path_DBprops_count, 'r', 'utf-8').read())\n",
        "  triple_sets_list = extract_info_from_WebNLG_XML (path_input_XML)\n",
        "  new_triple_set_Objects_list = []\n",
        "  total_number_of_triples = 0\n",
        "  for triple_set in triple_sets_list:\n",
        "    # print(triple_set.eid, triple_set.category, triple_set.size, triple_set.entities_by_frequency[0])\n",
        "    # Make a list where we will store the order of the triples using their index in the triple_set list\n",
        "    # E.g. list_triple_indices = [0, 4, 5, 2, 3, 1]\n",
        "    list_triple_indices = []\n",
        "    # Process entities by their respective importance in the triple set, so the most frequently found entity will go first, the second most frequently found will go second, and so on.\n",
        "    for entity_name in triple_set.entities_by_frequency:\n",
        "      # print(f'  {entity_name}')\n",
        "      # Make a list of property labels with the http://dbpedia.org/ontology/ prefix, one with the properties where the entity is subject, and one with the properties where the entity is object\n",
        "      # The properties in the ..._Subj list will go first, the properties in the ..._Obj list will go after.\n",
        "      list_dico_count_occurrences_dbp_props_keys_Subj = [[f'http://dbpedia.org/ontology/{triple.DBprop}', triple.id] for triple in triple_set.triples if triple.DBsubj == entity_name]\n",
        "      list_dico_count_occurrences_dbp_props_keys_Obj = [[f'http://dbpedia.org/ontology/{triple.DBprop}', triple.id] for triple in triple_set.triples if triple.DBobj == entity_name]\n",
        "      # Order that list according to the count in path_DBprops_count\n",
        "      sorted_list_dico_count_occurrences_dbp_props_keys_Subj = sorted(list_dico_count_occurrences_dbp_props_keys_Subj, key=lambda x: dico_count_occurrences_dbp_props[x[0]], reverse=True)\n",
        "      sorted_list_dico_count_occurrences_dbp_props_keys_Obj = sorted(list_dico_count_occurrences_dbp_props_keys_Obj, key=lambda x: dico_count_occurrences_dbp_props[x[0]], reverse=True)\n",
        "      # print(f'    {sorted_list_dico_count_occurrences_dbp_props_keys_Subj}')\n",
        "      # print(f'    {sorted_list_dico_count_occurrences_dbp_props_keys_Obj}')\n",
        "      # Now put all the triple indices for the current entity in list_triple_indices, starting with the triples in which the entity is subject\n",
        "      for list_triple_indices_Subj in sorted_list_dico_count_occurrences_dbp_props_keys_Subj:\n",
        "        # To avoid duplicated triples:\n",
        "        if list_triple_indices_Subj[1] not in list_triple_indices:\n",
        "          list_triple_indices.append(list_triple_indices_Subj[1])\n",
        "      for list_triple_indices_Obj in sorted_list_dico_count_occurrences_dbp_props_keys_Obj:\n",
        "        if list_triple_indices_Obj[1] not in list_triple_indices:\n",
        "          list_triple_indices.append(list_triple_indices_Obj[1])\n",
        "\n",
        "    #Now add the triples in a list, ordering the triples as defined in list_triple_indices (the create_xml function expects the triples ordered already)\n",
        "    new_triples_list = [triple_set.triples[i] for i in list_triple_indices]\n",
        "    assert len(new_triples_list) == len(triple_set.triples), f'Expected {len(triple_set.triples)} triples, found {len(new_triples_list)}'\n",
        "    total_number_of_triples += len(new_triples_list)\n",
        "    # print(len(new_triples_list), [new_triples_list[x].id for x in range(len(new_triples_list))])\n",
        "    new_triple_set_Objects_list.append(TripleSet(new_triples_list, triple_set.category, triple_set.eid, triple_set.shape, triple_set.shape_type))\n",
        "  assert len(new_triple_set_Objects_list) == len(triple_sets_list), f'Expected {len(triple_sets_list)} triple sets, found {len(new_triple_set_Objects_list)}'\n",
        "  print(f'    There are {len(new_triple_set_Objects_list)} sorted triple sets...')\n",
        "  print(f'    There are {total_number_of_triples} input triples in the sorted XML file.')\n",
        "\n",
        "  return new_triple_set_Objects_list\n",
        "\n",
        "def split_XMLs (path_input_XML, path_DBprops_count, max_num_triples, path_save_XMLs, DEBUG = False):\n",
        "  \"\"\"\n",
        "  path_input_XML: Path to an XML file that contains triple sets, e.g. as provided in the WebNLG shared tasks. The code expects that all triples mention the same entity, as subject or object.\n",
        "  path_DBprops_count: Path to a json file that contains DBpedia properties as keys (e.g. \"http://dbpedia.org/ontology/birthPlace\") and number of occurrences on DBpedia as values (e.g 1486579).\n",
        "  max_num_triples: the maximum number of triples desired in an XML\n",
        "  path_save_XMLs: the path where the output XMLs should be created\n",
        "  This function creates individual XML files for each split triple set.\n",
        "  \"\"\"\n",
        "  print('Splitting XML file...')\n",
        "  clear_folder(path_save_XMLs)\n",
        "  os.makedirs(path_save_XMLs)\n",
        "  # Get the list of TripleSet objects with the triples re-ordered. The object contains the following:\n",
        "  # self.triples, self.category, self.eid, self.size, self.shape, self.shape_type, self.entities_by_frequency\n",
        "  new_triple_set_Objects_list = sort_WebNLG_XMLs(path_input_XML, path_DBprops_count)\n",
        "  total_number_of_XMLs = 0\n",
        "  total_number_of_triples = 0\n",
        "  for new_triple_set in new_triple_set_Objects_list:\n",
        "    if DEBUG:\n",
        "      print(new_triple_set.size, new_triple_set.entities_by_frequency[0])\n",
        "    # Get \"ideal\" triple set split (see balanced_split_with_max function)\n",
        "    even_slices = None\n",
        "    if new_triple_set.size > max_num_triples:\n",
        "      # balanced_split_with_max returns a sequence of numbers that stand for a number of properties.\n",
        "      groups = balanced_split_with_max(new_triple_set.size, max_num_triples)\n",
        "      # Let's convert that to a sequence of numbers that correspond to list slices: [10, 10, 5] becomes [10, 20, 25]\n",
        "      even_slices = [sum(groups[:i]) for i in range(len(groups)+1)]\n",
        "    else:\n",
        "      even_slices = [0] + [new_triple_set.size]\n",
        "    if DEBUG:\n",
        "      print(f'  Before: {even_slices}')\n",
        "\n",
        "    # Initialise new list\n",
        "    new_slices = [0]\n",
        "    # Now we need to check if the split happened between two occurrences of the same property, which we'd like to avoid\n",
        "    # even_slices has at least 2 numbers, 0 and the end of the first or only slice.\n",
        "    if len(even_slices) > 2:\n",
        "      # Check for intermediate group boundaries (i.e. exclude the first boundary, which is 0, and the last one, because there is no property after it)\n",
        "      for boundary in even_slices[1:-1]:\n",
        "        previous_same_property = 0\n",
        "        # Since in the way even_slices is built, the last slices are the smallest ones, it's better to move boundaries to the left.\n",
        "        while new_triple_set.triples[boundary+previous_same_property].DBprop == new_triple_set.triples[boundary+previous_same_property-1].DBprop:\n",
        "          previous_same_property -= 1\n",
        "        if previous_same_property < 0:\n",
        "          new_slices.append(boundary+previous_same_property)\n",
        "          if DEBUG:\n",
        "            print(f'  {Fore.red}{Back.yellow}!!! Changed split {boundary}, {previous_same_property}!{Style.reset}')\n",
        "        else:\n",
        "          new_slices.append(boundary)\n",
        "      # Add last boundary\n",
        "      new_slices.append(even_slices[-1])\n",
        "    else:\n",
        "      # Add second and last boundary\n",
        "      new_slices.append(even_slices[1])\n",
        "    if DEBUG:\n",
        "      print(f'  After: {new_slices}')\n",
        "\n",
        "    # Create XMLs\n",
        "    # Set parameters for calling function that creates XMLs\n",
        "    input_category = new_triple_set.category\n",
        "    folder_name = input_category+'_max'+str(max_num_triples)\n",
        "    entity_name = new_triple_set.entities_by_frequency[0]\n",
        "    eid = new_triple_set.eid\n",
        "    # Clear/Create output folder\n",
        "    if not os.path.exists(os.path.join(path_save_XMLs, folder_name)):\n",
        "      os.makedirs(os.path.join(path_save_XMLs, folder_name))\n",
        "\n",
        "    # For each slice of the triple set, create an XML file\n",
        "    count_files_created = 0\n",
        "    for count_files, i in enumerate(range(len(new_slices)-1)):\n",
        "      list_triple_objects = new_triple_set.triples[new_slices[i]:new_slices[i+1]]\n",
        "      properties_selected_by_user = [i for i in range(len(list_triple_objects))] # Use all properties\n",
        "      unique_entity_name = entity_name+'_'+str(count_files)\n",
        "      list_triples_text = create_xml(list_triple_objects, properties_selected_by_user, input_category, os.path.join(path_save_XMLs, folder_name), entity_name=unique_entity_name, eid = eid)\n",
        "      count_files_created += 1\n",
        "      total_number_of_triples += len(list_triple_objects)\n",
        "    total_number_of_XMLs += count_files_created\n",
        "\n",
        "  print(f'  Created {total_number_of_XMLs} split XML files of approximate size {max_num_triples}.')\n",
        "  print(f'  There are {total_number_of_triples} input triples in the split XML files.')\n",
        "\n",
        "split_XMLs(path_xml, path_DBprops_count, max_num_triples, path_save_XMLs, DEBUG = debug)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Regroup FORGe outputs for split long-input inputs.\n",
        "# Above we needed to split some XMLs because FORGe can explode on inputs too large, so now we need to build on output file aligned with the input file\n",
        "import codecs\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "\n",
        "# 1 - Create a folder FORGe and upload outputs there, e.g. all_EN_dev_out_aligned.txt or v4_long-inputs_GREC_same3_min8_max100_SPLIT-22_en_000-299__SMorphText.conll_out.txt\n",
        "# 2 - Upload file list_entities_same3_min8_max100_SPLIT-22.json created at the same time as the split XML (and found in the same folder).\n",
        "\n",
        "forge_out_files = glob.glob('/content/FORGe/*.txt')\n",
        "list_entities_split = json.load(codecs.open('/content/list_entities_same3_min8_max100_SPLIT-22.json', 'r', 'utf-8'))\n",
        "\n",
        "# Put all FORGe texts in a list\n",
        "all_forge_lines = []\n",
        "for forge_out_file in sorted(forge_out_files):\n",
        "  with codecs.open(forge_out_file, 'r', 'utf-8') as f:\n",
        "    forge_out_lines = f.readlines()\n",
        "    all_forge_lines += forge_out_lines\n",
        "\n",
        "current_entity = None\n",
        "count_different_entities = 0\n",
        "with codecs.open('FORGe-all.txt', 'w', 'utf-8') as f:\n",
        "  for line_count, entity_with_count in enumerate(list_entities_split):\n",
        "    entity_name = entity_with_count.rsplit('_', 1)[0]\n",
        "    print(f'{line_count} - {entity_name}')\n",
        "    line = ''\n",
        "    if entity_name != current_entity:\n",
        "      current_entity = entity_name\n",
        "      if line_count == 0:\n",
        "        line = all_forge_lines[line_count].strip()\n",
        "      else:\n",
        "        line = line + '\\n'\n",
        "        line = line + all_forge_lines[line_count].strip()\n",
        "      count_different_entities += 1\n",
        "    else:\n",
        "      line = line + ' '+all_forge_lines[line_count].strip()\n",
        "    f.write(line)\n",
        "\n",
        "print(f'Found {count_different_entities} different entities.')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0TW-qZFH3RVL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make a shorter version of the reference texts\n",
        "import os\n",
        "import glob\n",
        "import codecs\n",
        "\n",
        "lang = 'EN'#@param['EN', 'GA']\n",
        "proportion_kept = 0.9#@param{type:\"slider\", min:0, max:1, step:0.01}\n",
        "folder_path = f'8_{lang}'\n",
        "zip_path = f\"/content/{folder_path}.zip\"\n",
        "# 9 is made with proportion_kept = 0.9\n",
        "# 10 is made with proportion_kept = 0.7\n",
        "# 11 is made with proportion kept = 0.5\n",
        "folder_out_num = '9'\n",
        "folder_out = f'{folder_out_num}_{lang}'\n",
        "\n",
        "if not os.path.exists(os.path.join('/content', folder_out)):\n",
        "  os.makedirs(os.path.join('/content', folder_out))\n",
        "\n",
        "#Unzip file\n",
        "# ! rm -r {folder_path}\n",
        "if not os.path.exists(folder_path):\n",
        "  ! unzip {zip_path} -d /content/{folder_path}\n",
        "\n",
        "\n",
        "for text_file_path in glob.glob(f'/content/{folder_path}/8/*.txt'):\n",
        "  filename = text_file_path.split('/')[-1]\n",
        "  new_filename = filename.replace('[8_', '['+folder_out_num+'_')\n",
        "  print(new_filename)\n",
        "  sentences_list = codecs.open(text_file_path, 'r', 'utf-8').read().strip().split('. ')\n",
        "  print(f'  {len(sentences_list)} sentences found.')\n",
        "  num_sentences_kept = int(round(len(sentences_list)*proportion_kept, 0))\n",
        "  print(f'  {len(sentences_list[:num_sentences_kept])} sentences kept')\n",
        "  with codecs.open(os.path.join('/content', folder_out, new_filename), 'w', 'utf-8') as f:\n",
        "    for sentence in sentences_list[:num_sentences_kept]:\n",
        "      f.write(sentence+'. ')\n",
        "\n",
        "# zip and download folder_out\n",
        "! zip -r /content/{folder_out}.zip /content/{folder_out}\n",
        "from google.colab import files\n",
        "files.download(f'/content/{folder_out}.zip')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3amcqAYVaZLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create sheets for human evaluation"
      ],
      "metadata": {
        "id": "8YBaVqV8e4cA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create fake output files for testing the code below\n",
        "# import os\n",
        "# import codecs\n",
        "\n",
        "# if not os.path.exists('/content/normalised_outputs'):\n",
        "#   os.makedirs('/content/normalised_outputs')\n",
        "\n",
        "# # Create fake outputs for 3 systems\n",
        "# for i in range(3):\n",
        "#   with codecs.open(f'/content/normalised_outputs/{i}-outputs-en.txt', 'w', 'utf-8') as f:\n",
        "#     for j in range(669):\n",
        "#       f.write(f'sys_{i}-line_{j} (sys and line counts started at 0). English text here')\n",
        "#       if j < 668:\n",
        "#         f.write('\\n')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DFlOvBJfre3H"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip the real files for the code below\n",
        "import os\n",
        "\n",
        "# Files from C:\\Users\\sfmil\\...\\data\\random_900_entities\\outputs\\LLMs_normalised_outputs\\content\\normalised_outputs\n",
        "language = 'en'#@param['en', 'ga']\n",
        "path_zip = '/content/outputs_D2T-1-FA_'+language+'.zip'\n",
        "\n",
        "if os.path.exists('/content/normalised_outputs'):\n",
        "  ! rm -r /content/normalised_outputs\n",
        "\n",
        "# Unzip files\n",
        "! unzip {path_zip} -d /content/normalised_outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "D7n6q6P812QG",
        "outputId": "0bea1e65-a2e8-4c6d-801b-758115e14b0a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/outputs_D2T-1-FA_en.zip\n",
            "  inflating: /content/normalised_outputs/0-e2e_D2T-1-FA_en.txt  \n",
            "  inflating: /content/normalised_outputs/1-default_D2T-1-FA_en.txt  \n",
            "  inflating: /content/normalised_outputs/2-unified_D2T-1-FA_en.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load sampling info and output texts (upload D2T-1-FA_samplingData.csv first)\n",
        "# We want to map the D2T-1-FA_samplingData.csv file to a format that matches dico_annotator_assignments created above\n",
        "import os\n",
        "import codecs\n",
        "import csv\n",
        "import glob\n",
        "import sys\n",
        "from pandas import *\n",
        "import pandas as pd\n",
        "\n",
        "# CSV from C:\\Users\\sfmil\\...\\data\\random_900_entities\\evaluation\n",
        "# Upload the sampling data csv into the repo\n",
        "path_sampling_data_csv = '/content/D2T-1-FA_samplingData.csv'\n",
        "path_output_files = '/content/normalised_outputs'\n",
        "\n",
        "# Extract sampling info\n",
        "# Original_id matches the original ID in the XML file (starts at 1); The \"id\" starts at 0, so it will match the line number in the output files.\n",
        "dico_sampled_ids_per_size = {}\n",
        "head, tail = os.path.split(path_sampling_data_csv)\n",
        "df_triple_sets = pd.read_csv(path_sampling_data_csv)\n",
        "# Extract all different values in the strat_field column:\n",
        "strat_field_labels = sorted(df_triple_sets['strat_field'].unique())\n",
        "# Create a dictionary that maps strat_field_labels to integers starting from 0\n",
        "strat_field_labels_to_int = {i : strat_field_labels[i] for i in range(len(strat_field_labels))}\n",
        "print(f'Mapping of strat_field values to the following keys: {strat_field_labels_to_int}')\n",
        "# strat_field_labels_to_int looks like this: {0: np.int64(0), 1: np.int64(2), 2: np.int64(4), 3: np.int64(6), 4: np.int64(8), 5: np.int64(10), 6: np.int64(12), 7: np.int64(14), 8: np.int64(16), 9: np.int64(18)}\n",
        "# Now for each \"normalised\" size, get the text IDs\n",
        "for n_int in strat_field_labels_to_int.keys():\n",
        "  dico_sampled_ids_per_size[n_int] = [df_triple_sets.loc[df_triple_sets['strat_field'] == strat_field_labels_to_int[n_int], 'id'].tolist()][0]\n",
        "print('\\ndico_sampled_ids_per_size: normalised size ranges (starting from 0) and the corresponding sampled text IDs (starting from 0):')\n",
        "print('', dico_sampled_ids_per_size)\n",
        "\n",
        "# Extract output texts\n",
        "dico_output_texts = {}\n",
        "for outfile in glob.glob(os.path.join(path_output_files, '*.txt')):\n",
        "  system_id = int(outfile.split('/')[-1].split('-')[0])\n",
        "  dico_output_texts[system_id] = {}\n",
        "  with codecs.open(outfile, 'r', 'utf-8') as f:\n",
        "    lines_output = f.readlines()\n",
        "    for i, line_output in enumerate(lines_output):\n",
        "      dico_output_texts[system_id][i] = line_output.strip()\n",
        "print('+\\ndico_output_texts: system IDs (starting from 0), all text IDs (starting from 0) and corresponding texts:')\n",
        "print('', dico_output_texts)\n",
        "if 0 not in dico_output_texts.keys():\n",
        "  sys.exit('There should be one system output file with the prefix \"0_\"!')\n",
        "\n",
        "# At this point, dico_sampled_ids_per_size looks like this, where keys are size categories:\n",
        "# {0: [570, 88, 246, 485, 371, 665], 1: [], 2: [213, 456, 348, 418, 382, 424], 3: [], 4: [652, 64, 332, 642, 318, 172], 5: [], 6: [479, 0, 303, 614, 7, 73]... }\n",
        "# And dico_output_texts looks like this, where keys are system IDs, and next level keys are input IDs:\n",
        "# {'3': {0: 'sys_3-line_0 (line count started at 0). English text here', 1: 'sys_3-line_1 (line count started at 0). English text here', ...} }\n",
        "\n",
        "# Let's now build a dictionary with all and only the contents for the spreadsheet\n",
        "dico_for_spreadsheets = {}\n",
        "for system_id in dico_output_texts.keys():\n",
        "  # Create on key per system ID in the new dico\n",
        "  dico_for_spreadsheets[system_id] = {}\n",
        "  for size in dico_sampled_ids_per_size.keys():\n",
        "    # For each system ID, create a key for each size\n",
        "    dico_for_spreadsheets[system_id][size] = []\n",
        "    # Now go get the corresponding text for each systemID+textID. The texts will be in the same order for all systems since the text ID come from a unique list of IDs in dico_sampled_ids_per_size.\n",
        "    for text_id in dico_sampled_ids_per_size[size]:\n",
        "      dico_for_spreadsheets[system_id][size].append(dico_output_texts[system_id][text_id])\n",
        "print('=\\ndico_for_spreadsheets: system IDs (starting from 0), normalised size ranges (starting from 0) and the corresponding sampled texts')\n",
        "print('', dico_for_spreadsheets)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "aCfdcR9HjaTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Automatically assign texts to evaluators (latin square)\n",
        "# To distribute evaluators as evenly as possible across input sizes and systems.\n",
        "# With two annotators per text.\n",
        "\n",
        "from collections import defaultdict\n",
        "import csv\n",
        "import math\n",
        "\n",
        "# Use dico_for_spreadsheets to get the number of systems.\n",
        "num_systems = len(dico_for_spreadsheets.keys())\n",
        "# Get the number of different sizes using the first key in dico_for_spreadsheets\n",
        "num_sizes = len(dico_for_spreadsheets[list(dico_for_spreadsheets.keys())[0]])\n",
        "\n",
        "# IMPORTANT: make sure the number of output per sizes matches the provided data.\n",
        "# It should be len(dico_sampled_ids_per_size[0]), but I leave it manual here so it is possible to play around with the numbers.\n",
        "# If a smaller number than len(dico_sampled_ids_per_size[0]) is chosen, the largest data sizes will be dropped.\n",
        "num_outputs_per_size = 3  #@param{type:'slider', min:1, max:10}\n",
        "num_annotators = 6        #@param{type:'slider', min:1, max:10}\n",
        "\n",
        "def choose_offset(num_annotators: int) -> int:\n",
        "  \"\"\" Function to compute the offset we will use to optimise the latin square\"\"\"\n",
        "  # Hand-pick for some small N\n",
        "  #\n",
        "  # The “+2” choice for 6 annotators is just a clean way to guarantee two different annotators per item while keeping things symmetric and balanced across the 6 annotators; it is not the only possible shift, but it is a convenient one given there are 6 annotators.\n",
        "  # With +1 for the two annotators of the same items, the distribution is not as even, e.g. each output is seen by 4 instead of 5 (when using +2) different annotators. With +3, there would be only 3 different pairs of annotators, so worse distribution of annotators overall.\n",
        "  if num_annotators <= 4:\n",
        "    return 1\n",
        "  elif num_annotators > 4 and num_annotators <= 6:\n",
        "    return 2\n",
        "  elif num_annotators == 7 or num_annotators == 8:\n",
        "    return 3\n",
        "  elif num_annotators == 9 or num_annotators == 10:\n",
        "    return 4\n",
        "  else:\n",
        "    # Randomly decided by me\n",
        "    estimate = int(num_annotators/(num_systems-1))\n",
        "    return estimate\n",
        "\n",
        "offset = choose_offset(num_annotators)\n",
        "print(f'Chose offset = {offset} for {num_annotators} annotators.')\n",
        "\n",
        "def twoAnnotators_per_item(size_i, system_j, output_k, offset):\n",
        "  # We want a1 and a2 to have a difference of \"offset\" (see choose_offset function) to maximise evaluator distribution. Using the \"modulo\" operator to ensure we always have a valid evaluator number.\n",
        "  a1 = (size_i + system_j + output_k) % num_annotators # add \"+ 1\" at the end if we want to start annotator ID at 1 instead of 0\n",
        "  a2 = (size_i + system_j + output_k + offset) % num_annotators # add \"+ 1\" at the end if we want to start annotator ID at 1 instead of 0\n",
        "  # print(f'  size: {size_i} system: {system_j} output: {output_k} annotators {a1}, {a2}')\n",
        "  return a1, a2\n",
        "\n",
        "# Collect assignments per annotator\n",
        "# defaultdict(list) automatically provides an empty list [] for any new key accessed.\n",
        "dico_annotator_assignments = defaultdict(list)\n",
        "for i in range(num_sizes):        # size index\n",
        "  for j in range(num_systems):    # system index\n",
        "    for k in range(num_outputs_per_size):  # output index\n",
        "      a1, a2 = twoAnnotators_per_item(i, j, k, offset)\n",
        "      item = {\"size\": i, \"system\": j, \"output\": k}\n",
        "      dico_annotator_assignments[a1].append(item)\n",
        "      dico_annotator_assignments[a2].append(item)\n",
        "\n",
        "# Sanity check\n",
        "for annotator in sorted(dico_annotator_assignments.keys()):\n",
        "  # print(f\"Annotator {annotator}: {len(dico_annotator_assignments[annotator])} items. {dico_annotator_assignments[annotator]}\")\n",
        "  print(f\"Annotator {annotator}: {len(dico_annotator_assignments[annotator])} items.\")\n",
        "\n",
        "# Build and print one table: system × global_output_id → list of annotators. Each cell will typically contain 2 annotators (since we want 2 per text).\n",
        "table = {s: defaultdict(list) for s in range(num_systems)}\n",
        "for size in range(num_sizes):\n",
        "  for system in range(num_systems):\n",
        "    for k in range(num_outputs_per_size):\n",
        "      a1, a2 = twoAnnotators_per_item(size, system, k, offset)\n",
        "      global_id = size * num_outputs_per_size + k\n",
        "      table[system][global_id].extend([a1, a2])\n",
        "systems = range(num_systems)\n",
        "all_cols = range(num_sizes * num_outputs_per_size)\n",
        "header = [\"IDs\"] + [str(c) for c in all_cols]\n",
        "print('\\n6/size: Size 0: 0-5; Size 1: 6-11; Size 2: 12-17; Size 3: 18-23; Size 4: 24-29; Size 5: 30-35; Size 6: 36-41; Size 7: 42-47; Size 8: 48-53; Size 9: 54-59')\n",
        "print('3/size: Size 0: 0-2; Size 1: 3-5; Size 2: 6-8; Size 3: 9-11; Size 4: 12-14; Size 5: 15-17; Size 6: 18-20; Size 7: 21-23; Size 8: 24-26; Size 9: 27-29')\n",
        "print('Rows = Systems, Columns = Outputs\\n---------------------------------\\n'+\"\\t\".join(header))\n",
        "for s in systems:\n",
        "  row = [f\"S{s}\"]\n",
        "  for gid in all_cols:\n",
        "    anns = table[s].get(gid, [])\n",
        "    cell = \",\".join(str(a) for a in sorted(set(anns))) if anns else \"\"\n",
        "    row.append(cell)\n",
        "  print(\"\\t\".join(row))\n",
        "\n",
        "# Write one CSV per annotator\n",
        "for a in range(num_annotators):\n",
        "  filename = f\"annotator_{a}.csv\"\n",
        "  with open(filename, \"w\", newline=\"\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"size\", \"system\", \"output\"])\n",
        "    writer.writeheader()\n",
        "    for row in dico_annotator_assignments[a]:\n",
        "      writer.writerow(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "fxFmxw_oxjxp",
        "outputId": "bc8add7f-25ba-4447-c41b-c71ae5cfcd7d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chose offset = 2 for 6 annotators.\n",
            "Annotator 0: 30 items.\n",
            "Annotator 1: 28 items.\n",
            "Annotator 2: 28 items.\n",
            "Annotator 3: 30 items.\n",
            "Annotator 4: 32 items.\n",
            "Annotator 5: 32 items.\n",
            "\n",
            "6/size: Size 0: 0-5; Size 1: 6-11; Size 2: 12-17; Size 3: 18-23; Size 4: 24-29; Size 5: 30-35; Size 6: 36-41; Size 7: 42-47; Size 8: 48-53; Size 9: 54-59\n",
            "3/size: Size 0: 0-2; Size 1: 3-5; Size 2: 6-8; Size 3: 9-11; Size 4: 12-14; Size 5: 15-17; Size 6: 18-20; Size 7: 21-23; Size 8: 24-26; Size 9: 27-29\n",
            "Rows = Systems, Columns = Outputs\n",
            "---------------------------------\n",
            "IDs\t0\t1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t11\t12\t13\t14\t15\t16\t17\t18\t19\t20\t21\t22\t23\t24\t25\t26\t27\t28\t29\n",
            "S0\t0,2\t1,3\t2,4\t1,3\t2,4\t3,5\t2,4\t3,5\t0,4\t3,5\t0,4\t1,5\t0,4\t1,5\t0,2\t1,5\t0,2\t1,3\t0,2\t1,3\t2,4\t1,3\t2,4\t3,5\t2,4\t3,5\t0,4\t3,5\t0,4\t1,5\n",
            "S1\t1,3\t2,4\t3,5\t2,4\t3,5\t0,4\t3,5\t0,4\t1,5\t0,4\t1,5\t0,2\t1,5\t0,2\t1,3\t0,2\t1,3\t2,4\t1,3\t2,4\t3,5\t2,4\t3,5\t0,4\t3,5\t0,4\t1,5\t0,4\t1,5\t0,2\n",
            "S2\t2,4\t3,5\t0,4\t3,5\t0,4\t1,5\t0,4\t1,5\t0,2\t1,5\t0,2\t1,3\t0,2\t1,3\t2,4\t1,3\t2,4\t3,5\t2,4\t3,5\t0,4\t3,5\t0,4\t1,5\t0,4\t1,5\t0,2\t1,5\t0,2\t1,3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print dictionaries built so far\n",
        "print('We now have three useful dictionaries!')\n",
        "print('\\ndico_sampled_ids_per_size (contains IDs of sampled texts): normalised size ranges (starting from 0) and the corresponding sampled text IDs (starting from 0):')\n",
        "print('', dico_sampled_ids_per_size)\n",
        "print('\\ndico_for_spreadsheets (contains sampled texts sorted by size and system): system IDs (starting from 0), normalised size ranges (starting from 0) and the corresponding sampled texts:')\n",
        "print('', dico_for_spreadsheets)\n",
        "print('\\ndico_annotator_assignments (contains assignments as combination of input size, system ID and local output ID): annotator IDs (starting from 0), lists of dicos that contain {system IDs, normalised size ranges and normalised output IDs} (all starting from 0)')\n",
        "# Local on the line above means the position of an output in the list of sampled outputs for a certain size, so if 6 outputs per size were selected above, this relative ID will be a number between 0 and 5.\n",
        "print('', dico_annotator_assignments)"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "Bl-q4LzNz6Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Authenticate in Sheets and open working sheet (upload credentials first)\n",
        "import gspread\n",
        "from google.oauth2.service_account import Credentials\n",
        "\n",
        "# Credentials from C:\\Users\\sfmil\\Desktop\\ADAPT-2025-2026\\Admin\\Google\n",
        "\n",
        "# 1. Auth scopes and credentials\n",
        "SCOPES = [\"https://www.googleapis.com/auth/spreadsheets\"]\n",
        "# Upload credentials file to the repo\n",
        "SERVICE_ACCOUNT_FILE = \"/content/credentials.json\"\n",
        "\n",
        "creds = Credentials.from_service_account_file(\n",
        "    SERVICE_ACCOUNT_FILE,\n",
        "    scopes=SCOPES,\n",
        ")\n",
        "\n",
        "# 2. Authorize client\n",
        "client = gspread.authorize(creds)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "LILKmY2bf7Nz"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Fill up annotator sheets (first assign sheet IDs manually in dico_annotator_sheets below)\n",
        "# I can probably use the position of the texts in dico_for_spreadsheets[systemID][sizeID] to match with the 'output' key in dico_annotator_assignments.\n",
        "# Because for all systems, the list of output IDs comes from the same source list, so the output IDs are in the same order for each system.\n",
        "import random\n",
        "from googleapiclient.errors import HttpError\n",
        "import time\n",
        "import re\n",
        "\n",
        "# For long D2T eval, used 42\n",
        "random.seed(42)\n",
        "\n",
        "# The first annotator has ID = 0\n",
        "dico_annotator_sheets = None\n",
        "if language == 'en':\n",
        "  dico_annotator_sheets = {0:'1nfuDA-1RizII_FxCWqDLRXfgb6DG9b5bUJCIA96a5ig',\n",
        "                           1:'1ij9Y5ydxb5mL6TwsvubOGVzVdjGi8aUP26mb7Id1DGQ',\n",
        "                           2:'1zVLDqm69-9Gwb2GOCEWh_svb98axId3-oX3ONUF3aUE',\n",
        "                           3:'12vuBNdzaLUWoZo63XhKY6aEZXOMMGhuG6tM1oV3aUnw',\n",
        "                           4:'1hGh0L9Hca4NLnqLknZph-ctf5I13nImkVfYeGIZZtWU',\n",
        "                           5:'14EWDERWpiHaIscwkGsFlsqpSFyX_-5nnwsyhe_ukmRg'}\n",
        "elif language == 'ga':\n",
        "  dico_annotator_sheets = {0:'1nRr-twKJXE4oJlRNUslL6sgdGXWwwwDbbjBG9pMgZus',\n",
        "                           1:'1p8y_-wdZM-M7gQtBtCn915YIfdDe8CN3zkW32O9N-Dk',\n",
        "                           2:'1kekPQXrt--pnzUz2ejYmbSi2ZldbfI0lSZJV5-Y8UXE',\n",
        "                           3:'1nnfmXA_GcM0bDpobzaMuR9v-MEqBZOPCHdQa4LOebrA',\n",
        "                           4:'1ihzKym3K-SRZ7yypBMktIFDWUTqMTRUzQILztLI5fQ0',\n",
        "                           5:'1BfW-Sa8BorYbU7MHXNCmT6ZYfqYd90GXkdVRDnr7hCs'}\n",
        "if 0 not in dico_annotator_sheets.keys():\n",
        "  sys.exit('There should be one key 0 in the dictionary!')\n",
        "\n",
        "for annotator_id in dico_annotator_sheets.keys():\n",
        "  # 3. Open spreadsheet and first worksheet\n",
        "  spreadsheet = client.open_by_key(dico_annotator_sheets[annotator_id])\n",
        "  worksheet = spreadsheet.get_worksheet(0)  # first sheet\n",
        "\n",
        "  data_columns_AB = []\n",
        "  data_column_G = []\n",
        "  # Note for self: I need to use list() here to actually create a copy of dico_annotator_assignments[annotator_id] before shuffling, otherwise the random seed doesn't work.\n",
        "  # list_dicos_assignments = dico_annotator_assignments[annotator_id] does not create a new list—it creates a reference (like a pointer) to the exact same list object already stored in the dictionary.\n",
        "  list_dicos_assignments = list(dico_annotator_assignments[annotator_id])\n",
        "  # Shuffle the assignments otherwise they are sorted by size, which is not ideal for the evaluation (each text would be larger than the previous one)\n",
        "  random.shuffle(list_dicos_assignments)\n",
        "  print(list_dicos_assignments)\n",
        "  for i, dico_assignment in enumerate(list_dicos_assignments):\n",
        "    # Get info about each text assignment for the current annotator\n",
        "    system_ID = dico_assignment['system']\n",
        "    size_ID = dico_assignment['size']\n",
        "    output_relID = dico_assignment['output']\n",
        "    # Get full ID from the dico_sampled_ids_per_size dico\n",
        "    output_fullID = dico_sampled_ids_per_size[size_ID][output_relID]\n",
        "    print(f'  {i} - sys:{system_ID} - size:{size_ID} - idRel:{output_relID} - idFull:{output_fullID}')\n",
        "    # Get text\n",
        "    eval_text = dico_for_spreadsheets[system_ID][size_ID][output_relID]\n",
        "    # Add linebreaks after each sentence.\n",
        "    eval_text_linebreaks = re.subn(r'([^\\s][^\\s]\\.) ([A-Z])', r'\\g<1>\\n\\g<2>', eval_text)[0]\n",
        "\n",
        "    # 4. Write content to cells; too many calls per minute need to batch the calls\n",
        "    # worksheet.update_acell(\"A\"+str(i+7), i)\n",
        "    # worksheet.update_acell(\"B\"+str(i+7), eval_text)\n",
        "    # worksheet.update_acell(\"G\"+str(i+7), f'{output_fullID}-{system_ID}-{size_ID}-{output_relID}')\n",
        "    data_columns_AB.append([i, eval_text_linebreaks])\n",
        "    data_column_G.append([f'{output_fullID}-{system_ID}-{size_ID}-{output_relID}'])\n",
        "\n",
        "  # print(data_columns_AB)\n",
        "  worksheet.update(data_columns_AB, 'A7')\n",
        "  # print(data_column_G)\n",
        "  worksheet.update(data_column_G, 'G7')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "UbcJbzaPnNCc",
        "outputId": "9d635a05-7a7a-43bf-caa6-e8fab42302b4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'size': 6, 'system': 2, 'output': 2}, {'size': 4, 'system': 1, 'output': 1}, {'size': 3, 'system': 1, 'output': 2}, {'size': 9, 'system': 0, 'output': 1}, {'size': 8, 'system': 0, 'output': 2}, {'size': 2, 'system': 2, 'output': 0}, {'size': 2, 'system': 1, 'output': 1}, {'size': 4, 'system': 0, 'output': 0}, {'size': 3, 'system': 2, 'output': 1}, {'size': 4, 'system': 2, 'output': 0}, {'size': 3, 'system': 1, 'output': 0}, {'size': 8, 'system': 2, 'output': 2}, {'size': 9, 'system': 2, 'output': 1}, {'size': 7, 'system': 2, 'output': 1}, {'size': 5, 'system': 0, 'output': 1}, {'size': 9, 'system': 1, 'output': 0}, {'size': 0, 'system': 2, 'output': 2}, {'size': 4, 'system': 0, 'output': 2}, {'size': 6, 'system': 0, 'output': 0}, {'size': 1, 'system': 1, 'output': 2}, {'size': 5, 'system': 1, 'output': 0}, {'size': 9, 'system': 1, 'output': 2}, {'size': 2, 'system': 0, 'output': 2}, {'size': 8, 'system': 2, 'output': 0}, {'size': 2, 'system': 2, 'output': 2}, {'size': 3, 'system': 0, 'output': 1}, {'size': 8, 'system': 1, 'output': 1}, {'size': 0, 'system': 0, 'output': 0}, {'size': 1, 'system': 2, 'output': 1}, {'size': 7, 'system': 1, 'output': 2}]\n",
            "  0 - sys:2 - size:6 - idRel:2 - idFull:74\n",
            "  1 - sys:1 - size:4 - idRel:1 - idFull:137\n",
            "  2 - sys:1 - size:3 - idRel:2 - idFull:303\n",
            "  3 - sys:0 - size:9 - idRel:1 - idFull:53\n",
            "  4 - sys:0 - size:8 - idRel:2 - idFull:34\n",
            "  5 - sys:2 - size:2 - idRel:0 - idFull:652\n",
            "  6 - sys:1 - size:2 - idRel:1 - idFull:64\n",
            "  7 - sys:0 - size:4 - idRel:0 - idFull:25\n",
            "  8 - sys:2 - size:3 - idRel:1 - idFull:0\n",
            "  9 - sys:2 - size:4 - idRel:0 - idFull:25\n",
            "  10 - sys:1 - size:3 - idRel:0 - idFull:479\n",
            "  11 - sys:2 - size:8 - idRel:2 - idFull:34\n",
            "  12 - sys:2 - size:9 - idRel:1 - idFull:53\n",
            "  13 - sys:2 - size:7 - idRel:1 - idFull:124\n",
            "  14 - sys:0 - size:5 - idRel:1 - idFull:52\n",
            "  15 - sys:1 - size:9 - idRel:0 - idFull:144\n",
            "  16 - sys:2 - size:0 - idRel:2 - idFull:246\n",
            "  17 - sys:0 - size:4 - idRel:2 - idFull:141\n",
            "  18 - sys:0 - size:6 - idRel:0 - idFull:126\n",
            "  19 - sys:1 - size:1 - idRel:2 - idFull:348\n",
            "  20 - sys:1 - size:5 - idRel:0 - idFull:147\n",
            "  21 - sys:1 - size:9 - idRel:2 - idFull:140\n",
            "  22 - sys:0 - size:2 - idRel:2 - idFull:332\n",
            "  23 - sys:2 - size:8 - idRel:0 - idFull:76\n",
            "  24 - sys:2 - size:2 - idRel:2 - idFull:332\n",
            "  25 - sys:0 - size:3 - idRel:1 - idFull:0\n",
            "  26 - sys:1 - size:8 - idRel:1 - idFull:68\n",
            "  27 - sys:0 - size:0 - idRel:0 - idFull:570\n",
            "  28 - sys:2 - size:1 - idRel:1 - idFull:456\n",
            "  29 - sys:1 - size:7 - idRel:2 - idFull:13\n",
            "[{'size': 1, 'system': 0, 'output': 0}, {'size': 7, 'system': 2, 'output': 2}, {'size': 4, 'system': 1, 'output': 2}, {'size': 3, 'system': 1, 'output': 1}, {'size': 6, 'system': 1, 'output': 0}, {'size': 5, 'system': 0, 'output': 2}, {'size': 5, 'system': 1, 'output': 1}, {'size': 9, 'system': 0, 'output': 2}, {'size': 9, 'system': 2, 'output': 2}, {'size': 3, 'system': 2, 'output': 2}, {'size': 4, 'system': 1, 'output': 0}, {'size': 8, 'system': 2, 'output': 1}, {'size': 5, 'system': 0, 'output': 0}, {'size': 5, 'system': 2, 'output': 0}, {'size': 0, 'system': 1, 'output': 0}, {'size': 1, 'system': 2, 'output': 2}, {'size': 7, 'system': 0, 'output': 0}, {'size': 3, 'system': 0, 'output': 2}, {'size': 2, 'system': 1, 'output': 2}, {'size': 9, 'system': 2, 'output': 0}, {'size': 4, 'system': 0, 'output': 1}, {'size': 4, 'system': 2, 'output': 1}, {'size': 8, 'system': 1, 'output': 2}, {'size': 2, 'system': 2, 'output': 1}, {'size': 0, 'system': 0, 'output': 1}, {'size': 9, 'system': 1, 'output': 1}, {'size': 3, 'system': 2, 'output': 0}, {'size': 6, 'system': 0, 'output': 1}]\n",
            "  0 - sys:0 - size:1 - idRel:0 - idFull:213\n",
            "  1 - sys:2 - size:7 - idRel:2 - idFull:13\n",
            "  2 - sys:1 - size:4 - idRel:2 - idFull:141\n",
            "  3 - sys:1 - size:3 - idRel:1 - idFull:0\n",
            "  4 - sys:1 - size:6 - idRel:0 - idFull:126\n",
            "  5 - sys:0 - size:5 - idRel:2 - idFull:91\n",
            "  6 - sys:1 - size:5 - idRel:1 - idFull:52\n",
            "  7 - sys:0 - size:9 - idRel:2 - idFull:140\n",
            "  8 - sys:2 - size:9 - idRel:2 - idFull:140\n",
            "  9 - sys:2 - size:3 - idRel:2 - idFull:303\n",
            "  10 - sys:1 - size:4 - idRel:0 - idFull:25\n",
            "  11 - sys:2 - size:8 - idRel:1 - idFull:68\n",
            "  12 - sys:0 - size:5 - idRel:0 - idFull:147\n",
            "  13 - sys:2 - size:5 - idRel:0 - idFull:147\n",
            "  14 - sys:1 - size:0 - idRel:0 - idFull:570\n",
            "  15 - sys:2 - size:1 - idRel:2 - idFull:348\n",
            "  16 - sys:0 - size:7 - idRel:0 - idFull:117\n",
            "  17 - sys:0 - size:3 - idRel:2 - idFull:303\n",
            "  18 - sys:1 - size:2 - idRel:2 - idFull:332\n",
            "  19 - sys:2 - size:9 - idRel:0 - idFull:144\n",
            "  20 - sys:0 - size:4 - idRel:1 - idFull:137\n",
            "  21 - sys:2 - size:4 - idRel:1 - idFull:137\n",
            "  22 - sys:1 - size:8 - idRel:2 - idFull:34\n",
            "  23 - sys:2 - size:2 - idRel:1 - idFull:64\n",
            "  24 - sys:0 - size:0 - idRel:1 - idFull:88\n",
            "  25 - sys:1 - size:9 - idRel:1 - idFull:53\n",
            "  26 - sys:2 - size:3 - idRel:0 - idFull:479\n",
            "  27 - sys:0 - size:6 - idRel:1 - idFull:165\n",
            "[{'size': 4, 'system': 0, 'output': 2}, {'size': 0, 'system': 2, 'output': 0}, {'size': 7, 'system': 1, 'output': 0}, {'size': 0, 'system': 0, 'output': 0}, {'size': 4, 'system': 2, 'output': 0}, {'size': 5, 'system': 0, 'output': 1}, {'size': 3, 'system': 1, 'output': 2}, {'size': 9, 'system': 2, 'output': 1}, {'size': 5, 'system': 1, 'output': 2}, {'size': 1, 'system': 1, 'output': 0}, {'size': 5, 'system': 1, 'output': 0}, {'size': 1, 'system': 0, 'output': 1}, {'size': 6, 'system': 2, 'output': 0}, {'size': 8, 'system': 0, 'output': 0}, {'size': 4, 'system': 2, 'output': 2}, {'size': 7, 'system': 0, 'output': 1}, {'size': 8, 'system': 2, 'output': 2}, {'size': 9, 'system': 1, 'output': 2}, {'size': 2, 'system': 2, 'output': 2}, {'size': 0, 'system': 0, 'output': 2}, {'size': 0, 'system': 1, 'output': 1}, {'size': 2, 'system': 0, 'output': 0}, {'size': 6, 'system': 0, 'output': 0}, {'size': 4, 'system': 1, 'output': 1}, {'size': 6, 'system': 0, 'output': 2}, {'size': 6, 'system': 1, 'output': 1}, {'size': 3, 'system': 2, 'output': 1}, {'size': 5, 'system': 2, 'output': 1}]\n",
            "  0 - sys:0 - size:4 - idRel:2 - idFull:141\n",
            "  1 - sys:2 - size:0 - idRel:0 - idFull:570\n",
            "  2 - sys:1 - size:7 - idRel:0 - idFull:117\n",
            "  3 - sys:0 - size:0 - idRel:0 - idFull:570\n",
            "  4 - sys:2 - size:4 - idRel:0 - idFull:25\n",
            "  5 - sys:0 - size:5 - idRel:1 - idFull:52\n",
            "  6 - sys:1 - size:3 - idRel:2 - idFull:303\n",
            "  7 - sys:2 - size:9 - idRel:1 - idFull:53\n",
            "  8 - sys:1 - size:5 - idRel:2 - idFull:91\n",
            "  9 - sys:1 - size:1 - idRel:0 - idFull:213\n",
            "  10 - sys:1 - size:5 - idRel:0 - idFull:147\n",
            "  11 - sys:0 - size:1 - idRel:1 - idFull:456\n",
            "  12 - sys:2 - size:6 - idRel:0 - idFull:126\n",
            "  13 - sys:0 - size:8 - idRel:0 - idFull:76\n",
            "  14 - sys:2 - size:4 - idRel:2 - idFull:141\n",
            "  15 - sys:0 - size:7 - idRel:1 - idFull:124\n",
            "  16 - sys:2 - size:8 - idRel:2 - idFull:34\n",
            "  17 - sys:1 - size:9 - idRel:2 - idFull:140\n",
            "  18 - sys:2 - size:2 - idRel:2 - idFull:332\n",
            "  19 - sys:0 - size:0 - idRel:2 - idFull:246\n",
            "  20 - sys:1 - size:0 - idRel:1 - idFull:88\n",
            "  21 - sys:0 - size:2 - idRel:0 - idFull:652\n",
            "  22 - sys:0 - size:6 - idRel:0 - idFull:126\n",
            "  23 - sys:1 - size:4 - idRel:1 - idFull:137\n",
            "  24 - sys:0 - size:6 - idRel:2 - idFull:74\n",
            "  25 - sys:1 - size:6 - idRel:1 - idFull:165\n",
            "  26 - sys:2 - size:3 - idRel:1 - idFull:0\n",
            "  27 - sys:2 - size:5 - idRel:1 - idFull:52\n",
            "[{'size': 0, 'system': 1, 'output': 2}, {'size': 0, 'system': 0, 'output': 1}, {'size': 4, 'system': 2, 'output': 1}, {'size': 7, 'system': 1, 'output': 1}, {'size': 7, 'system': 2, 'output': 0}, {'size': 5, 'system': 1, 'output': 1}, {'size': 9, 'system': 0, 'output': 0}, {'size': 6, 'system': 2, 'output': 1}, {'size': 3, 'system': 2, 'output': 2}, {'size': 8, 'system': 0, 'output': 1}, {'size': 2, 'system': 1, 'output': 0}, {'size': 0, 'system': 2, 'output': 1}, {'size': 5, 'system': 2, 'output': 0}, {'size': 1, 'system': 0, 'output': 0}, {'size': 1, 'system': 1, 'output': 1}, {'size': 6, 'system': 1, 'output': 0}, {'size': 6, 'system': 0, 'output': 1}, {'size': 7, 'system': 0, 'output': 0}, {'size': 0, 'system': 1, 'output': 0}, {'size': 3, 'system': 0, 'output': 0}, {'size': 8, 'system': 1, 'output': 0}, {'size': 9, 'system': 2, 'output': 2}, {'size': 6, 'system': 1, 'output': 2}, {'size': 2, 'system': 0, 'output': 1}, {'size': 4, 'system': 1, 'output': 2}, {'size': 5, 'system': 0, 'output': 2}, {'size': 1, 'system': 0, 'output': 2}, {'size': 1, 'system': 2, 'output': 0}, {'size': 7, 'system': 0, 'output': 2}, {'size': 5, 'system': 2, 'output': 2}]\n",
            "  0 - sys:1 - size:0 - idRel:2 - idFull:246\n",
            "  1 - sys:0 - size:0 - idRel:1 - idFull:88\n",
            "  2 - sys:2 - size:4 - idRel:1 - idFull:137\n",
            "  3 - sys:1 - size:7 - idRel:1 - idFull:124\n",
            "  4 - sys:2 - size:7 - idRel:0 - idFull:117\n",
            "  5 - sys:1 - size:5 - idRel:1 - idFull:52\n",
            "  6 - sys:0 - size:9 - idRel:0 - idFull:144\n",
            "  7 - sys:2 - size:6 - idRel:1 - idFull:165\n",
            "  8 - sys:2 - size:3 - idRel:2 - idFull:303\n",
            "  9 - sys:0 - size:8 - idRel:1 - idFull:68\n",
            "  10 - sys:1 - size:2 - idRel:0 - idFull:652\n",
            "  11 - sys:2 - size:0 - idRel:1 - idFull:88\n",
            "  12 - sys:2 - size:5 - idRel:0 - idFull:147\n",
            "  13 - sys:0 - size:1 - idRel:0 - idFull:213\n",
            "  14 - sys:1 - size:1 - idRel:1 - idFull:456\n",
            "  15 - sys:1 - size:6 - idRel:0 - idFull:126\n",
            "  16 - sys:0 - size:6 - idRel:1 - idFull:165\n",
            "  17 - sys:0 - size:7 - idRel:0 - idFull:117\n",
            "  18 - sys:1 - size:0 - idRel:0 - idFull:570\n",
            "  19 - sys:0 - size:3 - idRel:0 - idFull:479\n",
            "  20 - sys:1 - size:8 - idRel:0 - idFull:76\n",
            "  21 - sys:2 - size:9 - idRel:2 - idFull:140\n",
            "  22 - sys:1 - size:6 - idRel:2 - idFull:74\n",
            "  23 - sys:0 - size:2 - idRel:1 - idFull:64\n",
            "  24 - sys:1 - size:4 - idRel:2 - idFull:141\n",
            "  25 - sys:0 - size:5 - idRel:2 - idFull:91\n",
            "  26 - sys:0 - size:1 - idRel:2 - idFull:348\n",
            "  27 - sys:2 - size:1 - idRel:0 - idFull:213\n",
            "  28 - sys:0 - size:7 - idRel:2 - idFull:13\n",
            "  29 - sys:2 - size:5 - idRel:2 - idFull:91\n",
            "[{'size': 8, 'system': 0, 'output': 2}, {'size': 0, 'system': 0, 'output': 2}, {'size': 1, 'system': 1, 'output': 0}, {'size': 6, 'system': 2, 'output': 2}, {'size': 6, 'system': 2, 'output': 0}, {'size': 6, 'system': 1, 'output': 1}, {'size': 0, 'system': 2, 'output': 2}, {'size': 2, 'system': 0, 'output': 2}, {'size': 4, 'system': 0, 'output': 0}, {'size': 1, 'system': 1, 'output': 2}, {'size': 7, 'system': 0, 'output': 1}, {'size': 2, 'system': 1, 'output': 1}, {'size': 8, 'system': 0, 'output': 0}, {'size': 8, 'system': 1, 'output': 1}, {'size': 7, 'system': 1, 'output': 2}, {'size': 0, 'system': 1, 'output': 1}, {'size': 0, 'system': 2, 'output': 0}, {'size': 9, 'system': 1, 'output': 0}, {'size': 5, 'system': 1, 'output': 2}, {'size': 1, 'system': 0, 'output': 1}, {'size': 1, 'system': 2, 'output': 1}, {'size': 2, 'system': 2, 'output': 0}, {'size': 3, 'system': 0, 'output': 1}, {'size': 7, 'system': 2, 'output': 1}, {'size': 3, 'system': 1, 'output': 0}, {'size': 6, 'system': 0, 'output': 2}, {'size': 9, 'system': 0, 'output': 1}, {'size': 2, 'system': 0, 'output': 0}, {'size': 8, 'system': 2, 'output': 0}, {'size': 5, 'system': 2, 'output': 1}, {'size': 7, 'system': 1, 'output': 0}, {'size': 4, 'system': 2, 'output': 2}]\n",
            "  0 - sys:0 - size:8 - idRel:2 - idFull:34\n",
            "  1 - sys:0 - size:0 - idRel:2 - idFull:246\n",
            "  2 - sys:1 - size:1 - idRel:0 - idFull:213\n",
            "  3 - sys:2 - size:6 - idRel:2 - idFull:74\n",
            "  4 - sys:2 - size:6 - idRel:0 - idFull:126\n",
            "  5 - sys:1 - size:6 - idRel:1 - idFull:165\n",
            "  6 - sys:2 - size:0 - idRel:2 - idFull:246\n",
            "  7 - sys:0 - size:2 - idRel:2 - idFull:332\n",
            "  8 - sys:0 - size:4 - idRel:0 - idFull:25\n",
            "  9 - sys:1 - size:1 - idRel:2 - idFull:348\n",
            "  10 - sys:0 - size:7 - idRel:1 - idFull:124\n",
            "  11 - sys:1 - size:2 - idRel:1 - idFull:64\n",
            "  12 - sys:0 - size:8 - idRel:0 - idFull:76\n",
            "  13 - sys:1 - size:8 - idRel:1 - idFull:68\n",
            "  14 - sys:1 - size:7 - idRel:2 - idFull:13\n",
            "  15 - sys:1 - size:0 - idRel:1 - idFull:88\n",
            "  16 - sys:2 - size:0 - idRel:0 - idFull:570\n",
            "  17 - sys:1 - size:9 - idRel:0 - idFull:144\n",
            "  18 - sys:1 - size:5 - idRel:2 - idFull:91\n",
            "  19 - sys:0 - size:1 - idRel:1 - idFull:456\n",
            "  20 - sys:2 - size:1 - idRel:1 - idFull:456\n",
            "  21 - sys:2 - size:2 - idRel:0 - idFull:652\n",
            "  22 - sys:0 - size:3 - idRel:1 - idFull:0\n",
            "  23 - sys:2 - size:7 - idRel:1 - idFull:124\n",
            "  24 - sys:1 - size:3 - idRel:0 - idFull:479\n",
            "  25 - sys:0 - size:6 - idRel:2 - idFull:74\n",
            "  26 - sys:0 - size:9 - idRel:1 - idFull:53\n",
            "  27 - sys:0 - size:2 - idRel:0 - idFull:652\n",
            "  28 - sys:2 - size:8 - idRel:0 - idFull:76\n",
            "  29 - sys:2 - size:5 - idRel:1 - idFull:52\n",
            "  30 - sys:1 - size:7 - idRel:0 - idFull:117\n",
            "  31 - sys:2 - size:4 - idRel:2 - idFull:141\n",
            "[{'size': 9, 'system': 1, 'output': 1}, {'size': 4, 'system': 1, 'output': 0}, {'size': 2, 'system': 0, 'output': 1}, {'size': 9, 'system': 0, 'output': 2}, {'size': 3, 'system': 0, 'output': 2}, {'size': 8, 'system': 1, 'output': 0}, {'size': 9, 'system': 2, 'output': 0}, {'size': 1, 'system': 0, 'output': 2}, {'size': 7, 'system': 2, 'output': 0}, {'size': 6, 'system': 1, 'output': 2}, {'size': 3, 'system': 1, 'output': 1}, {'size': 1, 'system': 2, 'output': 0}, {'size': 7, 'system': 2, 'output': 2}, {'size': 0, 'system': 2, 'output': 1}, {'size': 5, 'system': 0, 'output': 0}, {'size': 6, 'system': 2, 'output': 1}, {'size': 8, 'system': 1, 'output': 2}, {'size': 0, 'system': 1, 'output': 2}, {'size': 4, 'system': 0, 'output': 1}, {'size': 1, 'system': 2, 'output': 2}, {'size': 3, 'system': 2, 'output': 0}, {'size': 2, 'system': 2, 'output': 1}, {'size': 1, 'system': 1, 'output': 1}, {'size': 3, 'system': 0, 'output': 0}, {'size': 7, 'system': 0, 'output': 2}, {'size': 8, 'system': 2, 'output': 1}, {'size': 2, 'system': 1, 'output': 2}, {'size': 8, 'system': 0, 'output': 1}, {'size': 5, 'system': 2, 'output': 2}, {'size': 9, 'system': 0, 'output': 0}, {'size': 7, 'system': 1, 'output': 1}, {'size': 2, 'system': 1, 'output': 0}]\n",
            "  0 - sys:1 - size:9 - idRel:1 - idFull:53\n",
            "  1 - sys:1 - size:4 - idRel:0 - idFull:25\n",
            "  2 - sys:0 - size:2 - idRel:1 - idFull:64\n",
            "  3 - sys:0 - size:9 - idRel:2 - idFull:140\n",
            "  4 - sys:0 - size:3 - idRel:2 - idFull:303\n",
            "  5 - sys:1 - size:8 - idRel:0 - idFull:76\n",
            "  6 - sys:2 - size:9 - idRel:0 - idFull:144\n",
            "  7 - sys:0 - size:1 - idRel:2 - idFull:348\n",
            "  8 - sys:2 - size:7 - idRel:0 - idFull:117\n",
            "  9 - sys:1 - size:6 - idRel:2 - idFull:74\n",
            "  10 - sys:1 - size:3 - idRel:1 - idFull:0\n",
            "  11 - sys:2 - size:1 - idRel:0 - idFull:213\n",
            "  12 - sys:2 - size:7 - idRel:2 - idFull:13\n",
            "  13 - sys:2 - size:0 - idRel:1 - idFull:88\n",
            "  14 - sys:0 - size:5 - idRel:0 - idFull:147\n",
            "  15 - sys:2 - size:6 - idRel:1 - idFull:165\n",
            "  16 - sys:1 - size:8 - idRel:2 - idFull:34\n",
            "  17 - sys:1 - size:0 - idRel:2 - idFull:246\n",
            "  18 - sys:0 - size:4 - idRel:1 - idFull:137\n",
            "  19 - sys:2 - size:1 - idRel:2 - idFull:348\n",
            "  20 - sys:2 - size:3 - idRel:0 - idFull:479\n",
            "  21 - sys:2 - size:2 - idRel:1 - idFull:64\n",
            "  22 - sys:1 - size:1 - idRel:1 - idFull:456\n",
            "  23 - sys:0 - size:3 - idRel:0 - idFull:479\n",
            "  24 - sys:0 - size:7 - idRel:2 - idFull:13\n",
            "  25 - sys:2 - size:8 - idRel:1 - idFull:68\n",
            "  26 - sys:1 - size:2 - idRel:2 - idFull:332\n",
            "  27 - sys:0 - size:8 - idRel:1 - idFull:68\n",
            "  28 - sys:2 - size:5 - idRel:2 - idFull:91\n",
            "  29 - sys:0 - size:9 - idRel:0 - idFull:144\n",
            "  30 - sys:1 - size:7 - idRel:1 - idFull:124\n",
            "  31 - sys:1 - size:2 - idRel:0 - idFull:652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process human evaluation sheets"
      ],
      "metadata": {
        "id": "N4XCsaIb1wM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract ratings from spreadsheets (upload CSVs in human-evals folder)\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "#======PARAMS=======\n",
        "path_eval_folder = '/content/human-evals'\n",
        "language = 'ga'#@param['en', 'ga']\n",
        "# first_score_row counter starts at 0\n",
        "annot_id_separator_in_filename = '_'+language+'-'\n",
        "file_extension = 'csv'\n",
        "# In the pilot, the rating values were slightly different, and there was no ID info in the sheets\n",
        "pilot = True#@param{type:'boolean'}\n",
        "allowed_scores = {'Very good: There are no grammatical errors in the text.':5,\n",
        "                  'Very good: The text flows well and can be read easily.':5,\n",
        "                  'Very good: The text is well organised and coherent.': 5,\n",
        "                  'Good: Somewhere between Very good and Fair.':4,\n",
        "                  'Fair: There are a few grammatical errors in the text.':3,\n",
        "                  'Fair: The text has occasional disfluencies.':3,\n",
        "                  'Fair: The text could be better organised for a better coherence.':3,\n",
        "                  'Poor: Somewhere between Very poor and Fair.':2,\n",
        "                  'Very poor: There are numerous grammatical errors in the text.':1,\n",
        "                  'Very poor: The text does not flow well and I had to start over to understand some parts.':1,\n",
        "                  'Very poor: The text is poorly organised and lacks coherence.':1\n",
        "}\n",
        "if pilot == True:\n",
        "  allowed_scores['Good: There are no grammatical errors in the text.'] = 5\n",
        "  allowed_scores['Good: The texts flows well and can be read easily.'] = 5\n",
        "  allowed_scores['Good: The text is well organised and coherent.'] = 5\n",
        "  allowed_scores['Poor: There are numerous grammatical errors in the text.'] = 1\n",
        "  allowed_scores['Poor: The text does not flow well and I had to start over to understand some parts.'] = 1\n",
        "  allowed_scores['Poor: The text is poorly organised and lacks coherence.'] = 1\n",
        "#======PARAMS=======\n",
        "\n",
        "def check_scores(score_list, allowed_scores):\n",
        "  for score in score_list:\n",
        "    # print(score, allowed_scores)\n",
        "    assert score in allowed_scores.keys(), f'Score out of range: {score}'\n",
        "    numerical_score = allowed_scores[score]\n",
        "    return numerical_score\n",
        "\n",
        "paths_files = glob.glob(os.path.join(path_eval_folder, '*.'+file_extension))\n",
        "\n",
        "list_human_eval_datapoints = []\n",
        "for path_file in paths_files:\n",
        "  annot_id = path_file.rsplit(annot_id_separator_in_filename, 1)[1].rsplit('.', 1)[0]\n",
        "  print('Annotator:', annot_id)\n",
        "  df = pd.read_csv(path_file)\n",
        "  # Make a copy of df considering the 6th row as the header row\n",
        "  # 1. Set new headers from the 5th row (index 4)\n",
        "  df.columns = df.iloc[4]\n",
        "  # 2. Drop the first 5 rows (0 to 5) and reset the index\n",
        "  df_noInstructions = df.iloc[5:].reset_index(drop=True)\n",
        "  # print(df_noInstructions)\n",
        "  header_values = list(df.columns.values)\n",
        "  # print(header_values)\n",
        "\n",
        "  x = 0\n",
        "  while x < len(df_noInstructions):\n",
        "    dico_eval_datapoint = {}\n",
        "    gram_score = df_noInstructions.at[x, 'Grammaticality']\n",
        "    flu_score = df_noInstructions.at[x, 'Fluency']\n",
        "    coh_score = df_noInstructions.at[x, 'Coherence']\n",
        "    # Check score and get numerical value\n",
        "    gram_score_num = check_scores([gram_score], allowed_scores)\n",
        "    flu_score_num = check_scores([flu_score], allowed_scores)\n",
        "    coh_score_num = check_scores([coh_score], allowed_scores)\n",
        "    # ID is in column 'G', which has no header because it's kind of hidden (not present in pilot)\n",
        "    id = None\n",
        "    if pilot == True:\n",
        "      id = str(x)+'_0'\n",
        "    else:\n",
        "      system_ID = df_noInstructions.iloc[x, 6].split('-')[1].split('-')[0]\n",
        "      input_id = df_noInstructions.iloc[x, 6].split('-')[0]\n",
        "      id = str(input_id)+'_'+str(system_ID)\n",
        "    print(f'  {x} id={id} - gram: {gram_score_num}; flu: {flu_score_num}; coh: {coh_score_num}')\n",
        "    dico_eval_datapoint['id'] = id\n",
        "    dico_eval_datapoint['input'] = ''\n",
        "    dico_eval_datapoint['output'] = ''\n",
        "    dico_eval_datapoint['grammaticality'] = gram_score_num\n",
        "    dico_eval_datapoint['fluency'] = flu_score_num\n",
        "    dico_eval_datapoint['coherence'] = coh_score_num\n",
        "    dico_eval_datapoint['annotator_id'] = int(annot_id)\n",
        "    list_human_eval_datapoints.append(dico_eval_datapoint)\n",
        "    x += 1\n",
        "\n",
        "# Save list_human_eval_datapoints as json file\n",
        "with open(os.path.join('/content', language+'_annotations.json'), 'w') as f:\n",
        "  json.dump(list_human_eval_datapoints, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "8HC-I2xzpE6V",
        "outputId": "e82f0465-191e-4dc1-a0c2-03a826ed4593"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotator: 4\n",
            "  0 id=0_0 - gram: 3; flu: 3; coh: 3\n",
            "  1 id=1_0 - gram: 3; flu: 5; coh: 3\n",
            "  2 id=2_0 - gram: 5; flu: 5; coh: 5\n",
            "  3 id=3_0 - gram: 3; flu: 5; coh: 3\n",
            "Annotator: 2\n",
            "  0 id=0_0 - gram: 5; flu: 5; coh: 5\n",
            "  1 id=1_0 - gram: 3; flu: 5; coh: 5\n",
            "  2 id=2_0 - gram: 3; flu: 3; coh: 3\n",
            "  3 id=3_0 - gram: 5; flu: 5; coh: 5\n",
            "Annotator: 1\n",
            "  0 id=0_0 - gram: 5; flu: 3; coh: 5\n",
            "  1 id=1_0 - gram: 5; flu: 3; coh: 5\n",
            "  2 id=2_0 - gram: 5; flu: 3; coh: 5\n",
            "  3 id=3_0 - gram: 5; flu: 3; coh: 5\n",
            "Annotator: 3\n",
            "  0 id=0_0 - gram: 3; flu: 5; coh: 5\n",
            "  1 id=1_0 - gram: 5; flu: 3; coh: 3\n",
            "  2 id=2_0 - gram: 3; flu: 3; coh: 3\n",
            "  3 id=3_0 - gram: 3; flu: 3; coh: 3\n",
            "Annotator: 0\n",
            "  0 id=0_0 - gram: 3; flu: 3; coh: 3\n",
            "  1 id=1_0 - gram: 3; flu: 3; coh: 5\n",
            "  2 id=2_0 - gram: 3; flu: 5; coh: 3\n",
            "  3 id=3_0 - gram: 3; flu: 5; coh: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate Fleiss's Kappa\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "path_json = '/content/ga_annotations.json'#@param\n",
        "# num_agg_categories can be 2 or 3 for now (into how many categories should the 7 scores aggregated)\n",
        "num_agg_categories = 2#@param{'type':'integer'}\n",
        "\n",
        "'''\n",
        "Created on Aug 1, 2016\n",
        "@author: skarumbaiah\n",
        "\n",
        "Computes Fleiss' Kappa\n",
        "Joseph L. Fleiss, Measuring Nominal Scale Agreement Among Many Raters, 1971.\n",
        "'''\n",
        "\n",
        "def checkInput(rate, n):\n",
        "    \"\"\"\n",
        "    Check correctness of the input matrix\n",
        "    @param rate - ratings matrix\n",
        "    @return n - number of raters\n",
        "    @throws AssertionError\n",
        "    \"\"\"\n",
        "    N = len(rate)\n",
        "    k = len(rate[0])\n",
        "    assert all(len(rate[i]) == k for i in range(N)), \"Row length != #categories)\"\n",
        "    assert all(isinstance(rate[i][j], int) for i in range(N) for j in range(k)), \"Element not integer\"\n",
        "    assert all(sum(row) == n for row in rate), \"Sum of ratings != #raters)\"\n",
        "\n",
        "def fleissKappaPerplexity(rate, n):\n",
        "    N = len(rate)\n",
        "    k = len(rate[0])\n",
        "    print(f\"#raters = {n}, #subjects = {N}, #categories = {k}\")\n",
        "    checkInput(rate, n)\n",
        "\n",
        "    PA = sum((sum(x*x for x in row) - n) / (n * (n - 1)) for row in rate) / N\n",
        "    print(\"PA =\", PA)\n",
        "\n",
        "    pj = [sum(row[j] for row in rate) / (N * n) for j in range(k)]\n",
        "    PE = sum(p**2 for p in pj)\n",
        "    print(\"PE =\", PE)\n",
        "\n",
        "    try:\n",
        "        kappa = (PA - PE) / (1 - PE)\n",
        "        kappa = float(f\"{kappa:.3f}\")\n",
        "    except ZeroDivisionError:\n",
        "        print(\"Expected agreement = 1\")\n",
        "        kappa = float(\"nan\")\n",
        "\n",
        "    print(\"Fleiss' Kappa =\", kappa)\n",
        "    return kappa\n",
        "\n",
        "def fleissKappa(rate,n):\n",
        "    \"\"\"\n",
        "    Computes the Kappa value\n",
        "    @param rate - ratings matrix containing number of ratings for each subject per category\n",
        "    [size - N X k where N = #subjects and k = #categories]\n",
        "    @param n - number of raters\n",
        "    @return fleiss' kappa\n",
        "    \"\"\"\n",
        "\n",
        "    N = len(rate)\n",
        "    k = len(rate[0])\n",
        "    print(\"#raters (n) = \", n, \", #subjects (N) = \", N, \", #categories (k) = \", k)\n",
        "    checkInput(rate, n)\n",
        "\n",
        "    #mean of the extent to which raters agree for the ith subject\n",
        "    PA = sum([(sum([i**2 for i in row])- n) / (n * (n - 1)) for row in rate])/N\n",
        "    print(\"PA = \", PA)\n",
        "\n",
        "    # mean of squares of proportion of all assignments which were to jth category\n",
        "    PE = sum([j**2 for j in [sum([rows[i] for rows in rate])/(N*n) for i in range(k)]])\n",
        "    print(\"PE =\", PE)\n",
        "\n",
        "    kappa = -float(\"inf\")\n",
        "    try:\n",
        "        kappa = (PA - PE) / (1 - PE)\n",
        "        kappa = float(\"{:.3f}\".format(kappa))\n",
        "    except ZeroDivisionError:\n",
        "        print(\"Expected agreement = 1\")\n",
        "\n",
        "    print(\"Fleiss' Kappa =\", kappa)\n",
        "\n",
        "    return kappa\n",
        "\n",
        "# Code above expects a matrix of d rows and s columns where:\n",
        "# d is the number of data points (also called \"subjects\")\n",
        "# s is the number of categories, i.e. possible scores (in our case, for one system, that's 7: 1 to 7).\n",
        "# Then at each position in the matrix, there should be the number n of annotators who chose the score s for datapoint d.\n",
        "# rate = [\n",
        "#     [2,0,0,0],\n",
        "#     [0,2,0,0],\n",
        "#     [0,0,2,0],\n",
        "#     [0,0,0,2]\n",
        "# ]\n",
        "# kappa = fleissKappa(rate,2)\n",
        "# assert(kappa==1)\n",
        "\n",
        "def count_ratings_exact(ratings_list):\n",
        "  # Create a list with 5 zeros, each index representing counts for 1-5\n",
        "  counts = [0] * 5\n",
        "  for rating in ratings_list:\n",
        "    if 1 <= rating <= 5:\n",
        "      counts[rating - 1] += 1\n",
        "    else:\n",
        "      raise ValueError(f\"Invalid rating: {rating}. Ratings must be between 1 and 5.\")\n",
        "  return counts\n",
        "\n",
        "def count_ratings_aggregate(ratings_list, num_categories):\n",
        "  # Create a list with 3 zeros, each index representing counts for 1-7\n",
        "  counts = [0] * num_categories\n",
        "  for rating in ratings_list:\n",
        "    # print(f'rating: {rating}')\n",
        "    if num_categories == 2:\n",
        "      if rating == 1 or rating == 2:\n",
        "        counts[0] += 1\n",
        "      elif rating == 3 or rating == 4 or rating == 5:\n",
        "        counts[1] += 1\n",
        "    if num_categories == 3:\n",
        "      if rating == 1 or rating == 2:\n",
        "        counts[0] += 1\n",
        "      elif rating == 3:\n",
        "        counts[1] += 1\n",
        "      elif rating == 4 or rating == 5:\n",
        "        counts[2] += 1\n",
        "      else:\n",
        "        raise ValueError(f\"Invalid rating: {rating}. Ratings must be between 1 and 5.\")\n",
        "  # print(counts)\n",
        "  return counts\n",
        "\n",
        "def makeFleissMatrix(json_filepath, num_categories):\n",
        "  # for IAA with all 5 scores\n",
        "  scores_matrix_dico_5scores = {'coherence':[], 'fluency':[], 'grammaticality':[]}\n",
        "  # fro IAA with 3 scores, aggregating 1-2 and 4-5\n",
        "  scores_matrix_dico_Aggscores = {'coherence':[], 'fluency':[], 'grammaticality':[]}\n",
        "  num_annot = 0\n",
        "  with open(json_filepath) as f:\n",
        "    json_data = json.load(f)\n",
        "  df_scores = pd.read_json(json_filepath)\n",
        "  df_scores_per_item = df_scores.groupby(['id'])\n",
        "  # key is the full ID\n",
        "  for key, item in df_scores_per_item:\n",
        "    # print(key)\n",
        "    coh_scores = []\n",
        "    gram_scores = []\n",
        "    flu_scores = []\n",
        "    # Collect scores\n",
        "    for index, row in item.iterrows():\n",
        "      coh_scores.append(row['coherence'])\n",
        "      gram_scores.append(row['grammaticality'])\n",
        "      flu_scores.append(row['fluency'])\n",
        "    assert len(coh_scores) == len(gram_scores) == len(flu_scores)\n",
        "    if num_annot == 0:\n",
        "      num_annot = len(coh_scores)\n",
        "    assert len(coh_scores) == num_annot\n",
        "    # Convert list of scores into a list of count of each score, e.g. [3, 3, 4, 2, 5] -> [0, 1, 2, 1, 1, 0, 0] (7 scores) or  [3, 3, 4, 2, 5] -> [3, 2] (2 scores)\n",
        "    scores_matrix_dico_5scores['coherence'].append(count_ratings_exact(coh_scores))\n",
        "    scores_matrix_dico_5scores['grammaticality'].append(count_ratings_exact(gram_scores))\n",
        "    scores_matrix_dico_5scores['fluency'].append(count_ratings_exact(flu_scores))\n",
        "    scores_matrix_dico_Aggscores['coherence'].append(count_ratings_aggregate(coh_scores, num_categories))\n",
        "    scores_matrix_dico_Aggscores['grammaticality'].append(count_ratings_aggregate(gram_scores, num_categories))\n",
        "    scores_matrix_dico_Aggscores['fluency'].append(count_ratings_aggregate(flu_scores, num_categories))\n",
        "  return scores_matrix_dico_5scores, scores_matrix_dico_Aggscores, num_annot\n",
        "\n",
        "# How many categories do we want for the aggregated scores?\n",
        "scores_matrix_dico_5scores, scores_matrix_dico_Aggscores, num_annot = makeFleissMatrix(path_json, num_agg_categories)\n",
        "\n",
        "print(scores_matrix_dico_5scores)\n",
        "print('Fleiss with 5 scores\\n----------------')\n",
        "for qualityCriterion in scores_matrix_dico_5scores:\n",
        "  print(qualityCriterion)\n",
        "  fleissKappa(scores_matrix_dico_5scores[qualityCriterion], num_annot)\n",
        "\n",
        "print(scores_matrix_dico_Aggscores)\n",
        "print('\\nFleiss with aggregated scores\\n----------------')\n",
        "for qualityCriterion in scores_matrix_dico_Aggscores:\n",
        "  print(qualityCriterion)\n",
        "  fleissKappa(scores_matrix_dico_Aggscores[qualityCriterion], num_annot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "KpMOAeWB9MsM",
        "outputId": "7c1d1685-6983-4a69-8790-e673d55b552b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'coherence': [[0, 0, 2, 0, 3], [0, 0, 2, 0, 3], [0, 0, 3, 0, 2], [0, 0, 3, 0, 2]], 'fluency': [[0, 0, 3, 0, 2], [0, 0, 3, 0, 2], [0, 0, 3, 0, 2], [0, 0, 2, 0, 3]], 'grammaticality': [[0, 0, 3, 0, 2], [0, 0, 3, 0, 2], [0, 0, 3, 0, 2], [0, 0, 3, 0, 2]]}\n",
            "Fleiss with 5 scores\n",
            "----------------\n",
            "coherence\n",
            "#raters (n) =  5 , #subjects (N) =  4 , #categories (k) =  5\n",
            "PA =  0.4\n",
            "PE = 0.5\n",
            "Fleiss' Kappa = -0.2\n",
            "fluency\n",
            "#raters (n) =  5 , #subjects (N) =  4 , #categories (k) =  5\n",
            "PA =  0.4\n",
            "PE = 0.5050000000000001\n",
            "Fleiss' Kappa = -0.212\n",
            "grammaticality\n",
            "#raters (n) =  5 , #subjects (N) =  4 , #categories (k) =  5\n",
            "PA =  0.4\n",
            "PE = 0.52\n",
            "Fleiss' Kappa = -0.25\n",
            "{'coherence': [[0, 5], [0, 5], [0, 5], [0, 5]], 'fluency': [[0, 5], [0, 5], [0, 5], [0, 5]], 'grammaticality': [[0, 5], [0, 5], [0, 5], [0, 5]]}\n",
            "\n",
            "Fleiss with aggregated scores\n",
            "----------------\n",
            "coherence\n",
            "#raters (n) =  5 , #subjects (N) =  4 , #categories (k) =  2\n",
            "PA =  1.0\n",
            "PE = 1.0\n",
            "Expected agreement = 1\n",
            "Fleiss' Kappa = -inf\n",
            "fluency\n",
            "#raters (n) =  5 , #subjects (N) =  4 , #categories (k) =  2\n",
            "PA =  1.0\n",
            "PE = 1.0\n",
            "Expected agreement = 1\n",
            "Fleiss' Kappa = -inf\n",
            "grammaticality\n",
            "#raters (n) =  5 , #subjects (N) =  4 , #categories (k) =  2\n",
            "PA =  1.0\n",
            "PE = 1.0\n",
            "Expected agreement = 1\n",
            "Fleiss' Kappa = -inf\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "a4qti7Qw_Yxo",
        "FynuNJCvC3Du",
        "Vhdqevp_7JZO",
        "NrZKd0bCDP39"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}