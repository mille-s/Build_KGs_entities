{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "a4qti7Qw_Yxo",
        "zxgI7FyZCzog",
        "FynuNJCvC3Du",
        "ixeaiGixDGMB",
        "NrZKd0bCDP39"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/Build_KGs_entities/blob/main/DBpedia_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clones, Installs and functions\n",
        "from IPython.display import clear_output, HTML, display\n",
        "import os\n",
        "! pip install SPARQLWrapper\n",
        "\n",
        "# Clone Build_KGs_entities repo\n",
        "! git clone https://github.com/mille-s/Build_KGs_entities.git\n",
        "# Delete locally to avoid confusion\n",
        "! rm '/content/Build_KGs_entities/DBpedia_dataset.ipynb'\n",
        "\n",
        "# clone wikipedia page generator repo\n",
        "! git clone https://github.com/mille-s/WikipediaPage_Generator.git\n",
        "# Delete locally to avoid confusion\n",
        "! rm 'WikipediaPage_Generator/Wikipedia_generator.ipynb'\n",
        "\n",
        "# clone dcu_tcd_webnlg repo\n",
        "! git clone https://github.com/mille-s/DCU_TCD_FORGe_WebNLG23.git\n",
        "# Delete locally to avoid confusion\n",
        "! rm 'DCU_TCD_FORGe_WebNLG23/DCU_TCD_FORGe_WebNLG23.ipynb'\n",
        "\n",
        "props_list_path = os.path.join('/content', 'DCU_TCD_FORGe_WebNLG23', 'code', 'sorted_properties.txt')\n",
        "\n",
        "triple2predArg = os.path.join('/content', 'XML')\n",
        "os.makedirs(triple2predArg)\n",
        "\n",
        "def aggregate_info_and_get_propLabel(ontology_properties):\n",
        "  dico_properties = {}\n",
        "  for prop in ontology_properties:\n",
        "    # Get raw labels without the url part\n",
        "    _ , property_no_prefix = os.path.split(prop['property'])\n",
        "    domain_no_prefix = 'Unknown'\n",
        "    if not prop['domain'] == 'Unknown':\n",
        "      _ , domain_no_prefix = os.path.split(prop['domain'])\n",
        "    range_no_prefix = 'Unknown'\n",
        "    if not prop['range'] == 'Unknown':\n",
        "      _ , range_no_prefix = os.path.split(prop['range'])\n",
        "    # The first time a property is found, create a dico entry with domain and range info\n",
        "    if property_no_prefix not in dico_properties.keys():\n",
        "      dico_properties[property_no_prefix] = {'domain': [domain_no_prefix], 'range': [range_no_prefix]}\n",
        "    # The second time, only append the domain and range if they haven't been seen to this point\n",
        "    else:\n",
        "      if domain_no_prefix not in dico_properties[property_no_prefix]['domain']:\n",
        "        dico_properties[property_no_prefix]['domain'].append(domain_no_prefix)\n",
        "      if range_no_prefix not in dico_properties[property_no_prefix]['range']:\n",
        "        dico_properties[property_no_prefix]['range'].append(range_no_prefix)\n",
        "  return dico_properties\n",
        "\n",
        "clear_output()\n",
        "print('Working folder ready!\\n--------------')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HN96nrT6_vv2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminary work: get info to build datasets"
      ],
      "metadata": {
        "id": "a4qti7Qw_Yxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SPARQL query to get all properties in DBpedia (from ChatGPT)\n",
        "\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "def getDBpediaProperties():\n",
        "  # Set up the SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query to retrieve properties with the prefix \"http://dbpedia.org/ontology\"\n",
        "  query = \"\"\"\n",
        "  SELECT DISTINCT ?property\n",
        "  WHERE {\n",
        "    ?property a rdf:Property .\n",
        "    FILTER(STRSTARTS(STR(?property), \"http://dbpedia.org/ontology/\"))\n",
        "  }\n",
        "  ORDER BY ?property\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  try:\n",
        "      # Execute the query\n",
        "      results = sparql.query().convert()\n",
        "\n",
        "      # Process and display the results\n",
        "      properties = [result[\"property\"][\"value\"] for result in results[\"results\"][\"bindings\"]]\n",
        "      print(f\"Number of properties used in DBpedia: {str(len(properties))}\")\n",
        "      # for prop in properties:\n",
        "      #     print(prop)\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "\n",
        "  return properties\n",
        "\n",
        "list_properties = getDBpediaProperties()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nrZyZgVBd30D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SPARQL query to get the number of instances of each property\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import os\n",
        "import json\n",
        "\n",
        "def getNumInstancesProperty(property_label):\n",
        "  # lowercase first character\n",
        "  head, tail = os.path.split(property_label)\n",
        "  lowCase_tail = tail[0].lower() + tail[1:]\n",
        "  lowCase_property_label = os.path.join(head, lowCase_tail)\n",
        "\n",
        "  # Set up the SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query to count the number of instances of the property 'dbo:birthDate'\n",
        "  query = f\"\"\"\n",
        "  SELECT (COUNT(*) AS ?count)\n",
        "  WHERE {{\n",
        "    ?subject <{lowCase_property_label}> ?object .\n",
        "  }}\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  try:\n",
        "      # Execute the query\n",
        "      results = sparql.query().convert()\n",
        "\n",
        "      # Extract and print the count\n",
        "      count = results[\"results\"][\"bindings\"][0][\"count\"][\"value\"]\n",
        "      # print(f\"Number of instances of {lowCase_property_label}: {count}\")\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "\n",
        "  return(lowCase_property_label, count)\n",
        "\n",
        "def createDicoCountOccurrenceProperties(list_properties):\n",
        "  dico_count_occurrences = {}\n",
        "  for i, property_label in enumerate(list_properties):\n",
        "    lowCase_property_label, count = getNumInstancesProperty(property_label)\n",
        "    dico_count_occurrences[lowCase_property_label] = int(count)\n",
        "    print(f'{str(i)}/{str(len(list_properties))}: {property_label} = {count}')\n",
        "\n",
        "  sorted_dico_count_occurrences = {k: v for k, v in sorted(dico_count_occurrences.items(), key=lambda item: item[1], reverse=True)}\n",
        "  with open(\"dico_count_occurrences_dbp_props.json\", \"w\") as outfile:\n",
        "      json.dump(sorted_dico_count_occurrences, outfile)\n",
        "\n",
        "createDicoCountOccurrenceProperties(list_properties)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "pc4ucYC1fFB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SPARQL query for all properties getting domain/range class\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "# Set up the DBpedia SPARQL endpoint\n",
        "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "sparql.setReturnFormat(JSON)\n",
        "\n",
        "# SPARQL query to select ontology properties with domain and range\n",
        "# This is supposed to return all properties, but a lot seem to be missing, not sure why\n",
        "query = \"\"\"\n",
        "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "\n",
        "SELECT ?property ?domain ?range WHERE {\n",
        "    ?property a rdf:Property .\n",
        "    FILTER(STRSTARTS(STR(?property), \"http://dbpedia.org/ontology/\")) .\n",
        "    OPTIONAL { ?property rdfs:domain ?domain . }\n",
        "    OPTIONAL { ?property rdfs:range ?range . }\n",
        "}\n",
        "LIMIT 50000  # Increase this limit if needed\n",
        "\"\"\"\n",
        "\n",
        "# Run the query\n",
        "sparql.setQuery(query)\n",
        "results = sparql.query().convert()\n",
        "\n",
        "# Extract and display the results\n",
        "ontology_properties = []\n",
        "for result in results[\"results\"][\"bindings\"]:\n",
        "    property_uri = result[\"property\"][\"value\"]\n",
        "    domain = result.get(\"domain\", {}).get(\"value\", \"Unknown\")\n",
        "    range_class = result.get(\"range\", {}).get(\"value\", \"Unknown\")\n",
        "    ontology_properties.append({\n",
        "        \"property\": property_uri,\n",
        "        \"domain\": domain,\n",
        "        \"range\": range_class\n",
        "    })\n",
        "\n",
        "# Print or process the results\n",
        "for prop in ontology_properties[:100]:\n",
        "    print(f\"Property: {prop['property']}\")\n",
        "    print(f\"  Domain: {prop['domain']}\")\n",
        "    print(f\"  Range: {prop['range']}\")\n",
        "    print()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "w83ObZjQ_Pn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SPARQL query for selected properties getting domain/range class\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "WebNLG_properties_list = ['http://dbpedia.org/ontology/'+line.strip() for line in codecs.open('/content/all_properties.txt', 'r', 'utf-8').readlines()]\n",
        "\n",
        "def get_domain_range(properties):\n",
        "    domain_range_info = []\n",
        "\n",
        "    for property_uri in properties:\n",
        "        print(f'Cheking property {property_uri}')\n",
        "        # SPARQL query to get domain and range for the specific property\n",
        "        query = f\"\"\"\n",
        "        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "\n",
        "        SELECT ?domain ?range WHERE {{\n",
        "            <{property_uri}> a rdf:Property .\n",
        "            OPTIONAL {{ <{property_uri}> rdfs:domain ?domain . }}\n",
        "            OPTIONAL {{ <{property_uri}> rdfs:range ?range . }}\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        # Run the query\n",
        "        sparql.setQuery(query)\n",
        "        results = sparql.query().convert()\n",
        "\n",
        "        # Extract domain and range information from the results\n",
        "        for result in results[\"results\"][\"bindings\"]:\n",
        "            domain = result.get(\"domain\", {}).get(\"value\", \"Unknown\")\n",
        "            range_class = result.get(\"range\", {}).get(\"value\", \"Unknown\")\n",
        "            domain_range_info.append({\n",
        "                \"property\": property_uri,\n",
        "                \"domain\": domain,\n",
        "                \"range\": range_class\n",
        "            })\n",
        "\n",
        "    return domain_range_info\n",
        "\n",
        "# Retrieve domain and range for each property in the list\n",
        "ontology_properties = get_domain_range(WebNLG_properties_list)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "b1Q4FSq8O4BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Aggregate possible domain/ranges for each property\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "dico_properties = aggregate_info_and_get_propLabel(ontology_properties)\n",
        "\n",
        "# for prop in properties_info:\n",
        "#     print(f\"Property: {prop['property']}\", f\"  Domain: {prop['domain']}\", f\"  Range: {prop['range']}\")\n",
        "\n",
        "# print(len(dico_properties.keys()))\n",
        "# print(len(properties_info))\n",
        "\n",
        "list_properties = [line.strip() for line in codecs.open('/content/all_properties.txt', 'r', 'utf-8').readlines()]\n",
        "missing_props = []\n",
        "for WebNLG_property in list_properties:\n",
        "  if WebNLG_property not in dico_properties.keys():\n",
        "    missing_props.append(WebNLG_property)\n",
        "\n",
        "print('Missing properties: '+str(len(missing_props))+'/'+str(len(list_properties)), missing_props)\n",
        "# We need to check the Original property labels, not the modified ones, that way we should get them all\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "cpS46ZYSTpIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SPARQL query for finding all the possible values for gold:hypernym on dbpedia\n",
        "# Could use dbo:type or rdf:type, but both look a bit messy at first sight\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "def get_types():\n",
        "  # Define the SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query\n",
        "  query = \"\"\"\n",
        "  SELECT DISTINCT ?type\n",
        "  WHERE {\n",
        "    ?s gold:hypernym ?type .\n",
        "  }\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query and the return format\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  # Execute the query and retrieve results\n",
        "  # Returns 10k results, and running it several times in a row always returns the same 10k results\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  return results[\"results\"][\"bindings\"]\n",
        "\n",
        "def count_entities_of_type(hypernym_type):\n",
        "  # Define the DBpedia SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query\n",
        "  query = f\"\"\"\n",
        "  SELECT (COUNT(?s) AS ?count)\n",
        "  WHERE {{\n",
        "      ?s gold:hypernym <{hypernym_type}> .\n",
        "  }}\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query and the return format\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  # Execute the query and retrieve results\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  # Extract and return the count\n",
        "  count = results[\"results\"][\"bindings\"][0][\"count\"][\"value\"]\n",
        "  return int(count)\n",
        "\n",
        "dico_hypernym_types = {}\n",
        "# Extract and print the types\n",
        "results_types = get_types()\n",
        "for i, result in enumerate(results_types):\n",
        "  hypernym_url = result['type']['value']\n",
        "  if hypernym_url not in dico_hypernym_types.keys():\n",
        "    count_occurrences = count_entities_of_type(hypernym_url)\n",
        "    dico_hypernym_types[hypernym_url] = count_occurrences\n",
        "    print(f'{str(i)}/{str(len(results_types))}: {hypernym_url} = {count_occurrences}')\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "STMOXxbNJaV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save dico_hypernym_types to a json and download\n",
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "with open(\"dico_hypernym_types.json\", \"w\") as outfile:\n",
        "  sorted_dico_hypernym_types = {k: v for k, v in sorted(dico_hypernym_types.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "  json.dump(sorted_dico_hypernym_types, outfile)\n",
        "\n",
        "# Download\n",
        "files.download('dico_hypernym_types.json')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HrXI2slUTtnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make a list of random entities to query for each hypernym (SPARQL query for getting n entities that have a specific hypernym)\n",
        "import random\n",
        "import json\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "def get_random_entities(hypernym: str, limit):\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "  query = f\"\"\"\n",
        "  SELECT DISTINCT ?entity WHERE {{\n",
        "      ?entity gold:hypernym <{hypernym}> .\n",
        "  }} LIMIT 50000\n",
        "  \"\"\"\n",
        "\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  entities = [result['entity']['value'] for result in results['results']['bindings']]\n",
        "  return random.sample(entities, min(len(entities), limit))\n",
        "\n",
        "# Load json that contains hypernyms as keys and count of instances of that hypernym on DBpedia as value\n",
        "dico_hypernym_types = None\n",
        "with open('/content/dico_hypernym_types_incomplete.json', 'r') as file:\n",
        "    dico_hypernym_types = json.load(file)\n",
        "\n",
        "# Get up to 1000 random entities for the classes that have at least 100 members\n",
        "dico_hypernym_sample_entities = {}\n",
        "for i, hypernym in enumerate(dico_hypernym_types.keys()):\n",
        "  if i < 5:\n",
        "    if dico_hypernym_types[hypernym] >= 100:\n",
        "      print(hypernym, dico_hypernym_types[hypernym])\n",
        "      dico_hypernym_sample_entities[hypernym] = get_random_entities(hypernym, 10)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "svm7JHYlGOyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save dico_hypernym_sample_entities as JSON and download\n",
        "from google.colab import files\n",
        "with open(\"dico_hypernym_sample_entities.json\", \"w\") as outfile:\n",
        "    json.dump(dico_hypernym_sample_entities, outfile)\n",
        "# Dowload json\n",
        "files.download('dico_hypernym_sample_entities.json')"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "yngHyKmiEbQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build dataset"
      ],
      "metadata": {
        "id": "9EWzVztW9vaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get properties for list of entities\n",
        "Creates dico_input_contents_DBp.pickle file.\n",
        "Skip if you want to use an already generated pickle file)"
      ],
      "metadata": {
        "id": "zxgI7FyZCzog"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSw0hkloXnV4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Get DBpedia properties online for an entity list\n",
        "import os\n",
        "import codecs\n",
        "import json\n",
        "import re\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout\n",
        "from WikipediaPage_Generator.code.queryDBpediaProps import get_dbpedia_properties\n",
        "from WikipediaPage_Generator.code.utils import removeReservedCharsFileName\n",
        "\n",
        "# Input json should be a dico_1 with category names (urls or name) as keys, and a list of entities (urls or names) as value.\n",
        "input_json_path = '/content/Build_KGs_entities/resources/GREC_NE.json'#@param{type:\"string\"}\n",
        "# triple-source should be Ontology for this experiment\n",
        "triple_source = 'Ontology' #@param['Infobox', 'Ontology', 'Wikidata']\n",
        "# Store here \"dirty\" properties\n",
        "ignore_properties = 'width, title'\n",
        "get_triples_where_entity_is_subj = True #@param {type:\"boolean\"}\n",
        "get_triples_where_entity_is_obj = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Load json dico with sample entities for each hypernym\n",
        "dico_hypernym_sample_entities_loaded = None\n",
        "with open(input_json_path, 'r') as file:\n",
        "    dico_hypernym_sample_entities_loaded = json.load(file)\n",
        "\n",
        "# dico_input_contents will contain category keys, which contain entity keys, which contain a list of triple objects\n",
        "dico_input_contents = {}\n",
        "for hypernym in sorted(dico_hypernym_sample_entities_loaded.keys()):\n",
        "  input_category = None\n",
        "  if re.search('/', hypernym):\n",
        "    input_category = hypernym.rsplit('/', 1)[1]\n",
        "  else:\n",
        "    input_category = hypernym\n",
        "  print(input_category)\n",
        "  dico_input_contents[input_category] = {}\n",
        "  # Format properties for passing as argument to python module\n",
        "  # list_triple_object contains object with 3 attributes: DBsubj, DBprop, DBobj\n",
        "  # list_propObj is used for UI (for triples selection by the user)\n",
        "  # list_obj is used for getting class and gender info later on\n",
        "  for sampled_entity in sorted(dico_hypernym_sample_entities_loaded[hypernym]):\n",
        "    entity_name = None\n",
        "    if re.search('/', sampled_entity):\n",
        "      entity_name = sampled_entity.rsplit('/', 1)[1]\n",
        "    else:\n",
        "      entity_name = '_'.join(sampled_entity.split(' '))\n",
        "\n",
        "    # Get all triples in which the entity is the subject\n",
        "    list_triple_objects, list_propObj, list_obj = get_dbpedia_properties(props_list_path, entity_name, triple_source, ignore_properties, get_triples_where_entity_is_subj, get_triples_where_entity_is_obj)\n",
        "\n",
        "    if len(list_triple_objects) > 0:\n",
        "      print(f'  {entity_name}: found {len(list_triple_objects)} properties.')\n",
        "      dico_input_contents[input_category][entity_name] = list_triple_objects"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Serialise dico_input_contents using pickle\n",
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "with open(\"dico_input_contents.pickle_DBp\", \"wb\") as handle:\n",
        "    pickle.dump(dico_input_contents, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Download file\n",
        "files.download('dico_input_contents_DBp.pickle')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8bEVe9qef3SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build knowledge graphs #1: WebNLG mirror input configuration"
      ],
      "metadata": {
        "id": "FynuNJCvC3Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check which entities have property (sub)sets that match WebNLG inputs. Creates dico_entities_for_triple_configuration.json file.\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "dico_input_contents = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "print_output = False #@param{type:'boolean'}\n",
        "\n",
        "# dico_input_contents_loaded should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have entity_name as key, and a list of Triple Objects as value.\n",
        "# Triple Objects have 3 attributes: DBsubj, DBprop, DBobj\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "# dico_triple_configs_WebNLG should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have strings of properties separated by \"##\" as keys, and integers (occurrence counts in WebNLG train data) as values.\n",
        "dico_triple_configs_WebNLG = None\n",
        "with open(\"/content/Build_KGs_entities/resources/dico_category_tripleConfigs_WebNLG.json\", \"r\") as handle:\n",
        "    dico_triple_configs_WebNLG = json.load(handle)\n",
        "\n",
        "# dico_mapping_categories = {'City':'Cities'}\n",
        "dico_mapping_categories = {'Person':'People', 'City':'Cities'}\n",
        "\n",
        "# Let's extract which entities have the properties that match a WebNLG configuration. The dico will have: { category: { triple_config: [entity1, entity2, etc.] } }\n",
        "dico_entities_for_triple_configuration = {}\n",
        "print('Finding which entities have the properties that match a WebNLG configuration...')\n",
        "for category_label_WebNLG in dico_mapping_categories.keys():\n",
        "  dico_entities_for_triple_configuration[category_label_WebNLG] = {}\n",
        "  for triple_config_WebNLG in dico_triple_configs_WebNLG[category_label_WebNLG].keys():\n",
        "    # Get input configurations (i.e. property sets) extracted from WebNLG\n",
        "    list_properties_WebNLG = triple_config_WebNLG.split('##')\n",
        "    # print(list_properties_WebNLG)\n",
        "    # Get category label used in GREC\n",
        "    category_label_GREC = dico_mapping_categories[category_label_WebNLG]\n",
        "    # For each GREC entity, extract the set of properties found on DBpedia\n",
        "    for entity_name in dico_input_contents_loaded[category_label_GREC].keys():\n",
        "      # Need a list of strings so we can then convert in sets and compare with other set of property labels\n",
        "      list_properties_entity = []\n",
        "      list_triple_objects_entity = dico_input_contents_loaded[category_label_GREC][entity_name]\n",
        "      for triple_object in list_triple_objects_entity:\n",
        "        list_properties_entity.append(triple_object.DBprop)\n",
        "      # Check if any of the WebNLG triple configurations can be built using the properties of each entity\n",
        "      if set(list_properties_WebNLG).issubset(set(list_properties_entity)):\n",
        "        if triple_config_WebNLG not in dico_entities_for_triple_configuration[category_label_WebNLG].keys():\n",
        "          dico_entities_for_triple_configuration[category_label_WebNLG][triple_config_WebNLG] = []\n",
        "        dico_entities_for_triple_configuration[category_label_WebNLG][triple_config_WebNLG].append(entity_name)\n",
        "\n",
        "# Save dico_entities_for_triple_configuration as json\n",
        "with open(\"dico_entities_for_triple_configuration.json\", \"w\") as outfile:\n",
        "    json.dump(dico_entities_for_triple_configuration, outfile)\n",
        "\n",
        "if print_output == True:\n",
        "  for category_label in dico_entities_for_triple_configuration.keys():\n",
        "    print('============')\n",
        "    print(category_label)\n",
        "    print('============')\n",
        "    for triple_config_overlap in dico_entities_for_triple_configuration[category_label].keys():\n",
        "      print('')\n",
        "      print(triple_config_overlap)\n",
        "      print('----------------------------')\n",
        "      for entity_name in dico_entities_for_triple_configuration[category_label][triple_config_overlap]:\n",
        "        print('-', entity_name)\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "ivwEj5vbNXa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save triple sets in XML format: WebNLG size and input config mirroring\n",
        "# Here we're trying to build a new dataset that has the same properties as in WebNLG, and the same property configurations in the outputs.\n",
        "import pickle\n",
        "import json\n",
        "from WikipediaPage_Generator.code.utils import create_xml, clear_folder\n",
        "\n",
        "dico_input_contents = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "dico_input_entities = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "\n",
        "dico_mapping_categories = {'Person':'People', 'City':'Cities'}\n",
        "\n",
        "# The dico has the following form: { category: { triple_config: [entity1, entity2, etc.] } }\n",
        "dico_entities_for_triple_configuration_l = None\n",
        "if dico_input_entities == 'Made_with_this_notebook':\n",
        "  dico_entities_for_triple_configuration_l = json.load(open('/content/dico_entities_for_triple_configuration.json', 'r'))\n",
        "elif dico_input_entities == 'GitHub_GREC_NEs':\n",
        "  dico_entities_for_triple_configuration_l = json.load(open('/content/Build_KGs_entities/resources/dico_entities_for_triple_configuration_GREC_NEs.json', 'r'))\n",
        "\n",
        "# dico_input_contents_loaded should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have entity_name as key, and a list of Triple Objects as value.\n",
        "# Triple Objects have 3 attributes: DBsubj, DBprop, DBobj\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "counter_datapoints = 0\n",
        "# Keep track of how many times an entity is used for an XML, so we can number the inputs corresponding to the same entity (an XML is named after the entity name)\n",
        "entity_counter = {}\n",
        "for category_l in dico_entities_for_triple_configuration_l:\n",
        "  print(category_l)\n",
        "  # Prepare output folder\n",
        "  clear_folder(os.path.join(triple2predArg, category_l))\n",
        "  # if not os.path.exists(os.path.join(triple2predArg, input_category)):\n",
        "  os.makedirs(os.path.join(triple2predArg, category_l))\n",
        "  catregory_grec = dico_mapping_categories[category_l]\n",
        "  for triple_config in dico_entities_for_triple_configuration_l[category_l]:\n",
        "    print('  ', triple_config, len(dico_entities_for_triple_configuration_l[category_l][triple_config]))\n",
        "    property_list_l = triple_config.split('##')\n",
        "    # print(f'{category_l}: {triple_config}: {len(dico_entities_for_triple_configuration_l[category_l][triple_config])}')\n",
        "    for entity_name in dico_entities_for_triple_configuration_l[category_l][triple_config]:\n",
        "      # Make filename by using entity name + number of times that entity is being used\n",
        "      if entity_name not in entity_counter.keys():\n",
        "        entity_counter[entity_name] = 0\n",
        "      else:\n",
        "        entity_counter[entity_name] += 1\n",
        "      filename = entity_name+'_'+str(entity_counter[entity_name])\n",
        "      list_triple_objects = dico_input_contents_loaded[catregory_grec][entity_name]\n",
        "      list_selected_triple_objects = []\n",
        "      for triple_object in list_triple_objects:\n",
        "        found_prop = False\n",
        "        if triple_object.DBprop in property_list_l:\n",
        "          # print(f'      {entity_name}: found {triple_object.DBprop}.')\n",
        "          if found_prop == False:\n",
        "            list_selected_triple_objects.append(triple_object)\n",
        "            found_prop = True\n",
        "      # The function that builds an XML expects a list of list IDs that correspond to selected triples. In this context, we want all triples.\n",
        "      properties_selected = [i for i in range(len(property_list_l))]\n",
        "      # create xml file passing the entity name to use as filename\n",
        "      counter_datapoints += 1\n",
        "      list_triples_text = create_xml(list_selected_triple_objects, properties_selected, category_l, os.path.join(triple2predArg, category_l), entity_name=filename, eid = counter_datapoints)\n",
        "\n",
        "print(f'----------\\n{counter_datapoints} new datapoints were created in total.')"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "IWkIclvxbM-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build knowledge graphs #2: WebNLG mirror input size distribution only"
      ],
      "metadata": {
        "id": "Vhdqevp_7JZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pseudo-code\n",
        "\n",
        "# Initialize empty dictionary dico_length_ratio_entity\n",
        "\n",
        "# For each entity in the dataset:\n",
        "#     Initialize two lists:\n",
        "#         subject_triples = []  // Triples where the entity is the subject\n",
        "#         object_triples = []   // Triples where the entity is the object\n",
        "\n",
        "#     For each triple related to the entity:\n",
        "#         If entity is the subject:\n",
        "#             Add triple to subject_triples\n",
        "#         Else if entity is the object:\n",
        "#             Add triple to object_triples\n",
        "\n",
        "#     // Create property-based dictionaries for subjects and objects\n",
        "#     subject_property_dict = Group subject_triples by property\n",
        "#     object_property_dict = Group object_triples by property\n",
        "\n",
        "#     For each desired input length (e.g., 1 to N triples, in our case N=7):\n",
        "#         For each possible subject/object ratio (e.g., 1:2, 2:1, etc.):\n",
        "#             // Ensure at least one triple has the entity as subject (Constraint 2)\n",
        "#             subject_count = number of triples to select as subject\n",
        "#             object_count = input_length - subject_count\n",
        "\n",
        "#             possible_subject_triples = []\n",
        "#             possible_object_triples = []\n",
        "\n",
        "#             For subject_properties in subject_property_dict:\n",
        "#                 Randomly select up to 2 triples per property (Constraint 3)\n",
        "#                 Add to possible_subject_triples\n",
        "\n",
        "#             For object_properties in object_property_dict:\n",
        "#                 Randomly select up to 2 triples per property (Constraint 3)\n",
        "#                 Add to possible_object_triples\n",
        "\n",
        "#             selected_triples = []\n",
        "#             Randomly select subject_count triples from possible_subject_triples\n",
        "#             Add to selected_triples\n",
        "#             Randomly select object_count triples from possible_object_triples\n",
        "#             Add to selected_triples\n",
        "\n",
        "#             Shuffle selected_triples\n",
        "\n",
        "#             Store selected_triples in dico_length_ratio_entity\n",
        "\n",
        "# Sample randomly from dico_length_ratio_entity using WebNLG's input length distribution"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mauku5SQDRq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check triples extracted from DBpedia\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# dico_input_contents_loaded should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have entity_name as key, and a list of Triple Objects as value.\n",
        "# Triple Objects have 3 attributes: DBsubj, DBprop, DBobj\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "dico_properties = {}\n",
        "dico_different_properties = {}\n",
        "dico_entity_as_subj = {}\n",
        "dico_entity_as_obj = {}\n",
        "for category in dico_input_contents_loaded.keys():\n",
        "  # print(category)\n",
        "  for entity_name in dico_input_contents_loaded[category].keys():\n",
        "    all_properties = []\n",
        "    different_properties = []\n",
        "    subj_of_properties = []\n",
        "    obj_of_properties = []\n",
        "    # print('  ', entity_name)\n",
        "    for triple_object in dico_input_contents_loaded[category][entity_name]:\n",
        "      all_properties.append(triple_object.DBprop)\n",
        "      if triple_object.DBprop not in different_properties:\n",
        "        different_properties.append(triple_object.DBprop)\n",
        "      if triple_object.DBsubj == entity_name:\n",
        "        subj_of_properties.append(triple_object.DBprop)\n",
        "      if triple_object.DBobj == entity_name:\n",
        "        obj_of_properties.append(triple_object.DBprop)\n",
        "    dico_properties[entity_name] = len(all_properties)\n",
        "    dico_different_properties[entity_name] = len(different_properties)\n",
        "    dico_entity_as_subj[entity_name] = len(subj_of_properties)\n",
        "    dico_entity_as_obj[entity_name] = len(obj_of_properties)\n",
        "    # print(f'    {len(different_properties)} different properties')\n",
        "    # print(f'    {len(subj_of_properties)} properties with {entity_name} as subject')\n",
        "    # print(f'    {len(obj_of_properties)} properties with {entity_name} as object')\n",
        "\n",
        "dico_properties_sorted = {k: v for k, v in sorted(dico_properties.items(), key=lambda item: item[1], reverse=True)}\n",
        "dico_different_properties_sorted = {k: v for k, v in sorted(dico_different_properties.items(), key=lambda item: item[1], reverse=True)}\n",
        "dico_entity_as_subj_sorted = {k: v for k, v in sorted(dico_entity_as_subj.items(), key=lambda item: item[1], reverse=True)}\n",
        "dico_entity_as_obj_sorted = {k: v for k, v in sorted(dico_entity_as_obj.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "# Plot dico_properties dictionary using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(dico_properties_sorted.keys(), dico_properties_sorted.values())\n",
        "plt.xlabel('Entity Name')\n",
        "plt.ylabel('Number of Properties')\n",
        "plt.show()\n",
        "\n",
        "# Plot dico_different_properties dictionary using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(dico_different_properties_sorted.keys(), dico_different_properties_sorted.values())\n",
        "plt.xlabel('Entity Name')\n",
        "plt.ylabel('Number of Different Properties')\n",
        "plt.show()\n",
        "\n",
        "# Plot dico_entity_as_subj_sorted dictionary using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(dico_entity_as_subj_sorted.keys(), dico_entity_as_subj_sorted.values())\n",
        "plt.xlabel('Entity Name')\n",
        "plt.ylabel('Number of Properties with Entity as Subject')\n",
        "plt.show()\n",
        "\n",
        "# Plot dico_entity_as_obj_sorted dictionary using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(dico_entity_as_obj_sorted.keys(), dico_entity_as_obj_sorted.values())\n",
        "plt.xlabel('Entity Name')\n",
        "plt.ylabel('Number of Properties with Entity as Object')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "5gSdmLEM7isu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save triple sets in XML format: Implementation of the algorithm for the search+triples sampling\n",
        "import pickle\n",
        "import random\n",
        "import json\n",
        "from WikipediaPage_Generator.code.utils import create_xml, clear_folder\n",
        "random.seed(785)\n",
        "\n",
        "dico_input_contents = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "suffle_selected_triples = True #@param{type:'boolean'}\n",
        "final_dataset_size = 100 #@param{type:'integer'}\n",
        "MAX_TRIPLES_SET_LENGTH = 7\n",
        "MAX_TRIPLES_PER_PROPERTY = 2\n",
        "DEBUG = True#@param{type:'boolean'}\n",
        "\n",
        "def group_triples_by_property(triples):\n",
        "  property_dict = {}\n",
        "  for triple in triples:\n",
        "    if triple.DBprop not in property_dict:\n",
        "      property_dict[triple.DBprop] = []\n",
        "    property_dict[triple.DBprop].append(triple)\n",
        "  return property_dict\n",
        "\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "print(f'Triples for {len(dico_input_contents_loaded[\"Cities\"].keys())+len(dico_input_contents_loaded[\"People\"].keys())} entities were found.')\n",
        "\n",
        "all_triples_sets = []\n",
        "for category_name, entities in dico_input_contents_loaded.items():\n",
        "  for entity_name, triples in entities.items():\n",
        "    if DEBUG:\n",
        "      print('\\n')\n",
        "      print(f'Entity name: {entity_name}')\n",
        "      print('', f'# Triples available: {len(triples)}')\n",
        "    # Extract all the triples that have the current entity as subject\n",
        "    subject_triples = [tri for tri in triples if tri.DBsubj == entity_name]\n",
        "    # Extract all the triples that have the current entity as object\n",
        "    object_triples = [tri for tri in triples if tri.DBobj == entity_name]\n",
        "    if DEBUG:\n",
        "      print('  ', f'# Triples with entity as subject: {len(subject_triples)}')\n",
        "      print('    ', [f\"{o.DBsubj} {o.DBprop} {o.DBobj}\" for o in subject_triples[:10]])\n",
        "      print('  ', f'# Triples with entity as object: {len(object_triples)}')\n",
        "      print('    ', [f\"{o.DBsubj} {o.DBprop} {o.DBobj}\" for o in object_triples[:10]])\n",
        "\n",
        "    # Group subject_triples by property to be able to select randomly a maximum of N triples with the same property (set in MAX_TRIPLES_PER_PROPERTY)\n",
        "    subject_property_dict = group_triples_by_property(subject_triples)\n",
        "    # Group object_triples by property\n",
        "    object_property_dict = group_triples_by_property(object_triples)\n",
        "    if DEBUG:\n",
        "      print('  ', f'# Unique properties entity as subj: {len(subject_property_dict)}')\n",
        "      print('  ', f'# Unique properties entity as obj: {len(object_property_dict)}')\n",
        "\n",
        "    # create one triple set for each triple set length and ratio\n",
        "    for triples_len in range(1, MAX_TRIPLES_SET_LENGTH+1):\n",
        "      if DEBUG:\n",
        "        print(\"Triple set length:\", triples_len)\n",
        "\n",
        "      # the subject/object ratio is extracted considering subject count between 1 (possibly change to 0?)\n",
        "      # (at least one triples with the current entity as subject must be in the triples set)\n",
        "      # and the triples set length, while object count is triples set length - subject count\n",
        "      # together they give the subject/object ratio\n",
        "      for subj_count in range(1, triples_len+1):\n",
        "        obj_count = triples_len - subj_count\n",
        "        if DEBUG:\n",
        "          print(f\"  Subject count: {subj_count}, Object count: {obj_count}\")\n",
        "\n",
        "        # extracted the possible triples with the entity as subject\n",
        "        # for each property, select up to 2 triples, this gives us the complete\n",
        "        # set of triples with entity as subject from which randomly select the\n",
        "        # triples for the final triples set\n",
        "        possible_subject_triples = []\n",
        "        for prop, prop_triples in subject_property_dict.items():\n",
        "          possible_subject_triples.extend(random.sample(prop_triples, min(MAX_TRIPLES_PER_PROPERTY, len(prop_triples))))\n",
        "        if DEBUG:\n",
        "          print('   ', [f\"{o.DBsubj} {o.DBprop} {o.DBobj}\" for o in possible_subject_triples])\n",
        "\n",
        "        # extracted the possible triples with the entity as object\n",
        "        # for each property, select up to 2 triples, this gives us the complete\n",
        "        # set of triples with entity as object from which randomly select the\n",
        "        # triples for the final triples set\n",
        "        possible_object_triples = []\n",
        "        for prop, prop_triples in object_property_dict.items():\n",
        "          possible_object_triples.extend(random.sample(prop_triples, min(MAX_TRIPLES_PER_PROPERTY, len(prop_triples))))\n",
        "\n",
        "        # check that we have enough triples to select\n",
        "        if len(possible_subject_triples) >= subj_count and len(possible_object_triples) >= obj_count:\n",
        "          selected_triples = []\n",
        "          # select subj_count triples where the entity is subject\n",
        "          selected_triples.extend(random.sample(possible_subject_triples, subj_count))\n",
        "          # select obj_count triples where the entity is object\n",
        "          selected_triples.extend(random.sample(possible_object_triples, obj_count))\n",
        "          # shuffle the selected triples\n",
        "          if suffle_selected_triples:\n",
        "            random.shuffle(selected_triples)\n",
        "          all_triples_sets.append({\n",
        "              'triples': selected_triples,\n",
        "              'subj_count': subj_count,\n",
        "              'obj_count': obj_count,\n",
        "              'triples_len': triples_len,\n",
        "              'entity_name': entity_name,\n",
        "              'category_name': category_name\n",
        "          })\n",
        "\n",
        "# sample all_triples_sets to reflect the same triples_len as WebNLG and specific\n",
        "# TODO replace with automatic extraction of real WebNLG distribution\n",
        "# IMPORTANT: Express distribution in %\n",
        "distr = {1: 20.8, 2: 19.6, 3: 19.6, 4: 17.2, 5: 12, 6: 6.4, 7: 4.4}\n",
        "total_prob = int(sum(distr.values()))\n",
        "# print(total_prob)\n",
        "assert total_prob == 100, \"Total probability should be 100%\"\n",
        "\n",
        "sampled_triple_sets = []\n",
        "for triples_len, prob in distr.items():\n",
        "  # Select any triple set that has the expected size (allows for multiple triple sets per entity)\n",
        "  triples_of_len = [tri for tri in all_triples_sets if tri['triples_len'] == triples_len]\n",
        "  num_to_sample = round(final_dataset_size * prob / 100)\n",
        "  if num_to_sample <= len(triples_of_len):\n",
        "    sampled_triple_sets.extend(random.sample(triples_of_len, num_to_sample))\n",
        "  else:\n",
        "    print(f'!!! Could not select triples sets of size {triples_len} (not enough triple sets).')\n",
        "  print(f'Length: {triples_len}')\n",
        "  print(f'  # Total triple sets of current size: {len(triples_of_len)}')\n",
        "  print(f'  # Selected triple sets of current size: {num_to_sample}')\n",
        "  print(f'  # Total Selected triples at this point: {len(sampled_triple_sets)}')\n",
        "\n",
        "print(f'# Selected triple sets: {len(sampled_triple_sets)}')\n",
        "print(sampled_triple_sets[:10])\n",
        "\n",
        "## Number of triples for each category\n",
        "# print(f'# Triple sets People: {len([tri for tri in sampled_triples if tri[\"category_name\"]==\"People\"])}')\n",
        "# print(f'# Triple sets Cities:{len([tri for tri in sampled_triples if tri[\"category_name\"]==\"Cities\"])}')\n",
        "## Total number of triple sets\n",
        "# print(f'# Triple sets before sampling: {len(all_triples_sets)}')\n",
        "# print(all_triples_sets[:10])\n",
        "\n",
        "# Save datapoints in individual XML files\n",
        "counter_datapoints = 0\n",
        "entity_counter = {}\n",
        "folder_name = ''\n",
        "# print(folder_name)\n",
        "clear_folder(triple2predArg)\n",
        "os.makedirs(os.path.join(triple2predArg, 'People'))\n",
        "os.makedirs(os.path.join(triple2predArg, 'Cities'))\n",
        "for sampled_triple_set in sampled_triple_sets:\n",
        "  counter_datapoints += 1\n",
        "  # print(counter_datapoints)\n",
        "  list_triple_objects = sampled_triple_set['triples']\n",
        "  properties_selected = [i for i in range(len(sampled_triple_set['triples']))]\n",
        "  input_category = sampled_triple_set['category_name']\n",
        "  folder_name = input_category\n",
        "  entity_name = sampled_triple_set['entity_name']\n",
        "  if entity_name not in entity_counter.keys():\n",
        "    entity_counter[entity_name] = 0\n",
        "  else:\n",
        "    entity_counter[entity_name] += 1\n",
        "  filename = entity_name+'_'+str(entity_counter[entity_name])\n",
        "  # create xml file passing the entity name to use as filename\n",
        "  list_triples_text = create_xml(list_triple_objects, properties_selected, input_category, os.path.join(triple2predArg, folder_name), entity_name=filename, eid = counter_datapoints)\n",
        "\n",
        "# print(sampled_triple_sets[:10])\n",
        "# TODO save created dataset preprocessing the triples (we cannot save the object as it is)\n",
        "# for i, tri in enumerate(sampled_triple_sets):\n",
        "#   tri['triples'] = [f\"{o.DBsubj} | {o.DBprop} | {o.DBobj}\" for o in tri['triples']]\n",
        "# with open('/content/sampled_triples.json', 'w') as f:\n",
        "#   json.dump(sampled_triple_sets, f)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VGse5Am5TWYd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build knowledge graphs #3: free distribution"
      ],
      "metadata": {
        "id": "ixeaiGixDGMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save triple set in XML format: Large DBpedia dataset\n",
        "from WikipediaPage_Generator.code.utils import get_first_n_instances_of_props, create_xml, clear_folder\n",
        "\n",
        "dico_input_contents = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "\n",
        "# Specifies the maximum number of occurrences of each property in the triple set\n",
        "max_num_of_instances_of_prop_desired = \"3\" #@param[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "############### TO COMPLETE\n",
        "############### TO COMPLETE\n",
        "############### TO COMPLETE\n",
        "############### TO COMPLETE\n",
        "############### TO COMPLETE\n",
        "# List here properties that cannot have 2 values (to filter bad stuff from i.e. infobox)\n",
        "properties_that_can_happen_once_only = ['budget', 'gross', 'imdbId', 'length', 'runtime']\n",
        "############### TO COMPLETE\n",
        "############### TO COMPLETE\n",
        "############### TO COMPLETE\n",
        "############### TO COMPLETE\n",
        "############### TO COMPLETE\n",
        "import pickle\n",
        "\n",
        "# Input data structure should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have entity_name as key, and a list of Triple Objects as value.\n",
        "# Triple Objects have 3 attributes: DBsubj, DBprop, DBobj\n",
        "counter_datapoints = 0\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "for input_category in dico_input_contents_loaded.keys():\n",
        "  print(input_category)\n",
        "  folder_name = input_category+'_'+str(max_num_of_instances_of_prop_desired)\n",
        "  clear_folder(os.path.join(triple2predArg, folder_name))\n",
        "  # if not os.path.exists(os.path.join(triple2predArg, input_category)):\n",
        "  os.makedirs(os.path.join(triple2predArg, folder_name))\n",
        "  for entity_name in dico_input_contents_loaded[input_category].keys():\n",
        "    # print('  ', entity_name, len(dico_input_contents_loaded[input_category][entity_name]))\n",
        "    # for triple_object in dico_input_contents_loaded[category][entity]:\n",
        "      # print('  ', triple_object.DBprop)\n",
        "    list_triple_objects = dico_input_contents_loaded[input_category][entity_name]\n",
        "    # Generate list of indices of properties selected by user (index in the list of Triple objects that contains all retrieved triples)\n",
        "    properties_selected = get_first_n_instances_of_props(list_triple_objects, int(max_num_of_instances_of_prop_desired), properties_that_can_happen_once_only)\n",
        "    print(f'  {entity_name}: selected {len(properties_selected)}/{len(dico_input_contents_loaded[input_category][entity_name])} properties.')\n",
        "\n",
        "    if len(properties_selected) > 0:\n",
        "      # create xml file passing the entity name to use as filename\n",
        "      counter_datapoints += 1\n",
        "      list_triples_text = create_xml(list_triple_objects, properties_selected, input_category, os.path.join(triple2predArg, folder_name), entity_name=entity_name, eid = counter_datapoints)\n",
        "\n",
        "print(f'----------\\n{counter_datapoints} new datapoints were created in total.')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hxGzSaD3Czzq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process output XMLs (group, etc.)\n",
        "Stratified sampling info 100 inputs:\n",
        "1:20.8 - 2:19.6 - 3:19.6 - 4:17.2 - 5:12 - 6:6.4 - 7:4.4"
      ],
      "metadata": {
        "id": "NrZKd0bCDP39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Put all XMLs in the same file for sampling\n",
        "import glob\n",
        "\n",
        "paths_folders_categories = glob.glob('/content/XML/*')\n",
        "with open('/content/XML/D2T-1-FA_SemAccExperiments.xml', 'w') as f:\n",
        "  f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
        "  f.write('<benchmark>\\n')\n",
        "  f.write('  <entries>\\n')\n",
        "  for path_folder_category in sorted(paths_folders_categories):\n",
        "    list_XMLS_for_category = glob.glob(os.path.join(path_folder_category, '*.xml'))\n",
        "    for XML in sorted(list_XMLS_for_category):\n",
        "      with open(XML, 'r') as file:\n",
        "        XML_lines = file.readlines()\n",
        "        for line in XML_lines:\n",
        "          if not line.startswith('<') and not line.startswith('  <'):\n",
        "            f.write(line)\n",
        "  f.write('  </entries>\\n')\n",
        "  f.write('</benchmark>\\n')\n",
        "print('Created XML file!')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4AXBt6r7QcH3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}