{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/Build_KGs_entities/blob/main/DBpedia_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "HN96nrT6_vv2"
      },
      "outputs": [],
      "source": [
        "#@title Clones, Installs and functions\n",
        "from IPython.display import clear_output, HTML, display\n",
        "import os\n",
        "! pip install SPARQLWrapper\n",
        "! pip install colored\n",
        "! pip install xmltodict\n",
        "\n",
        "# Clone Build_KGs_entities repo\n",
        "! git clone https://github.com/mille-s/Build_KGs_entities.git\n",
        "# Delete locally to avoid confusion\n",
        "! rm '/content/Build_KGs_entities/DBpedia_dataset.ipynb'\n",
        "\n",
        "# clone wikipedia page generator repo\n",
        "! git clone https://github.com/mille-s/WikipediaPage_Generator.git\n",
        "# Delete locally to avoid confusion\n",
        "! rm 'WikipediaPage_Generator/Wikipedia_generator.ipynb'\n",
        "\n",
        "# clone dcu_tcd_webnlg repo\n",
        "! git clone https://github.com/mille-s/DCU_TCD_FORGe_WebNLG23.git\n",
        "# Delete locally to avoid confusion\n",
        "! rm 'DCU_TCD_FORGe_WebNLG23/DCU_TCD_FORGe_WebNLG23.ipynb'\n",
        "\n",
        "triple2predArg = os.path.join('/content', 'XML')\n",
        "os.makedirs(triple2predArg)\n",
        "\n",
        "def aggregate_info_and_get_propLabel(ontology_properties):\n",
        "  dico_properties = {}\n",
        "  for prop in ontology_properties:\n",
        "    # Get raw labels without the url part\n",
        "    _ , property_no_prefix = os.path.split(prop['property'])\n",
        "    domain_no_prefix = 'Unknown'\n",
        "    if not prop['domain'] == 'Unknown':\n",
        "      _ , domain_no_prefix = os.path.split(prop['domain'])\n",
        "    range_no_prefix = 'Unknown'\n",
        "    if not prop['range'] == 'Unknown':\n",
        "      _ , range_no_prefix = os.path.split(prop['range'])\n",
        "    # The first time a property is found, create a dico entry with domain and range info\n",
        "    if property_no_prefix not in dico_properties.keys():\n",
        "      dico_properties[property_no_prefix] = {'domain': [domain_no_prefix], 'range': [range_no_prefix]}\n",
        "    # The second time, only append the domain and range if they haven't been seen to this point\n",
        "    else:\n",
        "      if domain_no_prefix not in dico_properties[property_no_prefix]['domain']:\n",
        "        dico_properties[property_no_prefix]['domain'].append(domain_no_prefix)\n",
        "      if range_no_prefix not in dico_properties[property_no_prefix]['range']:\n",
        "        dico_properties[property_no_prefix]['range'].append(range_no_prefix)\n",
        "  return dico_properties\n",
        "\n",
        "clear_output()\n",
        "print('Working folder ready!\\n--------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4qti7Qw_Yxo"
      },
      "source": [
        "# Preliminary work: get info to build datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nrZyZgVBd30D"
      },
      "outputs": [],
      "source": [
        "#@title SPARQL query to get all properties in DBpedia (from ChatGPT)\n",
        "\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "def getDBpediaProperties():\n",
        "  # Set up the SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query to retrieve properties with the prefix \"http://dbpedia.org/ontology\"\n",
        "  query = \"\"\"\n",
        "  SELECT DISTINCT ?property\n",
        "  WHERE {\n",
        "    ?property a rdf:Property .\n",
        "    FILTER(STRSTARTS(STR(?property), \"http://dbpedia.org/ontology/\"))\n",
        "  }\n",
        "  ORDER BY ?property\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  try:\n",
        "      # Execute the query\n",
        "      results = sparql.query().convert()\n",
        "\n",
        "      # Process and display the results\n",
        "      properties = [result[\"property\"][\"value\"] for result in results[\"results\"][\"bindings\"]]\n",
        "      print(f\"Number of properties used in DBpedia: {str(len(properties))}\")\n",
        "      # for prop in properties:\n",
        "      #     print(prop)\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "\n",
        "  return properties\n",
        "\n",
        "list_properties = getDBpediaProperties()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "pc4ucYC1fFB1"
      },
      "outputs": [],
      "source": [
        "#@title SPARQL query to get the number of instances of each property\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import os\n",
        "import json\n",
        "\n",
        "def getNumInstancesProperty(property_label):\n",
        "  # lowercase first character\n",
        "  head, tail = os.path.split(property_label)\n",
        "  lowCase_tail = tail[0].lower() + tail[1:]\n",
        "  lowCase_property_label = os.path.join(head, lowCase_tail)\n",
        "\n",
        "  # Set up the SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query to count the number of instances of the property 'dbo:birthDate'\n",
        "  query = f\"\"\"\n",
        "  SELECT (COUNT(*) AS ?count)\n",
        "  WHERE {{\n",
        "    ?subject <{lowCase_property_label}> ?object .\n",
        "  }}\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  try:\n",
        "      # Execute the query\n",
        "      results = sparql.query().convert()\n",
        "\n",
        "      # Extract and print the count\n",
        "      count = results[\"results\"][\"bindings\"][0][\"count\"][\"value\"]\n",
        "      # print(f\"Number of instances of {lowCase_property_label}: {count}\")\n",
        "\n",
        "  except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "\n",
        "  return(lowCase_property_label, count)\n",
        "\n",
        "def createDicoCountOccurrenceProperties(list_properties):\n",
        "  dico_count_occurrences = {}\n",
        "  for i, property_label in enumerate(list_properties):\n",
        "    lowCase_property_label, count = getNumInstancesProperty(property_label)\n",
        "    dico_count_occurrences[lowCase_property_label] = int(count)\n",
        "    print(f'{str(i)}/{str(len(list_properties))}: {property_label} = {count}')\n",
        "\n",
        "  sorted_dico_count_occurrences = {k: v for k, v in sorted(dico_count_occurrences.items(), key=lambda item: item[1], reverse=True)}\n",
        "  with open(\"dico_count_occurrences_dbp_props.json\", \"w\") as outfile:\n",
        "      json.dump(sorted_dico_count_occurrences, outfile)\n",
        "\n",
        "createDicoCountOccurrenceProperties(list_properties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "w83ObZjQ_Pn8"
      },
      "outputs": [],
      "source": [
        "#@title SPARQL query for all properties getting domain/range class\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "# Set up the DBpedia SPARQL endpoint\n",
        "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "sparql.setReturnFormat(JSON)\n",
        "\n",
        "# SPARQL query to select ontology properties with domain and range\n",
        "# This is supposed to return all properties, but a lot seem to be missing, not sure why\n",
        "query = \"\"\"\n",
        "PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "\n",
        "SELECT ?property ?domain ?range WHERE {\n",
        "    ?property a rdf:Property .\n",
        "    FILTER(STRSTARTS(STR(?property), \"http://dbpedia.org/ontology/\")) .\n",
        "    OPTIONAL { ?property rdfs:domain ?domain . }\n",
        "    OPTIONAL { ?property rdfs:range ?range . }\n",
        "}\n",
        "LIMIT 50000  # Increase this limit if needed\n",
        "\"\"\"\n",
        "\n",
        "# Run the query\n",
        "sparql.setQuery(query)\n",
        "results = sparql.query().convert()\n",
        "\n",
        "# Extract and display the results\n",
        "ontology_properties = []\n",
        "for result in results[\"results\"][\"bindings\"]:\n",
        "    property_uri = result[\"property\"][\"value\"]\n",
        "    domain = result.get(\"domain\", {}).get(\"value\", \"Unknown\")\n",
        "    range_class = result.get(\"range\", {}).get(\"value\", \"Unknown\")\n",
        "    ontology_properties.append({\n",
        "        \"property\": property_uri,\n",
        "        \"domain\": domain,\n",
        "        \"range\": range_class\n",
        "    })\n",
        "\n",
        "# Print or process the results\n",
        "for prop in ontology_properties[:100]:\n",
        "    print(f\"Property: {prop['property']}\")\n",
        "    print(f\"  Domain: {prop['domain']}\")\n",
        "    print(f\"  Range: {prop['range']}\")\n",
        "    print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "b1Q4FSq8O4BU"
      },
      "outputs": [],
      "source": [
        "#@title SPARQL query for selected properties getting domain/range class\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "WebNLG_properties_list = ['http://dbpedia.org/ontology/'+line.strip() for line in codecs.open('/content/all_properties.txt', 'r', 'utf-8').readlines()]\n",
        "\n",
        "def get_domain_range(properties):\n",
        "    domain_range_info = []\n",
        "\n",
        "    for property_uri in properties:\n",
        "        print(f'Cheking property {property_uri}')\n",
        "        # SPARQL query to get domain and range for the specific property\n",
        "        query = f\"\"\"\n",
        "        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "\n",
        "        SELECT ?domain ?range WHERE {{\n",
        "            <{property_uri}> a rdf:Property .\n",
        "            OPTIONAL {{ <{property_uri}> rdfs:domain ?domain . }}\n",
        "            OPTIONAL {{ <{property_uri}> rdfs:range ?range . }}\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        # Run the query\n",
        "        sparql.setQuery(query)\n",
        "        results = sparql.query().convert()\n",
        "\n",
        "        # Extract domain and range information from the results\n",
        "        for result in results[\"results\"][\"bindings\"]:\n",
        "            domain = result.get(\"domain\", {}).get(\"value\", \"Unknown\")\n",
        "            range_class = result.get(\"range\", {}).get(\"value\", \"Unknown\")\n",
        "            domain_range_info.append({\n",
        "                \"property\": property_uri,\n",
        "                \"domain\": domain,\n",
        "                \"range\": range_class\n",
        "            })\n",
        "\n",
        "    return domain_range_info\n",
        "\n",
        "# Retrieve domain and range for each property in the list\n",
        "ontology_properties = get_domain_range(WebNLG_properties_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "cpS46ZYSTpIn"
      },
      "outputs": [],
      "source": [
        "#@title Aggregate possible domain/ranges for each property\n",
        "import os\n",
        "import codecs\n",
        "\n",
        "dico_properties = aggregate_info_and_get_propLabel(ontology_properties)\n",
        "\n",
        "# for prop in properties_info:\n",
        "#     print(f\"Property: {prop['property']}\", f\"  Domain: {prop['domain']}\", f\"  Range: {prop['range']}\")\n",
        "\n",
        "# print(len(dico_properties.keys()))\n",
        "# print(len(properties_info))\n",
        "\n",
        "list_properties = [line.strip() for line in codecs.open('/content/all_properties.txt', 'r', 'utf-8').readlines()]\n",
        "missing_props = []\n",
        "for WebNLG_property in list_properties:\n",
        "  if WebNLG_property not in dico_properties.keys():\n",
        "    missing_props.append(WebNLG_property)\n",
        "\n",
        "print('Missing properties: '+str(len(missing_props))+'/'+str(len(list_properties)), missing_props)\n",
        "# We need to check the Original property labels, not the modified ones, that way we should get them all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "XYZTIMXpfs1b"
      },
      "outputs": [],
      "source": [
        "#@title Get examples for properties\n",
        "# Get a list of sample subject-object values given an input list of properties.\n",
        "\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import pandas as pd\n",
        "import csv\n",
        "import json\n",
        "\n",
        "num_examples_desired = 30#@param\n",
        "\n",
        "def get_dbpedia_property_examples(property_uri, dataframe_examples, count_props, num_occ_uri, limit=10):\n",
        "  sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
        "  query = f\"\"\"\n",
        "  SELECT DISTINCT ?subject ?object WHERE {{\n",
        "    ?subject <{property_uri}> ?object .\n",
        "  }} LIMIT {limit}\n",
        "  \"\"\"\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  count_examples = 0\n",
        "  for result in results[\"results\"][\"bindings\"]:\n",
        "    subject_uri = result[\"subject\"][\"value\"]\n",
        "    object_value = result[\"object\"][\"value\"]\n",
        "\n",
        "    # Extract a readable name from the subject and object URI\n",
        "    subject_label = \"\"\n",
        "    if subject_uri.startswith(\"http://dbpedia.org/resource/\"):\n",
        "      subject_label = subject_uri.split(\"/\")[-1].replace(\"_\", \" \")\n",
        "    else:\n",
        "      subject_label = subject_uri\n",
        "    object_label = \"\"\n",
        "    if object_value.startswith(\"http://dbpedia.org/resource/\"):\n",
        "      object_label = object_value.split(\"/\")[-1].replace(\"_\", \" \")\n",
        "    else:\n",
        "      object_label = object_value\n",
        "\n",
        "    prop_label = property_uri.split(\"/\")[-1]  # e.g., 'birthDate'\n",
        "    dataframe_examples.loc[count_props*limit+count_examples] = [count_props, num_occ_uri, subject_label, prop_label, object_label]\n",
        "    count_examples += 1\n",
        "\n",
        "# properties_uri = [\"http://dbpedia.org/ontology/birthDate\", \"http://dbpedia.org/ontology/birthPlace\"]\n",
        "list_props_dico = json.load(open('/content/dico_count_occurrences_dbp_props.json', 'r'))\n",
        "# Get list of URIs for which there is at least 10 occurrences of the property (to filter out possibly bad properties)\n",
        "properties_uri = []\n",
        "# Also get list of the number of occurrences of each URI to store in the final CSV\n",
        "num_occ_uris = []\n",
        "for prop in list_props_dico.keys():\n",
        "  if list_props_dico[prop] >= 10:\n",
        "    properties_uri.append(prop)\n",
        "    num_occ_uris.append(list_props_dico[prop])\n",
        "assert len(properties_uri) == len(num_occ_uris)\n",
        "print(f'Number of properties with at least 10 occurrences: {len(properties_uri)}')\n",
        "\n",
        "# Create dataframe\n",
        "dataframe_examples = pd.DataFrame(columns=[\"id\", \"num-occurrences\", \"Subject\", \"Property\", \"Object\"])\n",
        "# Example usage:\n",
        "for counter_uris, property_uri in enumerate(properties_uri):\n",
        "  print(f'{str(counter_uris)}/{str(len(properties_uri))}: {property_uri}...')\n",
        "  num_occ_uri = num_occ_uris[counter_uris]\n",
        "  get_dbpedia_property_examples(property_uri, dataframe_examples, counter_uris, num_occ_uri, int(num_examples_desired))\n",
        "\n",
        "# Save dataframe as CSV\n",
        "dataframe_examples.to_csv('dbp_props_examples.csv', index=False)\n",
        "\n",
        "print(dataframe_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "oRDQt9UED6Bh"
      },
      "outputs": [],
      "source": [
        "#@title Process CSV created from annotated examples for properties and get lists of properties to ignore and that can happen only once.\n",
        "# The produced files can be found in the GitHub WikipediaPage_Generator repo.\n",
        "# I needed to extract which properties can happen multiple times for a given subject and which can't, as annotated manually in a spreadsheet.\n",
        "# Guidelines for annotating a property are \"is it confusing if ever a text contains two values for that property\". E.g. \"X was born in Finland and Britain\" is weird, but \"X is the sister of Y and Z\" is not.\n",
        "# It doesn't necessarily have to do with factual truth, e.g. it is almost certain that a ID for an entity in a database is unique, but it's not shocking to say \"X has IDs A and B in database W\".\n",
        "# On the other hand, some cases with multiple object values are annotated as having a single value to avoid poorly entered DBpedia values: e.g. locations (frequently, people put as 3 values a town, the state, the country), subclasses/types (one value is more natural in a text), etc.\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "import json\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "# CSV file in C:\\Users\\sfmil\\Desktop\\ADAPT-2025-2026\\MyPapers\\2025-06_INLG-longText-D2T\n",
        "df = pd.read_csv('/content/dbp_props_examples_annotated.csv')\n",
        "\n",
        "# Make a new dataframe with the first 1000 rows of df\n",
        "# df_1000 = df.head(1000)\n",
        "\n",
        "list_props_that_can_happen_once_only = []\n",
        "list_props_that_can_happen_more_than_once = []\n",
        "list_props_to_filter = []\n",
        "# Iterate over the rows of the dataframe\n",
        "current_property = None\n",
        "for index, df_row in df.iterrows():\n",
        "  if df_row['Property'] != current_property:\n",
        "    current_property = df_row['Property']\n",
        "    current_num_possible_values = df_row['num-possible\\nvalues']\n",
        "    current_id = df_row['id']\n",
        "    # print(f\"Index: {index} - Subject: {df_row['Subject']} - Property: {df_row['Property']} - Object: {df_row['Object']}\")\n",
        "    if current_num_possible_values == '1':\n",
        "      list_props_that_can_happen_once_only.append(current_property)\n",
        "      # print(f'ID: {str(current_id).zfill(4)} - Status: {current_num_possible_values} {current_property}')\n",
        "    elif current_num_possible_values == 'Multiple':\n",
        "      list_props_that_can_happen_more_than_once.append(current_property)\n",
        "      # print(f'ID: {str(current_id).zfill(4)} - Status: M {current_property}')\n",
        "    # else is \"?\" or N/A\n",
        "    elif current_num_possible_values == '?' or current_num_possible_values == 'IgnoreProp':\n",
        "      list_props_to_filter.append(current_property)\n",
        "      # print(f'ID: {str(current_id).zfill(4)} - Status: - {current_property}')\n",
        "    else:\n",
        "      print('ERROR! Unexpected value')\n",
        "      break\n",
        "\n",
        "print(sorted(list_props_that_can_happen_once_only))\n",
        "print(len(list_props_that_can_happen_once_only))\n",
        "print()\n",
        "print(sorted(list_props_that_can_happen_more_than_once))\n",
        "print(len(list_props_that_can_happen_more_than_once))\n",
        "print()\n",
        "print(sorted(list_props_to_filter))\n",
        "print(len(list_props_to_filter))\n",
        "\n",
        "print(f'There are {len(list_props_that_can_happen_once_only)+len(list_props_that_can_happen_more_than_once)+len(list_props_to_filter)} properties collected (expected: 1208).')\n",
        "\n",
        "# Save lists as json\n",
        "with open(\"list_props_that_can_happen_once_only.json\", \"w\") as outfile:\n",
        "    json.dump(sorted(list_props_that_can_happen_once_only), outfile)\n",
        "# with open(\"list_props_that_can_happen_more_than_once.json\", \"w\") as outfile:\n",
        "#     json.dump(list_props_that_can_happen_more_than_once, outfile)\n",
        "with open(\"list_props_to_filter.json\", \"w\") as outfile:\n",
        "    json.dump(sorted(list_props_to_filter), outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "STMOXxbNJaV2"
      },
      "outputs": [],
      "source": [
        "#@title SPARQL query for finding all the possible values for gold:hypernym on dbpedia\n",
        "# Could use dbo:type or rdf:type, but both look a bit messy at first sight\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "def get_types():\n",
        "  # Define the SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query\n",
        "  query = \"\"\"\n",
        "  SELECT DISTINCT ?type\n",
        "  WHERE {\n",
        "    ?s gold:hypernym ?type .\n",
        "  }\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query and the return format\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  # Execute the query and retrieve results\n",
        "  # Returns 10k results, and running it several times in a row always returns the same 10k results\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  return results[\"results\"][\"bindings\"]\n",
        "\n",
        "def count_entities_of_type(hypernym_type):\n",
        "  # Define the DBpedia SPARQL endpoint\n",
        "  sparql = SPARQLWrapper(\"https://dbpedia.org/sparql\")\n",
        "\n",
        "  # Define the SPARQL query\n",
        "  query = f\"\"\"\n",
        "  SELECT (COUNT(?s) AS ?count)\n",
        "  WHERE {{\n",
        "      ?s gold:hypernym <{hypernym_type}> .\n",
        "  }}\n",
        "  \"\"\"\n",
        "\n",
        "  # Set the query and the return format\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "\n",
        "  # Execute the query and retrieve results\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  # Extract and return the count\n",
        "  count = results[\"results\"][\"bindings\"][0][\"count\"][\"value\"]\n",
        "  return int(count)\n",
        "\n",
        "dico_hypernym_types = {}\n",
        "# Extract and print the types\n",
        "results_types = get_types()\n",
        "for i, result in enumerate(results_types):\n",
        "  hypernym_url = result['type']['value']\n",
        "  if hypernym_url not in dico_hypernym_types.keys():\n",
        "    count_occurrences = count_entities_of_type(hypernym_url)\n",
        "    dico_hypernym_types[hypernym_url] = count_occurrences\n",
        "    print(f'{str(i)}/{str(len(results_types))}: {hypernym_url} = {count_occurrences}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HrXI2slUTtnQ"
      },
      "outputs": [],
      "source": [
        "#@title Save dico_hypernym_types to a json and download\n",
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "with open(\"dico_hypernym_types.json\", \"w\") as outfile:\n",
        "  sorted_dico_hypernym_types = {k: v for k, v in sorted(dico_hypernym_types.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "  json.dump(sorted_dico_hypernym_types, outfile)\n",
        "\n",
        "# Download\n",
        "files.download('dico_hypernym_types.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "svm7JHYlGOyX"
      },
      "outputs": [],
      "source": [
        "#@title Make a list of random entities to query for each hypernym (SPARQL query for getting n entities that have a specific hypernym)\n",
        "import random\n",
        "import json\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "def get_random_entities(hypernym: str, limit):\n",
        "  sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "  query = f\"\"\"\n",
        "  SELECT DISTINCT ?entity WHERE {{\n",
        "      ?entity gold:hypernym <{hypernym}> .\n",
        "  }} LIMIT 50000\n",
        "  \"\"\"\n",
        "\n",
        "  sparql.setQuery(query)\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  results = sparql.query().convert()\n",
        "\n",
        "  entities = [result['entity']['value'] for result in results['results']['bindings']]\n",
        "  return random.sample(entities, min(len(entities), limit))\n",
        "\n",
        "# Load json that contains hypernyms as keys and count of instances of that hypernym on DBpedia as value\n",
        "dico_hypernym_types = None\n",
        "with open('/content/dico_hypernym_types_incomplete.json', 'r') as file:\n",
        "    dico_hypernym_types = json.load(file)\n",
        "\n",
        "# Get up to 1000 random entities for the classes that have at least 100 members\n",
        "dico_hypernym_sample_entities = {}\n",
        "for i, hypernym in enumerate(dico_hypernym_types.keys()):\n",
        "  if i < 5:\n",
        "    if dico_hypernym_types[hypernym] >= 100:\n",
        "      print(hypernym, dico_hypernym_types[hypernym])\n",
        "      dico_hypernym_sample_entities[hypernym] = get_random_entities(hypernym, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "yngHyKmiEbQh"
      },
      "outputs": [],
      "source": [
        "#@title Save dico_hypernym_sample_entities as JSON and download\n",
        "from google.colab import files\n",
        "with open(\"dico_hypernym_sample_entities.json\", \"w\") as outfile:\n",
        "    json.dump(dico_hypernym_sample_entities, outfile)\n",
        "# Dowload json\n",
        "files.download('dico_hypernym_sample_entities.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aNtg43_AJCE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EWzVztW9vaZ"
      },
      "source": [
        "# Build dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxgI7FyZCzog"
      },
      "source": [
        "## Get properties for list of entities\n",
        "Creates dico_input_contents_DBp.pickle file.\n",
        "Skip if you want to use an already generated pickle file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "SSw0hkloXnV4"
      },
      "outputs": [],
      "source": [
        "# @title Get DBpedia properties online for an entity list (about 1h for GREC entities)\n",
        "import os\n",
        "import codecs\n",
        "import json\n",
        "import re\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout\n",
        "from WikipediaPage_Generator.code.queryDBpediaProps import get_dbpedia_properties\n",
        "from WikipediaPage_Generator.code.utils import removeReservedCharsFileName\n",
        "\n",
        "# Input json should be a dico_1 with category names (urls or name) as keys, and a list of entities (urls or names) as value.\n",
        "input_json_path = '/content/Build_KGs_entities/resources/GREC_NE.json'#@param{type:\"string\"}\n",
        "# triple-source should be Ontology for this experiment\n",
        "triple_source = 'Ontology' #@param['Infobox', 'Ontology', 'Wikidata']\n",
        "# (Sub)set of properties to consider (about 400 in WebNLG, about 1.2K in DBpedia all)\n",
        "consider_properties = 'DBpedia_WebNLG20' #@param['DBpedia_WebNLG20', 'DBpedia_all']\n",
        "# Store here \"dirty\" properties; string expected, it is later on split by ','\n",
        "# ignore_properties = 'width,title' # Used for semantic accuracy experiments with Rudali\n",
        "ignore_properties = ','.join(json.loads(open('/content/WikipediaPage_Generator/resources/list_props_to_filter.json', 'r').read()))\n",
        "get_triples_where_entity_is_subj = True #@param {type:\"boolean\"}\n",
        "get_triples_where_entity_is_obj = True #@param {type:\"boolean\"}\n",
        "triple_Validation = False #@param {type:\"boolean\"}\n",
        "props_list_path = ''\n",
        "if consider_properties == 'DBpedia_WebNLG20':\n",
        "  props_list_path = os.path.join('/content', 'DCU_TCD_FORGe_WebNLG23', 'code', 'sorted_properties.txt')\n",
        "elif consider_properties == 'DBpedia_all':\n",
        "  print('Add full DBpedia property list')\n",
        "\n",
        "# Load json dico with sample entities for each hypernym\n",
        "dico_hypernym_sample_entities_loaded = None\n",
        "with open(input_json_path, 'r') as file:\n",
        "    dico_hypernym_sample_entities_loaded = json.load(file)\n",
        "\n",
        "# dico_input_contents will contain category keys, which contain entity keys, which contain a list of triple objects\n",
        "dico_input_contents = {}\n",
        "for hypernym in sorted(dico_hypernym_sample_entities_loaded.keys()):\n",
        "  input_category = None\n",
        "  if re.search('/', hypernym):\n",
        "    input_category = hypernym.rsplit('/', 1)[1]\n",
        "  else:\n",
        "    input_category = hypernym\n",
        "  print(input_category)\n",
        "  dico_input_contents[input_category] = {}\n",
        "  # Format properties for passing as argument to python module\n",
        "  # list_triple_object contains object with 3 attributes: DBsubj, DBprop, DBobj\n",
        "  # list_propObj is used for UI (for triples selection by the user)\n",
        "  # list_obj is used for getting class and gender info later on\n",
        "  ##############################################################################\n",
        "  # WARNING-TODO: I forgot to apply removeReservedCharsFileName to entity_names and replace spaces by underscores\n",
        "  ##############################################################################\n",
        "  for sampled_entity in sorted(dico_hypernym_sample_entities_loaded[hypernym]):\n",
        "    entity_name = None\n",
        "    if re.search('/', sampled_entity):\n",
        "      entity_name = sampled_entity.rsplit('/', 1)[1]\n",
        "    else:\n",
        "      entity_name = '_'.join(sampled_entity.split(' '))\n",
        "\n",
        "    # Get all triples in which the entity is the subject\n",
        "    list_triple_objects, list_propObj, list_obj = get_dbpedia_properties(props_list_path, entity_name, triple_source, ignore_properties, get_triples_where_entity_is_subj, get_triples_where_entity_is_obj, triple_Validation)\n",
        "\n",
        "    if len(list_triple_objects) > 0:\n",
        "      print(f'  {entity_name}: found {len(list_triple_objects)} properties.')\n",
        "      dico_input_contents[input_category][entity_name] = list_triple_objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8bEVe9qef3SM"
      },
      "outputs": [],
      "source": [
        "#@title Serialise dico_input_contents using pickle and download\n",
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "with open(\"dico_input_contents_DBp.pickle\", \"wb\") as handle:\n",
        "    pickle.dump(dico_input_contents, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Download file\n",
        "files.download('dico_input_contents_DBp.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FynuNJCvC3Du"
      },
      "source": [
        "## Build knowledge graphs #1: WebNLG mirror input configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "ivwEj5vbNXa5"
      },
      "outputs": [],
      "source": [
        "#@title Check which entities have property (sub)sets that match WebNLG inputs. Creates dico_entities_for_triple_configuration.json file.\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "dico_input_contents = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "print_output = False #@param{type:'boolean'}\n",
        "\n",
        "# dico_input_contents_loaded should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have entity_name as key, and a list of Triple Objects as value.\n",
        "# Triple Objects have 3 attributes: DBsubj, DBprop, DBobj\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "# dico_triple_configs_WebNLG should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have strings of properties separated by \"##\" as keys, and integers (occurrence counts in WebNLG train data) as values.\n",
        "dico_triple_configs_WebNLG = None\n",
        "with open(\"/content/Build_KGs_entities/resources/dico_category_tripleConfigs_WebNLG.json\", \"r\") as handle:\n",
        "    dico_triple_configs_WebNLG = json.load(handle)\n",
        "\n",
        "# dico_mapping_categories = {'City':'Cities'}\n",
        "dico_mapping_categories = {'Person':'People', 'City':'Cities'}\n",
        "\n",
        "# Let's extract which entities have the properties that match a WebNLG configuration. The dico will have: { category: { triple_config: [entity1, entity2, etc.] } }\n",
        "dico_entities_for_triple_configuration = {}\n",
        "print('Finding which entities have the properties that match a WebNLG configuration...')\n",
        "for category_label_WebNLG in dico_mapping_categories.keys():\n",
        "  dico_entities_for_triple_configuration[category_label_WebNLG] = {}\n",
        "  for triple_config_WebNLG in dico_triple_configs_WebNLG[category_label_WebNLG].keys():\n",
        "    # Get input configurations (i.e. property sets) extracted from WebNLG\n",
        "    list_properties_WebNLG = triple_config_WebNLG.split('##')\n",
        "    # print(list_properties_WebNLG)\n",
        "    # Get category label used in GREC\n",
        "    category_label_GREC = dico_mapping_categories[category_label_WebNLG]\n",
        "    # For each GREC entity, extract the set of properties found on DBpedia\n",
        "    for entity_name in dico_input_contents_loaded[category_label_GREC].keys():\n",
        "      # Need a list of strings so we can then convert in sets and compare with other set of property labels\n",
        "      list_properties_entity = []\n",
        "      list_triple_objects_entity = dico_input_contents_loaded[category_label_GREC][entity_name]\n",
        "      for triple_object in list_triple_objects_entity:\n",
        "        list_properties_entity.append(triple_object.DBprop)\n",
        "      # Check if any of the WebNLG triple configurations can be built using the properties of each entity\n",
        "      if set(list_properties_WebNLG).issubset(set(list_properties_entity)):\n",
        "        if triple_config_WebNLG not in dico_entities_for_triple_configuration[category_label_WebNLG].keys():\n",
        "          dico_entities_for_triple_configuration[category_label_WebNLG][triple_config_WebNLG] = []\n",
        "        dico_entities_for_triple_configuration[category_label_WebNLG][triple_config_WebNLG].append(entity_name)\n",
        "\n",
        "# Save dico_entities_for_triple_configuration as json\n",
        "with open(\"dico_entities_for_triple_configuration.json\", \"w\") as outfile:\n",
        "    json.dump(dico_entities_for_triple_configuration, outfile)\n",
        "\n",
        "if print_output == True:\n",
        "  for category_label in dico_entities_for_triple_configuration.keys():\n",
        "    print('============')\n",
        "    print(category_label)\n",
        "    print('============')\n",
        "    for triple_config_overlap in dico_entities_for_triple_configuration[category_label].keys():\n",
        "      print('')\n",
        "      print(triple_config_overlap)\n",
        "      print('----------------------------')\n",
        "      for entity_name in dico_entities_for_triple_configuration[category_label][triple_config_overlap]:\n",
        "        print('-', entity_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IWkIclvxbM-i"
      },
      "outputs": [],
      "source": [
        "#@title Save triple sets in XML format: WebNLG size and input config mirroring\n",
        "# Here we're trying to build a new dataset that has the same properties as in WebNLG, and the same property configurations in the outputs.\n",
        "import pickle\n",
        "import json\n",
        "from WikipediaPage_Generator.code.utils import create_xml, clear_folder\n",
        "\n",
        "dico_input_contents = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "dico_input_entities = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "\n",
        "dico_mapping_categories = {'Person':'People', 'City':'Cities'}\n",
        "\n",
        "# The dico has the following form: { category: { triple_config: [entity1, entity2, etc.] } }\n",
        "dico_entities_for_triple_configuration_l = None\n",
        "if dico_input_entities == 'Made_with_this_notebook':\n",
        "  dico_entities_for_triple_configuration_l = json.load(open('/content/dico_entities_for_triple_configuration.json', 'r'))\n",
        "elif dico_input_entities == 'GitHub_GREC_NEs':\n",
        "  dico_entities_for_triple_configuration_l = json.load(open('/content/Build_KGs_entities/resources/dico_entities_for_triple_configuration_GREC_NEs.json', 'r'))\n",
        "\n",
        "# dico_input_contents_loaded should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have entity_name as key, and a list of Triple Objects as value.\n",
        "# Triple Objects have 3 attributes: DBsubj, DBprop, DBobj\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "counter_datapoints = 0\n",
        "# Keep track of how many times an entity is used for an XML, so we can number the inputs corresponding to the same entity (an XML is named after the entity name)\n",
        "entity_counter = {}\n",
        "for category_l in dico_entities_for_triple_configuration_l:\n",
        "  print(category_l)\n",
        "  # Prepare output folder\n",
        "  clear_folder(os.path.join(triple2predArg, category_l))\n",
        "  # if not os.path.exists(os.path.join(triple2predArg, input_category)):\n",
        "  os.makedirs(os.path.join(triple2predArg, category_l))\n",
        "  catregory_grec = dico_mapping_categories[category_l]\n",
        "  for triple_config in dico_entities_for_triple_configuration_l[category_l]:\n",
        "    print('  ', triple_config, len(dico_entities_for_triple_configuration_l[category_l][triple_config]))\n",
        "    property_list_l = triple_config.split('##')\n",
        "    # print(f'{category_l}: {triple_config}: {len(dico_entities_for_triple_configuration_l[category_l][triple_config])}')\n",
        "    for entity_name in dico_entities_for_triple_configuration_l[category_l][triple_config]:\n",
        "      # Make filename by using entity name + number of times that entity is being used\n",
        "      if entity_name not in entity_counter.keys():\n",
        "        entity_counter[entity_name] = 0\n",
        "      else:\n",
        "        entity_counter[entity_name] += 1\n",
        "      filename = entity_name+'_'+str(entity_counter[entity_name])\n",
        "      list_triple_objects = dico_input_contents_loaded[catregory_grec][entity_name]\n",
        "      list_selected_triple_objects = []\n",
        "      for triple_object in list_triple_objects:\n",
        "        found_prop = False\n",
        "        if triple_object.DBprop in property_list_l:\n",
        "          # print(f'      {entity_name}: found {triple_object.DBprop}.')\n",
        "          if found_prop == False:\n",
        "            list_selected_triple_objects.append(triple_object)\n",
        "            found_prop = True\n",
        "      # The function that builds an XML expects a list of list IDs that correspond to selected triples. In this context, we want all triples.\n",
        "      properties_selected = [i for i in range(len(property_list_l))]\n",
        "      # create xml file passing the entity name to use as filename\n",
        "      counter_datapoints += 1\n",
        "      list_triples_text = create_xml(list_selected_triple_objects, properties_selected, category_l, os.path.join(triple2predArg, category_l), entity_name=filename, eid = counter_datapoints)\n",
        "\n",
        "print(f'----------\\n{counter_datapoints} new datapoints were created in total.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhdqevp_7JZO"
      },
      "source": [
        "## Build knowledge graphs #2: WebNLG mirror input size distribution only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mauku5SQDRq7"
      },
      "outputs": [],
      "source": [
        "#@title Pseudo-code\n",
        "\n",
        "# Initialize empty dictionary dico_length_ratio_entity\n",
        "\n",
        "# For each entity in the dataset:\n",
        "#     Initialize two lists:\n",
        "#         subject_triples = []  // Triples where the entity is the subject\n",
        "#         object_triples = []   // Triples where the entity is the object\n",
        "\n",
        "#     For each triple related to the entity:\n",
        "#         If entity is the subject:\n",
        "#             Add triple to subject_triples\n",
        "#         Else if entity is the object:\n",
        "#             Add triple to object_triples\n",
        "\n",
        "#     // Create property-based dictionaries for subjects and objects\n",
        "#     subject_property_dict = Group subject_triples by property\n",
        "#     object_property_dict = Group object_triples by property\n",
        "\n",
        "#     For each desired input length (e.g., 1 to N triples, in our case N=7):\n",
        "#         For each possible subject/object ratio (e.g., 1:2, 2:1, etc.):\n",
        "#             // Ensure at least one triple has the entity as subject (Constraint 2)\n",
        "#             subject_count = number of triples to select as subject\n",
        "#             object_count = input_length - subject_count\n",
        "\n",
        "#             possible_subject_triples = []\n",
        "#             possible_object_triples = []\n",
        "\n",
        "#             For subject_properties in subject_property_dict:\n",
        "#                 Randomly select up to 2 triples per property (Constraint 3)\n",
        "#                 Add to possible_subject_triples\n",
        "\n",
        "#             For object_properties in object_property_dict:\n",
        "#                 Randomly select up to 2 triples per property (Constraint 3)\n",
        "#                 Add to possible_object_triples\n",
        "\n",
        "#             selected_triples = []\n",
        "#             Randomly select subject_count triples from possible_subject_triples\n",
        "#             Add to selected_triples\n",
        "#             Randomly select object_count triples from possible_object_triples\n",
        "#             Add to selected_triples\n",
        "\n",
        "#             Shuffle selected_triples\n",
        "\n",
        "#             Store selected_triples in dico_length_ratio_entity\n",
        "\n",
        "# Sample randomly from dico_length_ratio_entity using WebNLG's input length distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "VGse5Am5TWYd"
      },
      "outputs": [],
      "source": [
        "#@title Save triple sets in XML format: Implementation of the algorithm for the search+triples sampling\n",
        "import pickle\n",
        "import random\n",
        "import json\n",
        "from WikipediaPage_Generator.code.utils import create_xml, clear_folder\n",
        "\n",
        "dico_input_contents = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "suffle_selected_triples = True #@param{type:'boolean'}\n",
        "final_dataset_size = 20 #@param{type:'integer'}\n",
        "seed = 785 #@param{type:'integer'}\n",
        "MAX_TRIPLES_SET_LENGTH = 7\n",
        "MAX_TRIPLES_PER_PROPERTY = 2\n",
        "DEBUG = False#@param{type:'boolean'}\n",
        "\n",
        "random.seed(seed)\n",
        "\n",
        "def group_triples_by_property(triples):\n",
        "  property_dict = {}\n",
        "  for triple in triples:\n",
        "    if triple.DBprop not in property_dict:\n",
        "      property_dict[triple.DBprop] = []\n",
        "    property_dict[triple.DBprop].append(triple)\n",
        "  return property_dict\n",
        "\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "print(f'Triples for {len(dico_input_contents_loaded[\"Cities\"].keys())+len(dico_input_contents_loaded[\"People\"].keys())} entities were found.')\n",
        "\n",
        "all_triples_sets = []\n",
        "for category_name, entities in dico_input_contents_loaded.items():\n",
        "  for entity_name, triples in entities.items():\n",
        "    if DEBUG:\n",
        "      print('\\n')\n",
        "      print(f'Entity name: {entity_name}')\n",
        "      print('', f'# Triples available: {len(triples)}')\n",
        "    # Extract all the triples that have the current entity as subject\n",
        "    subject_triples = [tri for tri in triples if tri.DBsubj == entity_name]\n",
        "    # Extract all the triples that have the current entity as object\n",
        "    object_triples = [tri for tri in triples if tri.DBobj == entity_name]\n",
        "    if DEBUG:\n",
        "      print('  ', f'# Triples with entity as subject: {len(subject_triples)}')\n",
        "      print('    ', [f\"{o.DBsubj} {o.DBprop} {o.DBobj}\" for o in subject_triples[:10]])\n",
        "      print('  ', f'# Triples with entity as object: {len(object_triples)}')\n",
        "      print('    ', [f\"{o.DBsubj} {o.DBprop} {o.DBobj}\" for o in object_triples[:10]])\n",
        "\n",
        "    # Group subject_triples by property to be able to select randomly a maximum of N triples with the same property (set in MAX_TRIPLES_PER_PROPERTY)\n",
        "    subject_property_dict = group_triples_by_property(subject_triples)\n",
        "    # Group object_triples by property\n",
        "    object_property_dict = group_triples_by_property(object_triples)\n",
        "    if DEBUG:\n",
        "      print('  ', f'# Unique properties entity as subj: {len(subject_property_dict)}')\n",
        "      print('  ', f'# Unique properties entity as obj: {len(object_property_dict)}')\n",
        "\n",
        "    # create one triple set for each triple set length and ratio\n",
        "    for triples_len in range(1, MAX_TRIPLES_SET_LENGTH+1):\n",
        "      if DEBUG:\n",
        "        print(\"Triple set length:\", triples_len)\n",
        "\n",
        "      # the subject/object ratio is extracted considering subject count between 1 (possibly change to 0?)\n",
        "      # (at least one triples with the current entity as subject must be in the triples set)\n",
        "      # and the triples set length, while object count is triples set length - subject count\n",
        "      # together they give the subject/object ratio\n",
        "      for subj_count in range(1, triples_len+1):\n",
        "        obj_count = triples_len - subj_count\n",
        "        if DEBUG:\n",
        "          print(f\"  Subject count: {subj_count}, Object count: {obj_count}\")\n",
        "\n",
        "        # extracted the possible triples with the entity as subject\n",
        "        # for each property, select up to 2 triples, this gives us the complete\n",
        "        # set of triples with entity as subject from which randomly select the\n",
        "        # triples for the final triples set\n",
        "        possible_subject_triples = []\n",
        "        for prop, prop_triples in subject_property_dict.items():\n",
        "          possible_subject_triples.extend(random.sample(prop_triples, min(MAX_TRIPLES_PER_PROPERTY, len(prop_triples))))\n",
        "        # if DEBUG:\n",
        "        #   print('   ', [f\"{o.DBsubj} {o.DBprop} {o.DBobj}\" for o in possible_subject_triples])\n",
        "\n",
        "        # extracted the possible triples with the entity as object\n",
        "        # for each property, select up to 2 triples, this gives us the complete\n",
        "        # set of triples with entity as object from which randomly select the\n",
        "        # triples for the final triples set\n",
        "        possible_object_triples = []\n",
        "        for prop, prop_triples in object_property_dict.items():\n",
        "          possible_object_triples.extend(random.sample(prop_triples, min(MAX_TRIPLES_PER_PROPERTY, len(prop_triples))))\n",
        "\n",
        "        # check that we have enough triples to select\n",
        "        if len(possible_subject_triples) >= subj_count and len(possible_object_triples) >= obj_count:\n",
        "          selected_triples = []\n",
        "          # select subj_count triples where the entity is subject\n",
        "          selected_triples.extend(random.sample(possible_subject_triples, subj_count))\n",
        "          # select obj_count triples where the entity is object\n",
        "          selected_triples.extend(random.sample(possible_object_triples, obj_count))\n",
        "          # shuffle the selected triples\n",
        "          if suffle_selected_triples:\n",
        "            random.shuffle(selected_triples)\n",
        "          all_triples_sets.append({\n",
        "              'triples': selected_triples,\n",
        "              'subj_count': subj_count,\n",
        "              'obj_count': obj_count,\n",
        "              'triples_len': triples_len,\n",
        "              'entity_name': entity_name,\n",
        "              'category_name': category_name\n",
        "          })\n",
        "          if DEBUG:\n",
        "            print('   ', [f\"{o.DBsubj} {o.DBprop} {o.DBobj}\" for o in selected_triples])\n",
        "        else:\n",
        "          if DEBUG:\n",
        "            print('   ', 'Cannot make an input with the subj/obj ratio.')\n",
        "\n",
        "# sample all_triples_sets to reflect the same triples_len as WebNLG and specific\n",
        "# TODO replace with automatic extraction of real WebNLG distribution\n",
        "# IMPORTANT: Express distribution in %\n",
        "distr = {1: 20.8, 2: 19.6, 3: 19.6, 4: 17.2, 5: 12, 6: 6.4, 7: 4.4}\n",
        "total_prob = int(sum(distr.values()))\n",
        "# print(total_prob)\n",
        "assert total_prob == 100, \"Total probability should be 100%\"\n",
        "\n",
        "# If we are missing one sample in the total, find which number to increase by one. E.g. we want 20 samples and get 19 because several numbers are rounded down.\n",
        "# We look for the number that is the closest to the number above to round it up. e.g. we have 1.2, 3.4 and 3.1 samples, which round to 1, 3 and 3, we want to round 3.4 up to 4.\n",
        "# Keep here all float numbers to be sampled\n",
        "num_to_sample_list = [final_dataset_size*value/100 for key, value in distr.items()]\n",
        "# Keep here all rounded numbers to be sampled\n",
        "rounded_num_to_sample_list = [round(x) for x in num_to_sample_list]\n",
        "# If the total of rounded numbers to sample does not equal the actual number to sample, correct the float closest to the above number\n",
        "position_of_num_to_increase = 0\n",
        "highest_difference = 0\n",
        "if sum(rounded_num_to_sample_list) + 1 == final_dataset_size:\n",
        "  count = 0\n",
        "  while count < len(num_to_sample_list):\n",
        "    rounded_number = rounded_num_to_sample_list[count]\n",
        "    float_number = num_to_sample_list[count]\n",
        "    difference = float_number - rounded_number\n",
        "    if difference > highest_difference:\n",
        "      highest_difference = difference\n",
        "      position_of_num_to_increase = count\n",
        "    count += 1\n",
        "  # Now update the list with rounded numbers to sample\n",
        "  rounded_num_to_sample_list[position_of_num_to_increase] += 1\n",
        "  if DEBUG:\n",
        "    print(num_to_sample_list)\n",
        "    print(rounded_num_to_sample_list)\n",
        "    print(f'Position of number to increase: {position_of_num_to_increase} (diff = {highest_difference})')\n",
        "assert sum(rounded_num_to_sample_list) == final_dataset_size, 'Total number of samples does not match final_dataset_size.'\n",
        "\n",
        "sampled_triple_sets = []\n",
        "for count, (triples_len, prob) in enumerate(distr.items()):\n",
        "  # Select any triple set that has the expected size (allows for multiple triple sets per entity)\n",
        "  triples_of_len = [tri for tri in all_triples_sets if tri['triples_len'] == triples_len]\n",
        "  # num_to_sample = round(final_dataset_size * prob / 100)\n",
        "  num_to_sample = rounded_num_to_sample_list[count]\n",
        "  if num_to_sample <= len(triples_of_len):\n",
        "    sampled_triple_sets.extend(random.sample(triples_of_len, num_to_sample))\n",
        "  else:\n",
        "    print(f'!!! Could not select triples sets of size {triples_len} (not enough triple sets).')\n",
        "  print(f'Length: {triples_len}')\n",
        "  print(f'  # Total triple sets of current size: {len(triples_of_len)}')\n",
        "  print(f'  # Selected triple sets of current size: {num_to_sample}')\n",
        "  print(f'  # Total Selected triples at this point: {len(sampled_triple_sets)}')\n",
        "\n",
        "print(f'# Selected triple sets: {len(sampled_triple_sets)}')\n",
        "print(sampled_triple_sets[:10])\n",
        "\n",
        "## Number of triples for each category\n",
        "# print(f'# Triple sets People: {len([tri for tri in sampled_triples if tri[\"category_name\"]==\"People\"])}')\n",
        "# print(f'# Triple sets Cities:{len([tri for tri in sampled_triples if tri[\"category_name\"]==\"Cities\"])}')\n",
        "## Total number of triple sets\n",
        "# print(f'# Triple sets before sampling: {len(all_triples_sets)}')\n",
        "# print(all_triples_sets[:10])\n",
        "\n",
        "# Save datapoints in individual XML files\n",
        "counter_datapoints = 0\n",
        "entity_counter = {}\n",
        "folder_name = ''\n",
        "# print(folder_name)\n",
        "clear_folder(triple2predArg)\n",
        "os.makedirs(os.path.join(triple2predArg, 'People'))\n",
        "os.makedirs(os.path.join(triple2predArg, 'Cities'))\n",
        "for sampled_triple_set in sampled_triple_sets:\n",
        "  counter_datapoints += 1\n",
        "  # print(counter_datapoints)\n",
        "  list_triple_objects = sampled_triple_set['triples']\n",
        "  properties_selected = [i for i in range(len(sampled_triple_set['triples']))]\n",
        "  input_category = sampled_triple_set['category_name']\n",
        "  folder_name = input_category\n",
        "  entity_name = sampled_triple_set['entity_name']\n",
        "  if entity_name not in entity_counter.keys():\n",
        "    entity_counter[entity_name] = 0\n",
        "  else:\n",
        "    entity_counter[entity_name] += 1\n",
        "  filename = entity_name+'_'+str(entity_counter[entity_name])\n",
        "  # create xml file passing the entity name to use as filename\n",
        "  list_triples_text = create_xml(list_triple_objects, properties_selected, input_category, os.path.join(triple2predArg, folder_name), entity_name=filename, eid = counter_datapoints)\n",
        "\n",
        "# print(sampled_triple_sets[:10])\n",
        "# TODO save created dataset preprocessing the triples (we cannot save the object as it is)\n",
        "# for i, tri in enumerate(sampled_triple_sets):\n",
        "#   tri['triples'] = [f\"{o.DBsubj} | {o.DBprop} | {o.DBobj}\" for o in tri['triples']]\n",
        "# with open('/content/sampled_triples.json', 'w') as f:\n",
        "#   json.dump(sampled_triple_sets, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "5gSdmLEM7isu"
      },
      "outputs": [],
      "source": [
        "#@title Check triples extracted from DBpedia\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# dico_input_contents_loaded should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have entity_name as key, and a list of Triple Objects as value.\n",
        "# Triple Objects have 3 attributes: DBsubj, DBprop, DBobj\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "dico_properties = {}\n",
        "dico_different_properties = {}\n",
        "dico_entity_as_subj = {}\n",
        "dico_entity_as_obj = {}\n",
        "for category in dico_input_contents_loaded.keys():\n",
        "  # print(category)\n",
        "  for entity_name in dico_input_contents_loaded[category].keys():\n",
        "    all_properties = []\n",
        "    different_properties = []\n",
        "    subj_of_properties = []\n",
        "    obj_of_properties = []\n",
        "    # print('  ', entity_name)\n",
        "    for triple_object in dico_input_contents_loaded[category][entity_name]:\n",
        "      all_properties.append(triple_object.DBprop)\n",
        "      if triple_object.DBprop not in different_properties:\n",
        "        different_properties.append(triple_object.DBprop)\n",
        "      if triple_object.DBsubj == entity_name:\n",
        "        subj_of_properties.append(triple_object.DBprop)\n",
        "      if triple_object.DBobj == entity_name:\n",
        "        obj_of_properties.append(triple_object.DBprop)\n",
        "    dico_properties[entity_name] = len(all_properties)\n",
        "    dico_different_properties[entity_name] = len(different_properties)\n",
        "    dico_entity_as_subj[entity_name] = len(subj_of_properties)\n",
        "    dico_entity_as_obj[entity_name] = len(obj_of_properties)\n",
        "    # print(f'    {len(different_properties)} different properties')\n",
        "    # print(f'    {len(subj_of_properties)} properties with {entity_name} as subject')\n",
        "    # print(f'    {len(obj_of_properties)} properties with {entity_name} as object')\n",
        "\n",
        "dico_properties_sorted = {k: v for k, v in sorted(dico_properties.items(), key=lambda item: item[1], reverse=True)}\n",
        "dico_different_properties_sorted = {k: v for k, v in sorted(dico_different_properties.items(), key=lambda item: item[1], reverse=True)}\n",
        "dico_entity_as_subj_sorted = {k: v for k, v in sorted(dico_entity_as_subj.items(), key=lambda item: item[1], reverse=True)}\n",
        "dico_entity_as_obj_sorted = {k: v for k, v in sorted(dico_entity_as_obj.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "# Plot dico_properties dictionary using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(dico_properties_sorted.keys(), dico_properties_sorted.values())\n",
        "plt.xlabel('Entity Name')\n",
        "plt.ylabel('Number of Properties')\n",
        "plt.show()\n",
        "\n",
        "# Plot dico_different_properties dictionary using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(dico_different_properties_sorted.keys(), dico_different_properties_sorted.values())\n",
        "plt.xlabel('Entity Name')\n",
        "plt.ylabel('Number of Different Properties')\n",
        "plt.show()\n",
        "\n",
        "# Plot dico_entity_as_subj_sorted dictionary using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(dico_entity_as_subj_sorted.keys(), dico_entity_as_subj_sorted.values())\n",
        "plt.xlabel('Entity Name')\n",
        "plt.ylabel('Number of Properties with Entity as Subject')\n",
        "plt.show()\n",
        "\n",
        "# Plot dico_entity_as_obj_sorted dictionary using matplotlib\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(dico_entity_as_obj_sorted.keys(), dico_entity_as_obj_sorted.values())\n",
        "plt.xlabel('Entity Name')\n",
        "plt.ylabel('Number of Properties with Entity as Object')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "1JeN6DPmKnGC"
      },
      "outputs": [],
      "source": [
        "#@title Check dataset\n",
        "from colored import Fore, Back, Style\n",
        "\n",
        "sizes = {}\n",
        "property_count_per_datapoint = {}\n",
        "subjObj_ratios = {}\n",
        "category_count = {}\n",
        "for i, striple_set in enumerate(sampled_triple_sets):\n",
        "  print(f'Datapoint {i+1}:')\n",
        "  # Check that there is no error with the triple set size\n",
        "  assert striple_set['subj_count'] + striple_set['obj_count'] == striple_set['triples_len'], 'subj_count + obj_count should be equal to triples_len.'\n",
        "  print(f'{Fore.green}{Back.white}  subj_count and obj_count match triples_len{Style.reset}')\n",
        "  assert striple_set['triples_len'] == len(striple_set['triples']), 'triples_len should be equal to the number of triples in the triple set.'\n",
        "  print(f'{Fore.green}{Back.white}  triples_len and number of triples match{Style.reset}')\n",
        "\n",
        "  # Collect and check property count for each datapoint\n",
        "  if str(i) not in property_count_per_datapoint.keys():\n",
        "    property_count_per_datapoint[str(i)] = {}\n",
        "  for triple in striple_set['triples']:\n",
        "    if triple.DBprop not in property_count_per_datapoint[str(i)]:\n",
        "      property_count_per_datapoint[str(i)][triple.DBprop] = 0\n",
        "    property_count_per_datapoint[str(i)][triple.DBprop] += 1\n",
        "  assert property_count_per_datapoint[str(i)][triple.DBprop] <= 2, 'No more than 2 instances of a property per triple set.'\n",
        "  print(f'{Fore.green}{Back.white}  No more than 2 instances of the same property in the triple set{Style.reset}')\n",
        "\n",
        "  # Collect all triple sizes\n",
        "  if striple_set['triples_len'] not in sizes.keys():\n",
        "    sizes[striple_set['triples_len']] = 0\n",
        "  sizes[striple_set['triples_len']] += 1\n",
        "\n",
        "  # Collect category count\n",
        "  if striple_set['category_name'] not in category_count.keys():\n",
        "    category_count[striple_set['category_name']] = {}\n",
        "  if striple_set['entity_name'] not in category_count[striple_set['category_name']].keys():\n",
        "    category_count[striple_set['category_name']][striple_set['entity_name']] = 0\n",
        "  category_count[striple_set['category_name']][striple_set['entity_name']] += 1\n",
        "  # Sort entity names by frequency\n",
        "  category_count[striple_set['category_name']] = {k: v for k, v in sorted(category_count[striple_set['category_name']].items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "  # Collect configuration count of triples (i.e. for each size, what is the subj/obj ratio count)\n",
        "  if striple_set['triples_len'] not in subjObj_ratios.keys():\n",
        "    subjObj_ratios[striple_set['triples_len']] = {}\n",
        "  subj_obj_ratio = str(striple_set['subj_count'])+'/'+str(striple_set['obj_count'])\n",
        "  if subj_obj_ratio not in subjObj_ratios[striple_set['triples_len']].keys():\n",
        "    subjObj_ratios[striple_set['triples_len']][subj_obj_ratio] = 0\n",
        "  subjObj_ratios[striple_set['triples_len']][subj_obj_ratio] += 1\n",
        "\n",
        "print('')\n",
        "# Check number of datapoints for each size and in total\n",
        "assert sum(sizes.values()) == final_dataset_size\n",
        "print(f'{Fore.green}{Back.white}Total number of datapoints: {sum(sizes.values())}{Style.reset}')\n",
        "print(f'{Fore.green}{Back.white}Triple size distribution: {sizes}{Style.reset}')\n",
        "print('')\n",
        "print(f'Subject/Object ratios:    {subjObj_ratios}')\n",
        "# print sum of people\n",
        "print(f\"Category count:           'People': {sum(category_count['People'].values())} ({len(category_count['People'])} distinct), 'Cities': {sum(category_count['Cities'].values())} ({len(category_count['Cities'])} distinct)\")\n",
        "print(f'People instances:         {category_count[\"People\"]}')\n",
        "print(f'Cities instances:         {category_count[\"Cities\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixeaiGixDGMB"
      },
      "source": [
        "## Build knowledge graphs #3: free distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yuvbgosWS0I",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Test function\n",
        "# def get_first_n_instances_of_props(list_triple_objects, max_num_of_instances_of_prop_desired, properties_that_can_happen_once_only):\n",
        "#   \"\"\"\n",
        "#   Function that selects the first n occurrences of a triple that contains the same property.\n",
        "#   Input list_triple_objects: a list of triple objects, each triple should have 3 attributes: DBsubj, DBprop, DBobj.\n",
        "#   Input max_num_of_instances_of_prop_desired: an integer that specifies the maximum number of occurrences of each property in the triple set.\n",
        "#   Input properties_that_can_happen_once_only; a list of property labels that cannot have 2 or more values for the same subject, even though they do have more than one on the queried resource.\n",
        "#   Output: a list of ist indices, e.g [0, 1, 2, 3, 4, 5, 6, 8, 9].\n",
        "#   \"\"\"\n",
        "#   # Dico to keep track of properties already added for each subject\n",
        "#   dico_dbSubj_properties = {}\n",
        "#   # Dico to keep track of how many times each property is found in the triple set\n",
        "#   dico_num_instances_of_prop_found = {}\n",
        "\n",
        "#   selected_properties = []\n",
        "#   for i, triple_object in enumerate(list_triple_objects):\n",
        "#     # Add property to the list of properties in the triple set and initialise count\n",
        "#     if triple_object.DBprop not in dico_num_instances_of_prop_found.keys():\n",
        "#       dico_num_instances_of_prop_found[triple_object.DBprop] = 1\n",
        "#       selected_properties.append(i)\n",
        "#     # For the second and more instance of a property, increase counter and if the resulting number is below that of the maximum number of instances of each property specified in the input, then add the id to the selected list\n",
        "#     else:\n",
        "#       dico_num_instances_of_prop_found[triple_object.DBprop] += 1\n",
        "#       if dico_num_instances_of_prop_found[triple_object.DBprop] <= max_num_of_instances_of_prop_desired:\n",
        "#         # Only add a 2nd or 3rd property if (i) that property is not in the list of props that can happen only once, or (ii) if it is in that list but the subject doesn't already have that property in the triple set\n",
        "#         if (triple_object.DBprop not in properties_that_can_happen_once_only) or (triple_object.DBsubj not in dico_dbSubj_properties.keys()) or (triple_object.DBprop not in dico_dbSubj_properties[triple_object.DBsubj]):\n",
        "#           selected_properties.append(i)\n",
        "\n",
        "#     # Now that we processed a triple, fill up dico_dbSubj_properties\n",
        "#     # For the first instance of a property, create dico entry and add id of triple_object to the list of selected properties\n",
        "#     if triple_object.DBsubj not in dico_dbSubj_properties.keys():\n",
        "#       dico_dbSubj_properties[triple_object.DBsubj] = []\n",
        "#     # Add property to the list of properties used for a subject\n",
        "#     if triple_object.DBprop not in dico_dbSubj_properties[triple_object.DBsubj]:\n",
        "#       dico_dbSubj_properties[triple_object.DBsubj].append(triple_object.DBprop)\n",
        "#   # print(dico_dbSubj_properties)\n",
        "#   return selected_properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "hxGzSaD3Czzq"
      },
      "outputs": [],
      "source": [
        "#@title Save triple set in XML format: Large DBpedia dataset\n",
        "from WikipediaPage_Generator.code.utils import get_first_n_instances_of_props, create_xml, clear_folder\n",
        "# from WikipediaPage_Generator.code.utils import create_xml, clear_folder\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Used for long-input D2T experiments: C:\\Users\\sfmil\\Desktop\\ADAPT-2025-2026\\MyPapers\\2025-06_INLG-longText-D2T\\files\\triple_sets_full\\dico_input_contents_DBp.pickle\n",
        "dico_input_contents = 'GitHub_GREC_NEs'#@param['GitHub_GREC_NEs', 'Made_with_this_notebook']\n",
        "# Map some categories that are found in WebNLG to the name used in WebNLG\n",
        "dico_mapping_categories = {'People':'Person', 'Cities':'City'}\n",
        "\n",
        "# Specifies the maximum number of occurrences of each property in the triple set. The lower and upper bounds are used to vary the number of instances for each entity.\n",
        " # e.g. one may not always want at most 3 occurrences of the same property with an entity, but rather say between 3 and 6 occurrences so the text looks a bit less mechanical.\n",
        "max_num_of_instances_of_prop_desired_lowerBound = \"3\" #@param[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "max_num_of_instances_of_prop_desired_upperBound = \"6\" #@param[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "# Specifies the minimun and maximum size of input desired\n",
        "min_input_size = \"8\" #@param[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30, 35, 40, 45, 50]\n",
        "max_input_size = \"100\" #@param[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100, 150, 200, 250, 300, 500, 1000]\n",
        "# List here properties that cannot have 2 values (to filter bad stuff from i.e. infobox)\n",
        "# properties_that_can_happen_once_only = ['budget', 'gross', 'imdbId', 'length', 'runtime']\n",
        "properties_that_can_happen_once_only = json.loads(open('/content/WikipediaPage_Generator/resources/list_props_that_can_happen_once_only.json', 'r').read())\n",
        "fliter_out_unvalidated_triples = False #@param{type:\"boolean\"}\n",
        "entities_for_dev = ['Abu_Dhabi', 'Accra', 'Islamabad', 'Lagos', 'Aneurin_Bevan', 'Anthony_Giddens', 'Antoine_Lavoisier', 'Antonio_Negri']\n",
        "\n",
        "# Input data structure should be a dico_1 with category names as keys, and a dico_2 as value.\n",
        "# dico_2 should have entity_name as key, and a list of Triple Objects as value.\n",
        "# Triple Objects have 3 attributes: DBsubj, DBprop, DBobj\n",
        "counter_datapoints = 0\n",
        "dico_input_contents_loaded = None\n",
        "if dico_input_contents == 'Made_with_this_notebook':\n",
        "  with open(\"dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "elif dico_input_contents == 'GitHub_GREC_NEs':\n",
        "  with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "    dico_input_contents_loaded = pickle.load(handle)\n",
        "\n",
        "total_num_selected_props = 0\n",
        "longest_input_size = 0\n",
        "for input_category in dico_input_contents_loaded.keys():\n",
        "  webnlg_input_category = dico_mapping_categories[input_category]\n",
        "  print(webnlg_input_category)\n",
        "  folder_name = f'{webnlg_input_category}_{max_num_of_instances_of_prop_desired_lowerBound}to{max_num_of_instances_of_prop_desired_upperBound}'\n",
        "  clear_folder(os.path.join(triple2predArg, folder_name))\n",
        "  # if not os.path.exists(os.path.join(triple2predArg, input_category)):\n",
        "  os.makedirs(os.path.join(triple2predArg, folder_name))\n",
        "  for entity_name in dico_input_contents_loaded[input_category].keys():\n",
        "    # Comment this \"if\" to get v1 data; use \"if entity_name in entities_for_dev\" to get DEV data.\n",
        "    if entity_name not in entities_for_dev:\n",
        "      list_triple_objects = dico_input_contents_loaded[input_category][entity_name]\n",
        "      # Generate list of indices of all properties that can be part of an input (index in the list of Triple objects that contains all retrieved triples)\n",
        "      candidate_properties = get_first_n_instances_of_props(list_triple_objects, f'{max_num_of_instances_of_prop_desired_lowerBound}-{max_num_of_instances_of_prop_desired_upperBound}', properties_that_can_happen_once_only)\n",
        "      print(f'  {entity_name}: pre-selected {len(candidate_properties)}/{len(dico_input_contents_loaded[input_category][entity_name])} properties.')\n",
        "\n",
        "      if len(candidate_properties) >= int(min_input_size):\n",
        "        # Add another filter somwewhere around here!\n",
        "        # ALSO I don't seem to be checking for maximum triple set size anywhere...\n",
        "        # if fliter_out_unvalidated_triples:\n",
        "          # Check whether there is (at least) one intersection between the sets of expected and actual values\n",
        "          # if set(expected_ranges) & set(actual_ranges) and  set(expected_domain) & set(actual_domain):\n",
        "        if len(candidate_properties) > longest_input_size:\n",
        "          longest_input_size = len(candidate_properties)\n",
        "        # create xml file passing the entity name to use as filename\n",
        "        print(f'    {len(candidate_properties[:int(max_input_size)])} included in dataset')\n",
        "        total_num_selected_props += len(candidate_properties[:int(max_input_size)])\n",
        "        counter_datapoints += 1\n",
        "        list_triples_text = create_xml(list_triple_objects, candidate_properties[:int(max_input_size)], webnlg_input_category, os.path.join(triple2predArg, folder_name), entity_name=entity_name, eid = counter_datapoints)\n",
        "\n",
        "print(f'----------\\n{counter_datapoints} new datapoints were created in total (between {int(min_input_size)} and {longest_input_size} triples in input; up to {max_num_of_instances_of_prop_desired_lowerBound}to{max_num_of_instances_of_prop_desired_upperBound} instances of the same property in input).')\n",
        "print(f'Average number of triples per input: {total_num_selected_props/counter_datapoints}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(candidate_properties)"
      ],
      "metadata": {
        "id": "bFNBvgshcUWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrZKd0bCDP39"
      },
      "source": [
        "## Process output XMLs (group, etc.)\n",
        "Stratified sampling info 100 inputs:\n",
        "1:20.8 - 2:19.6 - 3:19.6 - 4:17.2 - 5:12 - 6:6.4 - 7:4.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4AXBt6r7QcH3"
      },
      "outputs": [],
      "source": [
        "#@title Put all XMLs in the same file for sampling\n",
        "import glob\n",
        "import codecs\n",
        "import json\n",
        "\n",
        "XML_folder = 'XML_Split'#@param['XML', 'XML_Split']\n",
        "\n",
        "paths_folders_categories = None\n",
        "out_filename = None\n",
        "out_entities_list = None\n",
        "\n",
        "############# In case code is applied to unsplit files generated with the cells above\n",
        "if XML_folder == 'XML':\n",
        "  paths_folders_categories = glob.glob('/content/XML/*')\n",
        "  out_filename = f'/content/XML/D2T-1-FA_same{max_num_of_instances_of_prop_desired}_min{min_input_size}_max{max_input_size}.xml'\n",
        "  out_entities_list = f'/content/XML/list_entities_same{max_num_of_instances_of_prop_desired}_min{min_input_size}_max{max_input_size}.json'\n",
        "############# In case code is applied to split files\n",
        "elif XML_folder == 'XML_Split':\n",
        "  paths_folders_categories = glob.glob('/content/XML_Split/*')\n",
        "  out_filename = f'/content/XML_Split/D2T-1-FA_same3_min8_max{max_num_triples}_SPLIT.xml'\n",
        "  out_entities_list = f'/content/XML_Split/list_entities_same3_min8_max100_SPLIT-{max_num_triples}.json'\n",
        "\n",
        "with codecs.open(out_filename, 'w', 'utf-8') as f:\n",
        "  list_entities = []\n",
        "  f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
        "  f.write('<benchmark>\\n')\n",
        "  f.write('  <entries>\\n')\n",
        "  for path_folder_category in sorted(paths_folders_categories):\n",
        "    list_XMLS_for_category = glob.glob(os.path.join(path_folder_category, '*.xml'))\n",
        "    for XML in sorted(list_XMLS_for_category):\n",
        "      filename = os.path.basename(XML).rsplit('.', 1)[0]\n",
        "      list_entities.append(filename)\n",
        "      with codecs.open(XML, 'r', 'utf-8') as file:\n",
        "        XML_lines = file.readlines()\n",
        "        for line in XML_lines:\n",
        "          if not line.startswith('<') and not line.startswith('  <'):\n",
        "            f.write(line)\n",
        "  f.write('  </entries>\\n')\n",
        "  f.write('</benchmark>\\n')\n",
        "  # Save entity list as json\n",
        "  # print(list_entities)\n",
        "  with codecs.open(out_entities_list, 'w', 'utf-8') as f:\n",
        "    json.dump(list_entities, f)\n",
        "print('Created XML file!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "6Gmn1qKRr6wj"
      },
      "outputs": [],
      "source": [
        "#@title Zip and download XML folder\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "XML_folder = 'XML_Split'#@param['XML', 'XML_Split']\n",
        "\n",
        "zip_name_xml = None\n",
        "folder_to_zip = None\n",
        "if XML_folder == 'XML':\n",
        "  zip_name_xml = f'/content/XMLs_{dico_input_contents}_same{max_num_of_instances_of_prop_desired}_min{min_input_size}_max{max_input_size}.zip'\n",
        "  folder_to_zip = '/content/XML'\n",
        "elif XML_folder == 'XML_Split':\n",
        "  zip_name_xml = f'v4_XMLs_GREC_NEs_Updated_same3_min8_max100_SPLIT-{max_num_triples}.zip'\n",
        "  folder_to_zip = '/content/XML_Split'\n",
        "\n",
        "if os.path.exists(zip_name_xml):\n",
        "  os.remove(zip_name_xml)\n",
        "\n",
        "! zip -r {zip_name_xml} {folder_to_zip}\n",
        "files.download(zip_name_xml)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Split each triple set in an XML file into several smaller triple sets\n",
        "from WikipediaPage_Generator.code.utils import TripleSet, Triple_withID, create_xml, clear_folder, balanced_split_with_max, extract_info_from_WebNLG_XML, sort_WebNLG_XMLs, split_XMLs\n",
        "import os\n",
        "\n",
        "##### INPUT VARIABLES\n",
        "path_xml = \"/content/v4_long-inputs_GREC_same3_min8_max100.xml\"#@param{type:\"string\"}\n",
        "path_DBprops_count = \"/content/WikipediaPage_Generator/resources/dico_count_occurrences_dbp_props.json\"#@param{type:\"string\"}\n",
        "# Take into account a few triples can be added to a group in case boundaries between groups are changed (to keep same properties in the same group)\n",
        "max_num_triples = 22#@param{type:\"integer\"}\n",
        "path_save_XMLs = \"/content/XML_Split\"#@param{type:\"string\"}\n",
        "debug = False#@param{type:\"boolean\"}\n",
        "\n",
        "if os.path.exists(path_xml):\n",
        "  split_XMLs(path_xml, path_DBprops_count, max_num_triples, path_save_XMLs, DEBUG = debug)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FxqXr9a8lWbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "Pj7mpBYpMoVi"
      },
      "outputs": [],
      "source": [
        "#@title Compare Semantic accuracy pickle file and Long-input D2T pickle file\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "dico_input_contents_SemEx_loaded = None\n",
        "with open(\"/content/Build_KGs_entities/resources/dico_input_contents_DBp_GREC_NEs.pickle\", \"rb\") as handle:\n",
        "  dico_input_contents_SemEx_loaded = pickle.load(handle)\n",
        "\n",
        "dico_input_contents_LongIn_loaded = None\n",
        "with open(\"/content/dico_input_contents_DBp.pickle\", \"rb\") as handle:\n",
        "  dico_input_contents_LongIn_loaded = pickle.load(handle)\n",
        "\n",
        "def showContentsDico(dico_input_contents_loaded):\n",
        "  dico_contents = {}\n",
        "  for category in dico_input_contents_loaded.keys():\n",
        "    print(category)\n",
        "    if category not in dico_contents.keys():\n",
        "      dico_contents[category] = {}\n",
        "    for entity_name in dico_input_contents_loaded[category].keys():\n",
        "      print(f'  {entity_name} ({len(dico_input_contents_loaded[category][entity_name])} properties)')\n",
        "      if entity_name not in dico_contents[category].keys():\n",
        "        dico_contents[category][entity_name] = []\n",
        "      for triple_object in dico_input_contents_loaded[category][entity_name]:\n",
        "        # print('    ', triple_object.DBsubj, triple_object.DBprop, triple_object.DBobj)\n",
        "        triple = f'{triple_object.DBsubj} {triple_object.DBprop} {triple_object.DBobj}'\n",
        "        dico_contents[category][entity_name].append(triple)\n",
        "  return dico_contents\n",
        "\n",
        "dico_SemEx = showContentsDico(dico_input_contents_SemEx_loaded)\n",
        "dico_LongIn = showContentsDico(dico_input_contents_LongIn_loaded)\n",
        "\n",
        "# Save dicos as jsons\n",
        "with open('dico_input_contents_DBp_GREC_NEs_SemEx.json', 'w') as f:\n",
        "  json.dump(dico_SemEx, f)\n",
        "with open('dico_input_contents_DBp_GREC_NEs_LongIn.json', 'w') as f:\n",
        "  json.dump(dico_LongIn, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWAn5XF1WDOw"
      },
      "source": [
        "# More stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBIoHi7FWEpQ",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Test Functions\n",
        "\n",
        "# To split large XML inputs that can trigger memory issues in FORGe\n",
        "# 1 Check what the most frequent entity is (as subject or object)\n",
        "# 2 order all triples in which this entity is subject by frequency of occurrence of the property in DBpedia\n",
        "# 3 order all triples in which this entity is object by frequency of occurrence of the property in DBpedia\n",
        "  # If the entity is the object of a property that ends with a preposition, put it at the bottom in the list.\n",
        "# 4 Repeat 2 and 3 for the next most frequent entities as subject.\n",
        "# 5 Create an XML with at most n entities. Do not cut between two same properties.\n",
        "\n",
        "##### INSTALLS\n",
        "# ! pip install xmltodict\n",
        "##### IMPORTS\n",
        "from WikipediaPage_Generator.code.utils import create_xml, clear_folder\n",
        "import xmltodict\n",
        "import json\n",
        "import codecs\n",
        "import os\n",
        "from colored import Fore, Back, Style\n",
        "##### INPUT VARIABLES\n",
        "# path_xml = \"/content/v3_long-inputs_GREC_same3_min8_max100.xml\"\n",
        "path_xml = \"/content/v3_long-inputs_GREC_same3_min8_just1.xml\"\n",
        "path_DBprops_count = \"/content/WikipediaPage_Generator/resources/dico_count_occurrences_dbp_props.json\"\n",
        "# Take into account a few triples can be added to a group in case boundaries between groups are changed (to keep same properties in the same group)\n",
        "max_num_triples = 22\n",
        "path_save_XMLs = \"/content/XML_Split\"\n",
        "debug = False\n",
        "\n",
        "##### FUNCTIONS\n",
        "class TripleSet:\n",
        "  def __init__(self, triples_list, category, eid, shape, shape_type):\n",
        "    self.triples = triples_list\n",
        "    self.category = category\n",
        "    self.eid = eid\n",
        "    self.size = len(triples_list)\n",
        "    self.shape = shape\n",
        "    self.shape_type = shape_type\n",
        "    self.entities_by_frequency = []\n",
        "    # The main entity is the one that has the most occurrences as subject or object (if several have the same num of occurrences, \"max\" returns the first one)\n",
        "    entity_counter_dico = {}\n",
        "    for triple in self.triples:\n",
        "      if triple.DBsubj not in entity_counter_dico.keys():\n",
        "        entity_counter_dico[triple.DBsubj] = 1\n",
        "      else:\n",
        "        entity_counter_dico[triple.DBsubj] += 1\n",
        "      if triple.DBobj not in entity_counter_dico.keys():\n",
        "        entity_counter_dico[triple.DBobj] = 1\n",
        "      else:\n",
        "        entity_counter_dico[triple.DBobj] += 1\n",
        "    self.entities_by_frequency = sorted(entity_counter_dico, key=entity_counter_dico.get, reverse=True)\n",
        "    # print(entity_counter_dico)\n",
        "    # print(self.entities_by_frequency)\n",
        "\n",
        "class Triple_withID:\n",
        "  def __init__(self, prop, subj_value, obj_value, triple_id):\n",
        "    self.DBprop = prop\n",
        "    self.DBsubj = subj_value\n",
        "    self.DBobj = obj_value\n",
        "    self.id = triple_id\n",
        "\n",
        "def balanced_split_with_max(N1, N2):\n",
        "  \"\"\"\n",
        "  Function that splits as evenly as possible a number N1 into smaller numbers each as close as possible to another number N2 without being larger than N2.\n",
        "  For example, if N1 == 50 and N2 == 20, the output is 17, 17 16.\n",
        "  \"\"\"\n",
        "  assert N1 >= N2, \"N1 must be greater than or equal to N2\"\n",
        "  assert N2 >= 1, \"N1 must be at least 1\"\n",
        "  # Start with the minimal number of parts needed to respect the max constraint\n",
        "  for k in range(N1 // N2, N1 + 1):\n",
        "    # print(f'k: {k}')\n",
        "    base = N1 // k\n",
        "    remainder = N1 % k\n",
        "    # print(f'N1//N2, N1+1: {N1//N2}, {N1+1}')\n",
        "    # print(f'base: {base}')\n",
        "    # print(f'remainder: {remainder}')\n",
        "    # print()\n",
        "    # The largest part will be base + 1 (if remainder > 0)\n",
        "    if base + (1 if remainder > 0 else 0) <= N2:\n",
        "      result = [base + 1] * remainder + [base] * (k - remainder)\n",
        "      return result\n",
        "\n",
        "def extract_info_from_WebNLG_XML (path_input_XML):\n",
        "  \"\"\"\n",
        "  path_input_XML: Path to an XML file that contains triple sets, e.g. as provided in the WebNLG shared tasks.\n",
        "  returns a list of TripleSet objects. Each object contains as attributes: triples (a list of Triple objects), category, eid, size, shape, main_entity\n",
        "  \"\"\"\n",
        "  with codecs.open(path_input_XML, 'r', 'utf-8') as file:\n",
        "    XML_file = file.read()\n",
        "    XML_dict = xmltodict.parse(XML_file)\n",
        "    print(f'    Reading file {path_input_XML}..')\n",
        "    # triple_sets_list will be a list of objects of class TripleSet\n",
        "    total_number_of_triples = 0\n",
        "    triple_sets_list = []\n",
        "    if isinstance(XML_dict['benchmark']['entries']['entry'], list):\n",
        "      print(f\"      There are {len(XML_dict['benchmark']['entries']['entry'])} inputs in the original XML file.\")\n",
        "      for entry in XML_dict['benchmark']['entries']['entry']:\n",
        "        category = entry['@category']\n",
        "        eid = entry['@eid']\n",
        "        size = entry['@size']\n",
        "        shape = entry['@shape']\n",
        "        shape_type = entry['@shape-type']\n",
        "        # mtriples_list will be a list of objects of class Triple\n",
        "        mtriples_list = []\n",
        "        # Get modified triples\n",
        "        if isinstance(entry['modifiedtripleset']['mtriple'], list):\n",
        "          for triple_id, mtriple in enumerate(entry['modifiedtripleset']['mtriple']):\n",
        "            triple_object = Triple_withID(mtriple.split(' | ')[1], mtriple.split(' | ')[0], mtriple.split(' | ')[2], triple_id)\n",
        "            mtriples_list.append(triple_object)\n",
        "        else:\n",
        "          mtriples_list.append(entry['modifiedtripleset']['mtriple'])\n",
        "        assert int(size) == len(mtriples_list), f'Error: found size {size} but found {len(mtriples_list)} triples.'\n",
        "        total_number_of_triples += len(mtriples_list)\n",
        "        # Create object of class TripleSet\n",
        "        tripleSet_object = TripleSet(mtriples_list, category, eid, shape, shape_type)\n",
        "        triple_sets_list.append(tripleSet_object)\n",
        "    else:\n",
        "      print(f\"      There is 1 input in the original XML file.\")\n",
        "      category = XML_dict['benchmark']['entries']['entry']['@category']\n",
        "      eid = XML_dict['benchmark']['entries']['entry']['@eid']\n",
        "      size = XML_dict['benchmark']['entries']['entry']['@size']\n",
        "      shape = XML_dict['benchmark']['entries']['entry']['@shape']\n",
        "      shape_type = XML_dict['benchmark']['entries']['entry']['@shape-type']\n",
        "      # Block repeated from above\n",
        "      # mtriples_list will be a list of objects of class Triple\n",
        "      mtriples_list = []\n",
        "      # Get modified triples\n",
        "      if isinstance(XML_dict['benchmark']['entries']['entry']['modifiedtripleset']['mtriple'], list):\n",
        "        for triple_id, mtriple in enumerate(XML_dict['benchmark']['entries']['entry']['modifiedtripleset']['mtriple']):\n",
        "          triple_object = Triple_withID(mtriple.split(' | ')[1], mtriple.split(' | ')[0], mtriple.split(' | ')[2], triple_id)\n",
        "          mtriples_list.append(triple_object)\n",
        "      else:\n",
        "        mtriples_list.append(XML_dict['benchmark']['entries']['entry']['modifiedtripleset']['mtriple'])\n",
        "      assert int(size) == len(mtriples_list), f'Error: found size {size} but found {len(mtriples_list)} triples.'\n",
        "      total_number_of_triples += len(mtriples_list)\n",
        "      # Create object of class TripleSet\n",
        "      tripleSet_object = TripleSet(mtriples_list, category, eid, shape, shape_type)\n",
        "      triple_sets_list.append(tripleSet_object)\n",
        "\n",
        "    print(f\"      There are {total_number_of_triples} input triples in the original XML file.\")\n",
        "\n",
        "  return triple_sets_list\n",
        "\n",
        "def sort_WebNLG_XMLs (path_input_XML, path_DBprops_count):\n",
        "  \"\"\"\n",
        "  path_input_XML: Path to an XML file that contains triple sets, e.g. as provided in the WebNLG shared tasks. The code expects that all triples mention the same entity, as subject or object.\n",
        "  path_DBprops_count: Path to a json file that contains DBpedia properties as keys (e.g. \"http://dbpedia.org/ontology/birthPlace\") and number of occurrences on DBpedia as values (e.g 1486579).\n",
        "  This function returns a list of TripleSets objects. TripleSet.triples contains Triple objects; in each triple set, Triple objects are sorted by \"importance\" (i.e. sorted by frequency of entity in the triple set, and by frequency of property on DBpedia)\n",
        "  \"\"\"\n",
        "  print('  Sorting triples sets by frequency of entity in the triple set, and by frequency of respective properties on DBpedia...')\n",
        "  dico_count_occurrences_dbp_props = json.loads(codecs.open(path_DBprops_count, 'r', 'utf-8').read())\n",
        "  triple_sets_list = extract_info_from_WebNLG_XML (path_input_XML)\n",
        "  new_triple_set_Objects_list = []\n",
        "  total_number_of_triples = 0\n",
        "  for triple_set in triple_sets_list:\n",
        "    # print(triple_set.eid, triple_set.category, triple_set.size, triple_set.entities_by_frequency[0])\n",
        "    # Make a list where we will store the order of the triples using their index in the triple_set list\n",
        "    # E.g. list_triple_indices = [0, 4, 5, 2, 3, 1]\n",
        "    list_triple_indices = []\n",
        "    # Process entities by their respective importance in the triple set, so the most frequently found entity will go first, the second most frequently found will go second, and so on.\n",
        "    for entity_name in triple_set.entities_by_frequency:\n",
        "      # print(f'  {entity_name}')\n",
        "      # Make a list of property labels with the http://dbpedia.org/ontology/ prefix, one with the properties where the entity is subject, and one with the properties where the entity is object\n",
        "      # The properties in the ..._Subj list will go first, the properties in the ..._Obj list will go after.\n",
        "      list_dico_count_occurrences_dbp_props_keys_Subj = [[f'http://dbpedia.org/ontology/{triple.DBprop}', triple.id] for triple in triple_set.triples if triple.DBsubj == entity_name]\n",
        "      list_dico_count_occurrences_dbp_props_keys_Obj = [[f'http://dbpedia.org/ontology/{triple.DBprop}', triple.id] for triple in triple_set.triples if triple.DBobj == entity_name]\n",
        "      # Order that list according to the count in path_DBprops_count\n",
        "      sorted_list_dico_count_occurrences_dbp_props_keys_Subj = sorted(list_dico_count_occurrences_dbp_props_keys_Subj, key=lambda x: dico_count_occurrences_dbp_props[x[0]], reverse=True)\n",
        "      sorted_list_dico_count_occurrences_dbp_props_keys_Obj = sorted(list_dico_count_occurrences_dbp_props_keys_Obj, key=lambda x: dico_count_occurrences_dbp_props[x[0]], reverse=True)\n",
        "      # print(f'    {sorted_list_dico_count_occurrences_dbp_props_keys_Subj}')\n",
        "      # print(f'    {sorted_list_dico_count_occurrences_dbp_props_keys_Obj}')\n",
        "      # Now put all the triple indices for the current entity in list_triple_indices, starting with the triples in which the entity is subject\n",
        "      for list_triple_indices_Subj in sorted_list_dico_count_occurrences_dbp_props_keys_Subj:\n",
        "        # To avoid duplicated triples:\n",
        "        if list_triple_indices_Subj[1] not in list_triple_indices:\n",
        "          list_triple_indices.append(list_triple_indices_Subj[1])\n",
        "      for list_triple_indices_Obj in sorted_list_dico_count_occurrences_dbp_props_keys_Obj:\n",
        "        if list_triple_indices_Obj[1] not in list_triple_indices:\n",
        "          list_triple_indices.append(list_triple_indices_Obj[1])\n",
        "\n",
        "    #Now add the triples in a list, ordering the triples as defined in list_triple_indices (the create_xml function expects the triples ordered already)\n",
        "    new_triples_list = [triple_set.triples[i] for i in list_triple_indices]\n",
        "    assert len(new_triples_list) == len(triple_set.triples), f'Expected {len(triple_set.triples)} triples, found {len(new_triples_list)}'\n",
        "    total_number_of_triples += len(new_triples_list)\n",
        "    # print(len(new_triples_list), [new_triples_list[x].id for x in range(len(new_triples_list))])\n",
        "    new_triple_set_Objects_list.append(TripleSet(new_triples_list, triple_set.category, triple_set.eid, triple_set.shape, triple_set.shape_type))\n",
        "  assert len(new_triple_set_Objects_list) == len(triple_sets_list), f'Expected {len(triple_sets_list)} triple sets, found {len(new_triple_set_Objects_list)}'\n",
        "  print(f'    There are {len(new_triple_set_Objects_list)} sorted triple sets...')\n",
        "  print(f'    There are {total_number_of_triples} input triples in the sorted XML file.')\n",
        "\n",
        "  return new_triple_set_Objects_list\n",
        "\n",
        "def split_XMLs (path_input_XML, path_DBprops_count, max_num_triples, path_save_XMLs, DEBUG = False):\n",
        "  \"\"\"\n",
        "  path_input_XML: Path to an XML file that contains triple sets, e.g. as provided in the WebNLG shared tasks. The code expects that all triples mention the same entity, as subject or object.\n",
        "  path_DBprops_count: Path to a json file that contains DBpedia properties as keys (e.g. \"http://dbpedia.org/ontology/birthPlace\") and number of occurrences on DBpedia as values (e.g 1486579).\n",
        "  max_num_triples: the maximum number of triples desired in an XML\n",
        "  path_save_XMLs: the path where the output XMLs should be created\n",
        "  This function creates individual XML files for each split triple set.\n",
        "  \"\"\"\n",
        "  print('Splitting XML file...')\n",
        "  clear_folder(path_save_XMLs)\n",
        "  os.makedirs(path_save_XMLs)\n",
        "  # Get the list of TripleSet objects with the triples re-ordered. The object contains the following:\n",
        "  # self.triples, self.category, self.eid, self.size, self.shape, self.shape_type, self.entities_by_frequency\n",
        "  new_triple_set_Objects_list = sort_WebNLG_XMLs(path_input_XML, path_DBprops_count)\n",
        "  total_number_of_XMLs = 0\n",
        "  total_number_of_triples = 0\n",
        "  for new_triple_set in new_triple_set_Objects_list:\n",
        "    if DEBUG:\n",
        "      print(new_triple_set.size, new_triple_set.entities_by_frequency[0])\n",
        "    # Get \"ideal\" triple set split (see balanced_split_with_max function)\n",
        "    even_slices = None\n",
        "    if new_triple_set.size > max_num_triples:\n",
        "      # balanced_split_with_max returns a sequence of numbers that stand for a number of properties.\n",
        "      groups = balanced_split_with_max(new_triple_set.size, max_num_triples)\n",
        "      # Let's convert that to a sequence of numbers that correspond to list slices: [10, 10, 5] becomes [10, 20, 25]\n",
        "      even_slices = [sum(groups[:i]) for i in range(len(groups)+1)]\n",
        "    else:\n",
        "      even_slices = [0] + [new_triple_set.size]\n",
        "    if DEBUG:\n",
        "      print(f'  Before: {even_slices}')\n",
        "\n",
        "    # Initialise new list\n",
        "    new_slices = [0]\n",
        "    # Now we need to check if the split happened between two occurrences of the same property, which we'd like to avoid\n",
        "    # even_slices has at least 2 numbers, 0 and the end of the first or only slice.\n",
        "    if len(even_slices) > 2:\n",
        "      # Check for intermediate group boundaries (i.e. exclude the first boundary, which is 0, and the last one, because there is no property after it)\n",
        "      for boundary in even_slices[1:-1]:\n",
        "        previous_same_property = 0\n",
        "        # Since in the way even_slices is built, the last slices are the smallest ones, it's better to move boundaries to the left.\n",
        "        while new_triple_set.triples[boundary+previous_same_property].DBprop == new_triple_set.triples[boundary+previous_same_property-1].DBprop:\n",
        "          previous_same_property -= 1\n",
        "        if previous_same_property < 0:\n",
        "          new_slices.append(boundary+previous_same_property)\n",
        "          if DEBUG:\n",
        "            print(f'  {Fore.red}{Back.yellow}!!! Changed split {boundary}, {previous_same_property}!{Style.reset}')\n",
        "        else:\n",
        "          new_slices.append(boundary)\n",
        "      # Add last boundary\n",
        "      new_slices.append(even_slices[-1])\n",
        "    else:\n",
        "      # Add second and last boundary\n",
        "      new_slices.append(even_slices[1])\n",
        "    if DEBUG:\n",
        "      print(f'  After: {new_slices}')\n",
        "\n",
        "    # Create XMLs\n",
        "    # Set parameters for calling function that creates XMLs\n",
        "    input_category = new_triple_set.category\n",
        "    folder_name = input_category+'_max'+str(max_num_triples)\n",
        "    entity_name = new_triple_set.entities_by_frequency[0]\n",
        "    eid = new_triple_set.eid\n",
        "    # Clear/Create output folder\n",
        "    if not os.path.exists(os.path.join(path_save_XMLs, folder_name)):\n",
        "      os.makedirs(os.path.join(path_save_XMLs, folder_name))\n",
        "\n",
        "    # For each slice of the triple set, create an XML file\n",
        "    count_files_created = 0\n",
        "    for count_files, i in enumerate(range(len(new_slices)-1)):\n",
        "      list_triple_objects = new_triple_set.triples[new_slices[i]:new_slices[i+1]]\n",
        "      properties_selected_by_user = [i for i in range(len(list_triple_objects))] # Use all properties\n",
        "      unique_entity_name = entity_name+'_'+str(count_files)\n",
        "      list_triples_text = create_xml(list_triple_objects, properties_selected_by_user, input_category, os.path.join(path_save_XMLs, folder_name), entity_name=unique_entity_name, eid = eid)\n",
        "      count_files_created += 1\n",
        "      total_number_of_triples += len(list_triple_objects)\n",
        "    total_number_of_XMLs += count_files_created\n",
        "\n",
        "  print(f'  Created {total_number_of_XMLs} split XML files of approximate size {max_num_triples}.')\n",
        "  print(f'  There are {total_number_of_triples} input triples in the split XML files.')\n",
        "\n",
        "split_XMLs(path_xml, path_DBprops_count, max_num_triples, path_save_XMLs, DEBUG = debug)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Regroup FORGe outputs for split long-input inputs.\n",
        "# Above we needed to split some XMLs because FORGe can explode on inputs too large, so now we need to build on output file aligned with the input file\n",
        "import codecs\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "\n",
        "# 1 - Create a folder FORGe and upload outputs there, e.g. all_EN_dev_out_aligned.txt or v4_long-inputs_GREC_same3_min8_max100_SPLIT-22_en_000-299__SMorphText.conll_out.txt\n",
        "# 2 - Upload file list_entities_same3_min8_max100_SPLIT-22.json created at the same time as the split XML (and found in the same folder).\n",
        "\n",
        "forge_out_files = glob.glob('/content/FORGe/*.txt')\n",
        "list_entities_split = json.load(codecs.open('/content/list_entities_same3_min8_max100_SPLIT-22.json', 'r', 'utf-8'))\n",
        "\n",
        "# Put all FORGe texts in a list\n",
        "all_forge_lines = []\n",
        "for forge_out_file in sorted(forge_out_files):\n",
        "  with codecs.open(forge_out_file, 'r', 'utf-8') as f:\n",
        "    forge_out_lines = f.readlines()\n",
        "    all_forge_lines += forge_out_lines\n",
        "\n",
        "current_entity = None\n",
        "count_different_entities = 0\n",
        "with codecs.open('FORGe-all.txt', 'w', 'utf-8') as f:\n",
        "  for line_count, entity_with_count in enumerate(list_entities_split):\n",
        "    entity_name = entity_with_count.rsplit('_', 1)[0]\n",
        "    print(f'{line_count} - {entity_name}')\n",
        "    line = ''\n",
        "    if entity_name != current_entity:\n",
        "      current_entity = entity_name\n",
        "      if line_count == 0:\n",
        "        line = all_forge_lines[line_count].strip()\n",
        "      else:\n",
        "        line = line + '\\n'\n",
        "        line = line + all_forge_lines[line_count].strip()\n",
        "      count_different_entities += 1\n",
        "    else:\n",
        "      line = line + ' '+all_forge_lines[line_count].strip()\n",
        "    f.write(line)\n",
        "\n",
        "print(f'Found {count_different_entities} different entities.')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0TW-qZFH3RVL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make a shorter version of the reference texts\n",
        "import os\n",
        "import glob\n",
        "import codecs\n",
        "\n",
        "lang = 'EN'#@param['EN', 'GA']\n",
        "proportion_kept = 0.9#@param{type:\"slider\", min:0, max:1, step:0.01}\n",
        "folder_path = f'8_{lang}'\n",
        "zip_path = f\"/content/{folder_path}.zip\"\n",
        "# 9 is made with proportion_kept = 0.9\n",
        "# 10 is made with proportion_kept = 0.7\n",
        "# 11 is made with proportion kept = 0.5\n",
        "folder_out_num = '9'\n",
        "folder_out = f'{folder_out_num}_{lang}'\n",
        "\n",
        "if not os.path.exists(os.path.join('/content', folder_out)):\n",
        "  os.makedirs(os.path.join('/content', folder_out))\n",
        "\n",
        "#Unzip file\n",
        "# ! rm -r {folder_path}\n",
        "if not os.path.exists(folder_path):\n",
        "  ! unzip {zip_path} -d /content/{folder_path}\n",
        "\n",
        "\n",
        "for text_file_path in glob.glob(f'/content/{folder_path}/8/*.txt'):\n",
        "  filename = text_file_path.split('/')[-1]\n",
        "  new_filename = filename.replace('[8_', '['+folder_out_num+'_')\n",
        "  print(new_filename)\n",
        "  sentences_list = codecs.open(text_file_path, 'r', 'utf-8').read().strip().split('. ')\n",
        "  print(f'  {len(sentences_list)} sentences found.')\n",
        "  num_sentences_kept = int(round(len(sentences_list)*proportion_kept, 0))\n",
        "  print(f'  {len(sentences_list[:num_sentences_kept])} sentences kept')\n",
        "  with codecs.open(os.path.join('/content', folder_out, new_filename), 'w', 'utf-8') as f:\n",
        "    for sentence in sentences_list[:num_sentences_kept]:\n",
        "      f.write(sentence+'. ')\n",
        "\n",
        "# zip and download folder_out\n",
        "! zip -r /content/{folder_out}.zip /content/{folder_out}\n",
        "from google.colab import files\n",
        "files.download(f'/content/{folder_out}.zip')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3amcqAYVaZLn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "a4qti7Qw_Yxo",
        "FynuNJCvC3Du",
        "Vhdqevp_7JZO",
        "NrZKd0bCDP39"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}