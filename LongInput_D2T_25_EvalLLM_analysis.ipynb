{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/Build_KGs_entities/blob/main/LongInput_D2T_25_EvalLLM_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Aixplain (needed to load pickle files)\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "! pip install aixplain\n",
        "clear_output()\n",
        "\n",
        "! pip install diversity\n",
        "\n",
        "os.environ[\"AIXPLAIN_API_KEY\"] = '3d039057dcf4e221c7c722debaf56d05d33a565438db0b04b5ead21dac4449de'"
      ],
      "metadata": {
        "id": "BpDr0fwvAS3Z",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate system-level scores LLM-as-judge\n",
        "Code taken and adapted from the GEM 2024 code"
      ],
      "metadata": {
        "id": "y-ZX0ugscXC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip uploaded file\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import re\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# !unzip /content/GPT_results.zip -d /content\n",
        "zip_language = 'GA' #@param['EN', 'GA']\n",
        "# Claude2 is for the second evaluation of long texts\n",
        "zip_model = 'Claude2' #@param['GPT-o3', 'Claude', 'Claude2', 'Llama-13B', 'R1_Llama-70B', 'R1_Llama-70B_bis']\n",
        "zip_data = 'regular' #@param['regular', 'iaa']\n",
        "zip_input_percentage = '100' #@param['100', '85to100']\n",
        "\n",
        "def unzipEvals(path_dir, zip_path):\n",
        "  if os.path.exists(path_dir):\n",
        "    # Remove folder contents\n",
        "    for filename in os.listdir(path_dir):\n",
        "      file_path = os.path.join(path_dir, filename)\n",
        "  else:\n",
        "    os.makedirs(path_dir)\n",
        "    print(f'Created folder {path_dir}')\n",
        "  ! unzip {zip_path} -d {path_dir}\n",
        "  print(f'Unzipped {zip_path} to {path_dir}')\n",
        "\n",
        "path_dir = os.path.join(zip_language+'_'+zip_data, zip_model)\n",
        "zip_path = ''\n",
        "if zip_data == 'regular':\n",
        "  if zip_model == 'GPT-o3':\n",
        "    if zip_language == 'EN':\n",
        "      zip_path = '/content/EN_GPT_results.zip'\n",
        "    elif zip_language == 'GA':\n",
        "      zip_path = '/content/GA_GPT_results.zip'\n",
        "  elif zip_model == 'Claude':\n",
        "    if zip_language == 'EN':\n",
        "      zip_path = '/content/EN_Claude_results.zip'\n",
        "    elif zip_language == 'GA':\n",
        "      zip_path = '/content/GA_Claude_results.zip'\n",
        "  elif zip_model == 'Claude2':\n",
        "    if zip_language == 'EN':\n",
        "      zip_path = f'/content/en_longInputD2T_LLMtexts_inputs{zip_input_percentage}.zip'\n",
        "    elif zip_language == 'GA':\n",
        "      zip_path = f'/content/ga_longInputD2T_LLMtexts_inputs{zip_input_percentage}.zip'\n",
        "  elif zip_model == 'Llama-13B':\n",
        "    if zip_language == 'EN':\n",
        "      zip_path = '/content/EN_aiXplainLlama_results.zip'\n",
        "    elif zip_language == 'GA':\n",
        "      zip_path = '/content/GA_aiXplainLlama_results.zip'\n",
        "  elif zip_model == 'R1_Llama-70B':\n",
        "    if zip_language == 'EN':\n",
        "      zip_path = '/content/EN_aiXplainDeepseek_results.zip'\n",
        "    elif zip_language == 'GA':\n",
        "      zip_path = '/content/GA_aiXplainDeepseek_results.zip'\n",
        "  elif zip_model == 'R1_Llama-70B_bis':\n",
        "    if zip_language == 'EN':\n",
        "      zip_path = '/content/EN_aiXplainDeepseek_results2.zip'\n",
        "    elif zip_language == 'GA':\n",
        "      zip_path = ''\n",
        "\n",
        "unzipEvals(path_dir, zip_path)\n",
        "\n",
        "#title Rename files in en_longInputD2T_LLMtexts_inputs85to100\n",
        "if os.path.exists(f'/content/{zip_language}_regular/Claude2/{zip_language.lower()}_longInputD2T_LLMtexts_inputs85to100'):\n",
        "  list_filepaths = glob.glob(f'/content/{zip_language}_regular/Claude2/{zip_language.lower()}_longInputD2T_LLMtexts_inputs85to100/*.pkl')\n",
        "  for filepath in list_filepaths:\n",
        "    head, tail = os.path.split(filepath)\n",
        "    fullname = os.path.splitext(tail)[0]\n",
        "    extension = os.path.splitext(tail)[1]\n",
        "    # remove system ID\n",
        "    basename = fullname.rsplit('_', 1)[0]\n",
        "    new_fullname = ''\n",
        "    if re.search('inputs100', fullname):\n",
        "      new_fullname = basename.replace('inputs100', 'inputs85to100')+'_0'+extension\n",
        "    if re.search('inputs95', fullname):\n",
        "      new_fullname = basename.replace('inputs95', 'inputs85to100')+'_6'+extension\n",
        "    elif re.search('inputs90', fullname):\n",
        "      new_fullname = basename.replace('inputs90', 'inputs85to100')+'_7'+extension\n",
        "    elif re.search('inputs85', fullname):\n",
        "      new_fullname = basename.replace('inputs85', 'inputs85to100')+'_8'+extension\n",
        "    os.rename(filepath, os.path.join(head, new_fullname))\n",
        "\n",
        "# clear_output()"
      ],
      "metadata": {
        "id": "ajyFBxlFPWaD",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load unzipped files and save in normalised json files\n",
        "import pickle\n",
        "import glob\n",
        "import json\n",
        "import re\n",
        "import codecs\n",
        "\n",
        "def is_valid_pickle(filepath):\n",
        "  try:\n",
        "    with open(filepath, 'rb') as f:\n",
        "      # Check header\n",
        "      header = f.read(1)\n",
        "      if header != b'\\x80':\n",
        "          return False\n",
        "      f.seek(0)\n",
        "      # Attempt to load safely\n",
        "      pickle.load(f)\n",
        "    return True\n",
        "  except Exception:\n",
        "    return False\n",
        "\n",
        "\n",
        "update_params_unzip = True #@param {type:\"boolean\"}\n",
        "if update_params_unzip:\n",
        "  zip_language = 'GA' #@param['EN', 'GA']\n",
        "  zip_model = 'Claude2' #@param['GPT-o3', 'Claude', 'Claude2', 'Llama-13B', 'R1_Llama-70B', 'R1_Llama-70B_bis']\n",
        "  zip_data = 'regular' #@param['regular', 'iaa']\n",
        "\n",
        "model_scores = [1, 2, 3, 4, 5, 6, 7]\n",
        "# load_gemini_folder = True #@param {type:\"boolean\"}\n",
        "# load_gpt_folder = False #@param {type:\"boolean\"}\n",
        "path_dir_unzipped = ''\n",
        "model_prefix = ''\n",
        "if zip_model.startswith('Claude_bis'):\n",
        "  path_dir_unzipped = os.path.join('/content', zip_language+'_'+zip_data, zip_model, zip_language+'_Claude_not_aiXplainLlama_results')\n",
        "  model_prefix = 'claude-3-7-sonnet-latest'\n",
        "elif zip_model.startswith('Claude2'):\n",
        "  path_dir_unzipped = os.path.join('/content', zip_language+'_'+zip_data, zip_model, zip_language.lower()+'_longInputD2T_LLMtexts_inputs'+zip_input_percentage)\n",
        "  model_prefix = 'claude_anthropic'\n",
        "elif zip_model.startswith('Claude'):\n",
        "  path_dir_unzipped = os.path.join('/content', zip_language+'_'+zip_data, zip_model, zip_language+'_Claude_results')\n",
        "  model_prefix = 'claude-3-7-sonnet-latest'\n",
        "elif zip_model.startswith('GPT'):\n",
        "  path_dir_unzipped = os.path.join('/content', zip_language+'_'+zip_data, zip_model, zip_language+'_GPT_results')\n",
        "  model_prefix = 'o3'\n",
        "elif zip_model.startswith('Llama'):\n",
        "  path_dir_unzipped = os.path.join('/content', zip_language+'_'+zip_data, zip_model, zip_language+'_aiXplainLlama_results')\n",
        "  # ERROR HERE? The Llama file actually has that prefix inside\n",
        "  model_prefix = '6704c91bfdf7d14548c9fedb'\n",
        "elif zip_model.startswith('R1_Llama-70B_bis'):\n",
        "  path_dir_unzipped = os.path.join('/content', zip_language+'_'+zip_data, zip_model, zip_language+'_aiXplainDeepseek_results2')\n",
        "  model_prefix = '67976f47e341d313f66bb835'\n",
        "elif zip_model.startswith('R1_Llama-70B'):\n",
        "  path_dir_unzipped = os.path.join('/content', zip_language+'_'+zip_data, zip_model, zip_language+'_aiXplainDeepseek_results')\n",
        "  model_prefix = '67976f47e341d313f66bb835'\n",
        "\n",
        "def separateJustification(LLMoutString, criterion):\n",
        "  \"\"\"\n",
        "  The Justifications returned by the models often break the json format, so I extract them\n",
        "  \"\"\"\n",
        "  search_expression = '(\"'+criterion+'\":[^\\{]+\\{[^\\}]*\"Justification\":)([^\\}]+)(\"Score\":[^\\}]+\\})'\n",
        "  if re.search(search_expression, LLMoutString):\n",
        "    justificationRemoved = re.sub(search_expression, '\\g<1> \"\", \\g<3>',  LLMoutString)\n",
        "    justification = re.sub('^.*'+search_expression+'.*$', '\\g<2>',  LLMoutString)\n",
        "  else:\n",
        "    justificationRemoved = LLMoutString\n",
        "    justification = ''\n",
        "  return justificationRemoved, justification\n",
        "\n",
        "def loadDataPoint(dbfile_x, model, sys0_to_update):\n",
        "  eval_missing = None\n",
        "  wrong_score = None\n",
        "  dico_key = 'scores_'+str(model)\n",
        "  formatted_scores = {}\n",
        "  # load data with pickle\n",
        "  dp = {}\n",
        "  if is_valid_pickle(dbfile_x.name):\n",
        "    dp = pickle.load(dbfile_x)\n",
        "  else:\n",
        "    print('The input file is not a pickle file!')\n",
        "  print(f'\\nHere is the full dico with ratings:\\n{dp}')\n",
        "  if dico_key in dp:\n",
        "    print(f\"\\nID found: {dp['id']}\")\n",
        "    print(f\"Triples found: {dp['triples']}\")\n",
        "    print(f\"Text found: {dp['text']}\")\n",
        "    justifications = []\n",
        "    # pickle.load uses single quotes, whereas json.load expects double quotes\n",
        "    # Gemini adds a node \"query\" in the json, unlike OpenAI's models\n",
        "    LLMout_string = str(dp[dico_key]).replace(\"'query'\", '\"query\"')\n",
        "    LLMout_string = LLMout_string.replace(\"```json\", \"\")\n",
        "    LLMout_string = LLMout_string.replace(\"```\", \"\")\n",
        "    # DeepSeek output is quite different from the rest\n",
        "    if model == '67976f47e341d313f66bb835':\n",
        "      LLMout_string = LLMout_string.replace(\"\\\\n\", '')\n",
        "      LLMout_string = LLMout_string.replace(\"\\\\\", '')\n",
        "      LLMout_string = LLMout_string.replace('\"', \"'\")\n",
        "      # Can't rely on double bracket closing because sometimes there's only one.\n",
        "      LLMout_string = re.sub(\"^.*('output': '\\{'No-Omissions': \\{[^\\}]+\\}, 'No-Additions': \\{[^\\}]+\\}, 'Grammaticality': \\{[^\\}]+\\}, 'Fluency': \\{[^\\}]+\\}).*$\", \"\\g<1>\", LLMout_string)\n",
        "    LLMout_string = LLMout_string.replace(\"'No-Omissions'\", '\"No-Omissions\"')\n",
        "    # There's a typo in one of the Gemini outputs\n",
        "    LLMout_string = LLMout_string.replace(\"'No-Omissons'\", '\"No-Omissions\"')\n",
        "    LLMout_string = LLMout_string.replace('\"No-Omissons\"', '\"No-Omissions\"')\n",
        "    LLMout_string = LLMout_string.replace(\"'No-Additions'\", '\"No-Additions\"')\n",
        "    LLMout_string = LLMout_string.replace(\"'Grammaticality'\", '\"Grammaticality\"')\n",
        "    LLMout_string = LLMout_string.replace(\"'Fluency'\", '\"Fluency\"')\n",
        "    # Sometimes justifications are followed by single quotes, sometimes by double quotes\n",
        "    LLMout_string = LLMout_string.replace(\"'Justification': '\", '\"Justification\": \"').replace(\"'Justification'\", '\"Justification\"')\n",
        "    LLMout_string = LLMout_string.replace(\"', 'Score'\", '\", \"Score\"').replace(\"'Score'\", '\"Score\"')\n",
        "    # print(LLMout_string)\n",
        "    if LLMout_string == 'None':\n",
        "      eval_missing = dp['id']\n",
        "    else:\n",
        "      # print(LLMout_string)\n",
        "      # LLMout_string = re.sub('(\"No-Omissions\":[^\\{]+\\{\"Justification\":)([^\\}]+)(\"Score\":[^\\}]+\\})', '\\g<1> \"\", \\g<3>',  LLMout_string)\n",
        "      # Claude adds some comments before giving the eval results, this breaks the format\n",
        "      LLMout_string = re.sub(\"^[^\\{]+\", \"\", LLMout_string)\n",
        "      LLMout_string, justifNoOm = separateJustification(LLMout_string, 'No-Omissions')\n",
        "      justifications.append(justifNoOm)\n",
        "      LLMout_string, justifNoAdd = separateJustification(LLMout_string, 'No-Additions')\n",
        "      justifications.append(justifNoAdd)\n",
        "      LLMout_string, justifGram = separateJustification(LLMout_string, 'Grammaticality')\n",
        "      justifications.append(justifGram)\n",
        "      LLMout_string, justifFlu = separateJustification(LLMout_string, 'Fluency')\n",
        "      justifications.append(justifFlu)\n",
        "      print(f\"Justifications found: {justifications}\")\n",
        "      LLMout_string = LLMout_string.replace(\"'1'\", '\"1\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'2'\", '\"2\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'3'\", '\"3\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'4'\", '\"4\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'5'\", '\"5\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'6'\", '\"6\"')\n",
        "      LLMout_string = LLMout_string.replace(\"'7'\", '\"7\"')\n",
        "      # LLMout_string should finish with 2 braces (cases with only one have been seen)\n",
        "      if LLMout_string.count('{') > LLMout_string.count('}'):\n",
        "        while not re.search('\\}[\\s\\r\\n]*\\}$', LLMout_string):\n",
        "          LLMout_string = LLMout_string + ' }'\n",
        "      # print(f'LLMOUT: <<<<<< {LLMout_string}>>>>>>')\n",
        "      scores_json = json.loads(LLMout_string)\n",
        "      clean_scores_json = None\n",
        "      # Gemini adds a node \"query\" in the json, unlike OpenAI's models\n",
        "      if 'query' in scores_json:\n",
        "        clean_scores_json = scores_json['query']\n",
        "      else:\n",
        "        clean_scores_json = scores_json\n",
        "\n",
        "      gram_score = int(clean_scores_json['Grammaticality']['Score'])\n",
        "      flu_score = int(clean_scores_json['Fluency']['Score'])\n",
        "      no_om_score = int(clean_scores_json['No-Omissions']['Score'])\n",
        "      no_ad_score = int(clean_scores_json['No-Additions']['Score'])\n",
        "      coh_score = 0\n",
        "      if zip_model == 'Claude2':\n",
        "        coh_score = int(clean_scores_json['Coherence']['Score'])\n",
        "        if coh_score not in model_scores:\n",
        "          wrong_score = dp['id']\n",
        "\n",
        "      if (gram_score not in model_scores) or (flu_score not in model_scores) or (no_om_score not in model_scores) or (no_ad_score not in model_scores):\n",
        "        wrong_score = dp['id']\n",
        "\n",
        "      id_to_use = dp['id']\n",
        "      if not sys0_to_update == None:\n",
        "        id_to_use = dp['id'].rsplit('_', 1)[0]+f'_{sys0_to_update}'\n",
        "      formatted_scores[\"eid\"] = id_to_use\n",
        "      formatted_scores[\"annotator_id\"] = str(zip_model)\n",
        "      formatted_scores[\"no-omissions\"] = no_om_score\n",
        "      formatted_scores[\"no-additions\"] = no_ad_score\n",
        "      formatted_scores[\"grammaticality\"] = gram_score\n",
        "      formatted_scores[\"fluency\"] = flu_score\n",
        "      if zip_model == 'Claude2':\n",
        "        formatted_scores[\"coherence\"] = coh_score\n",
        "\n",
        "      if zip_model == 'Claude2':\n",
        "        print(f\"Scores found - NoOm: {no_om_score}; NoAd: {no_ad_score}; Gram: {gram_score}; Flu: {flu_score}; Coh: {coh_score}.\")\n",
        "      else:\n",
        "        print(f\"Scores found - Gram: {gram_score}; Flu: {flu_score}; NoOm: {no_om_score}; NoAd: {no_ad_score}.\")\n",
        "      print('')\n",
        "\n",
        "  return formatted_scores, eval_missing, wrong_score\n",
        "\n",
        "print(f'Loaded evaluations found in {path_dir_unzipped}')\n",
        "print(f'Model ID used inside evaluation files: {model_prefix}')\n",
        "eval_files = glob.glob(os.path.join(path_dir_unzipped, '*'))\n",
        "evals_missing = []\n",
        "wrong_scores = []\n",
        "all_scores = []\n",
        "for filepath in eval_files:\n",
        "  print(filepath)\n",
        "  sys0_to_update = None\n",
        "  if re.search('_inputs85to100_', filepath):\n",
        "    _, tail = os.path.split(filepath)\n",
        "    filename = os.path.splitext(tail)[0]\n",
        "    sys0_to_update = filename.rsplit('_', 1)[1]\n",
        "    print(f'Will update system ID to {sys0_to_update}')\n",
        "  dbfile_x = open(filepath, 'rb')\n",
        "  formatted_scores, eval_missing, wrong_score = loadDataPoint(dbfile_x, model_prefix, sys0_to_update)\n",
        "  if eval_missing != None:\n",
        "    evals_missing.append(eval_missing)\n",
        "  if wrong_score != None:\n",
        "    wrong_scores.append(wrong_score)\n",
        "  dbfile_x.close()\n",
        "  if len(formatted_scores) > 0:\n",
        "    all_scores.append(formatted_scores)\n",
        "print(f'Missing evaluations: {evals_missing}')\n",
        "print(f'Wrong scores: {wrong_scores}')\n",
        "\n",
        "# Save all scores into a json file\n",
        "path_json_out = zip_language+'_'+zip_model+'_scores.json'\n",
        "with codecs.open(path_json_out, 'w', 'utf-8') as outfile:\n",
        "  json.dump(all_scores, outfile)"
      ],
      "metadata": {
        "id": "cQDPBbTqIVOW",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate system_level scores D2T assigned by each LLM for each size\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "DEBUG = True #@param {type:\"boolean\"}\n",
        "update_params_unzip = True #@param {type:\"boolean\"}\n",
        "if update_params_unzip:\n",
        "  zip_model = 'Claude2' #@param['GPT-o3', 'Claude', 'Claude2', 'Llama-13B', 'R1_Llama-70B', 'R1_Llama-70B_bis']\n",
        "\n",
        "if not os.path.exists('/content/LLM-eval_final_jsons'):\n",
        "  assert os.path.exists('/content/LLM-eval_final_jsons.zip'), 'You need to upload LLM-eval_final_jsons.zip'\n",
        "  ! unzip /content/LLM-eval_final_jsons.zip\n",
        "\n",
        "\n",
        "csv_sys_level_scores = '/content/csv_sysLevel_scores'\n",
        "if not os.path.exists(csv_sys_level_scores):\n",
        "  os.makedirs(csv_sys_level_scores)\n",
        "\n",
        "pd.set_option('display.max_rows', 10)\n",
        "allowed_scores = [1, 2, 3, 4, 5, 6, 7]\n",
        "# group_by_size = False#@param{type:'boolean'}\n",
        "# Store here input sizes for making subpopulations of the test set; should contain input_id:size_category\n",
        "# input_id starts from 0000\n",
        "dico_sizes = {}\n",
        "if zip_model == 'Claude2':\n",
        "  dico_sizes = {'0570': '0', '0088': '0', '0246': '0', '0213': '1', '0456': '1', '0348': '1', '0652': '2', '0064': '2', '0332': '2', '0479': '3', '0000': '3', '0303': '3', '0025': '4', '0137': '4', '0141': '4', '0147': '5', '0052': '5', '0091': '5', '0126': '6', '0165': '6', '0074': '6', '0117': '7', '0124': '7', '0013': '7', '0076': '8', '0068': '8', '0034': '8', '0144': '9', '0053': '9', '0140': '9'}\n",
        "else:\n",
        "  dico_sizes = {'0015': '5' ,'0049': '5' ,'0050': '2' ,'0070': '4' ,'0090': '6' ,'0097': '6' ,'0112': '0' ,'0120': '4' ,'0130': '2' ,'0143': '3' ,'0148': '5' ,'0158': '6' ,'0159': '4' ,'0163': '3' ,'0243': '3' ,'0255': '1' ,'0273': '2' ,'0286': '1' ,'0374': '1' ,'0378': '0' ,'0532': '0'}\n",
        "# Use this dictionary to map values to other values\n",
        "score_map = {0:0}\n",
        "\n",
        "def check_scores(score_list, allowed_scores):\n",
        "  new_score_list = []\n",
        "  for score in score_list:\n",
        "    if score in score_map:\n",
        "      score = score_map[score]\n",
        "    assert score in allowed_scores, f'Score out of range: {score}'\n",
        "    new_score_list.append(score)\n",
        "  return pd.Series(new_score_list)\n",
        "\n",
        "# Load the files one at a time; there is one file per LLM-language combination (4*3 = 12 files in total)\n",
        "json_folder = '/content/LLM-eval_final_jsons'\n",
        "json_files = glob.glob(os.path.join(json_folder, '*.json'))\n",
        "\n",
        "pd_list = [pd.read_json(json_file) for json_file in sorted(json_files)]\n",
        "\n",
        "# Create dataframes to store system-level scores\n",
        "\n",
        "sysLevel_scores_perLLM = None\n",
        "if zip_model == 'Claude2':\n",
        "  sysLevel_scores_perLLM = pd.DataFrame(columns=['language', 'task', 'system', 'input_size', 'LLM', 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency', 'avg_coherence'])\n",
        "else:\n",
        "  sysLevel_scores_perLLM = pd.DataFrame(columns=['language', 'task', 'system', 'input_size', 'LLM', 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency'])\n",
        "\n",
        "system_task_counter = 0\n",
        "for df in pd_list:\n",
        "  if DEBUG:\n",
        "    print(df)\n",
        "  # LLM and Human files use different IDs (eid VS id)\n",
        "  # split eid column using underscores and create corresponding columns in the dataframe\n",
        "  df[['language', 'task', 'id', 'system']] = df['eid'].str.split('_', expand=True)\n",
        "\n",
        "  # NEW Add a column to df with the size of the input according to dico_sizes\n",
        "  df['input_size'] = df['id'].map(dico_sizes)\n",
        "\n",
        "  # Convert 'task' and 'system' columns to string to avoid TypeError during sorting\n",
        "  df['task'] = df['task'].astype(str)\n",
        "  df['system'] = df['system'].astype(str)\n",
        "\n",
        "  print('Getting scores for', df['annotator_id'][0], df['language'][0], '...')\n",
        "\n",
        "  # Get all possible values for task and system column\n",
        "  unique_tasks = sorted(df['task'].unique())\n",
        "  unique_systems = sorted(df['system'].unique())\n",
        "  unique_input_sizes = sorted(df['input_size'].unique())\n",
        "  print(' Unique tasks:', unique_tasks)\n",
        "  print(' Unique systems:', unique_systems)\n",
        "  print(' Unique input sizes:', unique_input_sizes)\n",
        "\n",
        "  # Get the average scores for each system for each task\n",
        "  for task in unique_tasks:\n",
        "    if DEBUG:\n",
        "      print(f'Task: {task}\\n===========')\n",
        "    for system in unique_systems:\n",
        "      if DEBUG:\n",
        "        print(f'\\nSystem: {system}')\n",
        "      for size in unique_input_sizes:\n",
        "        # get scores in each of the 4 score columns\n",
        "        no_omissions_scores = df.loc[(df['task'] == task) & (df['system'] == system) & (df['input_size'] == size), 'no-omissions']\n",
        "        no_omissions_scores = check_scores(no_omissions_scores, allowed_scores)\n",
        "        no_additions_scores = ''\n",
        "        # LLM and human files have different criterion names (hyphen VS underscore)\n",
        "        no_additions_scores = df.loc[(df['task'] == task) & (df['system'] == system) & (df['input_size'] == size), 'no-additions']\n",
        "        no_additions_scores = check_scores(no_additions_scores, allowed_scores)\n",
        "        grammaticality_scores = df.loc[(df['task'] == task) & (df['system'] == system) & (df['input_size'] == size), 'grammaticality']\n",
        "        grammaticality_scores = check_scores(grammaticality_scores, allowed_scores)\n",
        "        fluency_scores = df.loc[(df['task'] == task) & (df['system'] == system) & (df['input_size'] == size), 'fluency']\n",
        "        fluency_scores = check_scores(fluency_scores, allowed_scores)\n",
        "        #Get average scores\n",
        "        average_no_omissions_score = no_omissions_scores.mean()\n",
        "        average_no_additions_score = no_additions_scores.mean()\n",
        "        average_grammaticality_score = grammaticality_scores.mean()\n",
        "        average_fluency_score = fluency_scores.mean()\n",
        "        average_coherence_score = 0\n",
        "        if zip_model == 'Claude2':\n",
        "          coherence_scores = df.loc[(df['task'] == task) & (df['system'] == system) & (df['input_size'] == size), 'coherence']\n",
        "          coherence_scores = check_scores(coherence_scores, allowed_scores)\n",
        "          average_coherence_score = coherence_scores.mean()\n",
        "\n",
        "        if DEBUG:\n",
        "          if len(no_omissions_scores) > 0:\n",
        "            print(f'  Size = {size}')\n",
        "            print(f'    Average no-omissions score: {average_no_omissions_score}')\n",
        "            print(f'    Average no-additions score: {average_no_additions_score}')\n",
        "            print(f'    Average grammaticality score: {average_grammaticality_score}')\n",
        "            print(f'    Average fluency score: {average_fluency_score}')\n",
        "            if zip_model == 'Claude2':\n",
        "              print(f'    Average Coherence score: {average_coherence_score}')\n",
        "\n",
        "        # If there are no numbers to average, \"mean\" returns a float that prints 'nan'. Filter out these cases.\n",
        "        if not math.isnan(average_fluency_score):\n",
        "          # Add a row to sysLevel_scores_perLLM dataframe\n",
        "          if zip_model == 'Claude2':\n",
        "            sysLevel_scores_perLLM.loc[system_task_counter] = [df['language'][0], task, system, size, df['annotator_id'][0], average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score, average_coherence_score]\n",
        "          else:\n",
        "            sysLevel_scores_perLLM.loc[system_task_counter] = [df['language'][0], task, system, size, df['annotator_id'][0], average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "          system_task_counter += 1\n",
        "\n",
        "print()\n",
        "print(sysLevel_scores_perLLM)\n",
        "# Dump dataframe in a CSV file\n",
        "sysLevel_scores_perLLM.to_csv(os.path.join(csv_sys_level_scores, 'sysLevel_scores_perLLM_perSize.csv'), index=False)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jDCnqzKZ81rV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get average (across LLMs) system-level scores D2T and create CSV\n",
        "# There are 4 LLMs which gave evaluations for all datapoints; we need this extra cell for LLMs to get an average LLM score for each system\n",
        "\n",
        "DEBUG = True\n",
        "\n",
        "# Create dataframe to store average LLM-assigned system-level scores\n",
        "avg_sysLevel_scores_LLMs = None\n",
        "if zip_model == 'Claude2':\n",
        "  avg_sysLevel_scores_LLMs = pd.DataFrame(columns=['language', 'task', 'system', 'input_size', 'LLM', 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency', 'avg_coherence'])\n",
        "else:\n",
        "  avg_sysLevel_scores_LLMs = pd.DataFrame(columns=['language', 'task', 'system', 'input_size', 'LLM', 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency'])\n",
        "\n",
        "# Now add a row that averages all LLM scores for each system for each task\n",
        "unique_languages = sorted(sysLevel_scores_perLLM['language'].unique())\n",
        "unique_tasks_avg = sorted(sysLevel_scores_perLLM['task'].unique())\n",
        "unique_systems_avg = sorted(sysLevel_scores_perLLM['system'].unique())\n",
        "unique_input_sizes_avg = sorted(sysLevel_scores_perLLM['input_size'].unique())\n",
        "\n",
        "# print(unique_languages)\n",
        "# print(unique_tasks_avg)\n",
        "# print(unique_systems_avg)\n",
        "\n",
        "language_system_task_counter = 0\n",
        "for language in unique_languages:\n",
        "  if DEBUG:\n",
        "    print(f'Language: {language}')\n",
        "  for task in unique_tasks_avg:\n",
        "    if DEBUG:\n",
        "      print(f'Task: {task}')\n",
        "    for system in unique_systems_avg:\n",
        "      if DEBUG:\n",
        "        print(f'System: {system}')\n",
        "      for size in unique_input_sizes_avg:\n",
        "        if DEBUG:\n",
        "          print(f'Size: {size}')\n",
        "        # Get the average scores for each system for each task\n",
        "        no_omissions_scores = sysLevel_scores_perLLM.loc[(sysLevel_scores_perLLM['language'] == language) & (sysLevel_scores_perLLM['task'] == task) & (sysLevel_scores_perLLM['system'] == system) & (sysLevel_scores_perLLM['input_size'] == size), 'avg_no-omissions']\n",
        "        no_additions_scores = sysLevel_scores_perLLM.loc[(sysLevel_scores_perLLM['language'] == language) & (sysLevel_scores_perLLM['task'] == task) & (sysLevel_scores_perLLM['system'] == system) & (sysLevel_scores_perLLM['input_size'] == size), 'avg_no-additions']\n",
        "        grammaticality_scores = sysLevel_scores_perLLM.loc[(sysLevel_scores_perLLM['language'] == language) & (sysLevel_scores_perLLM['task'] == task) & (sysLevel_scores_perLLM['system'] == system) & (sysLevel_scores_perLLM['input_size'] == size), 'avg_grammaticality']\n",
        "        fluency_scores = sysLevel_scores_perLLM.loc[(sysLevel_scores_perLLM['language'] == language) & (sysLevel_scores_perLLM['task'] == task) & (sysLevel_scores_perLLM['system'] == system) & (sysLevel_scores_perLLM['input_size'] == size), 'avg_fluency']\n",
        "        # Get average\n",
        "        average_no_omissions_score = no_omissions_scores.mean()\n",
        "        average_no_additions_score = no_additions_scores.mean()\n",
        "        average_grammaticality_score = grammaticality_scores.mean()\n",
        "        average_fluency_score = fluency_scores.mean()\n",
        "        average_coherence_score = 0\n",
        "        if zip_model == 'Claude2':\n",
        "          coherence_scores = sysLevel_scores_perLLM.loc[(sysLevel_scores_perLLM['language'] == language) & (sysLevel_scores_perLLM['task'] == task) & (sysLevel_scores_perLLM['system'] == system) & (sysLevel_scores_perLLM['input_size'] == size), 'avg_coherence']\n",
        "          average_coherence_score = coherence_scores.mean()\n",
        "        if DEBUG:\n",
        "          if len(no_omissions_scores) > 0:\n",
        "            print(f'  Average no-omissions score: {average_no_omissions_score}')\n",
        "            print(f'  Average no-additions score: {average_no_additions_score}')\n",
        "            print(f'  Average grammaticality score: {average_grammaticality_score}')\n",
        "            print(f'  Average fluency score: {average_fluency_score}')\n",
        "            if zip_model == 'Claude2':\n",
        "              print(f'  Average Coherence score: {average_coherence_score}')\n",
        "\n",
        "        # If there are no numbers to average, \"mean\" returns a float that prints 'nan'. Filter out these cases.\n",
        "        if not math.isnan(average_fluency_score):\n",
        "          # Add a row to avg_scores dataframe\n",
        "          if zip_model == 'Claude2':\n",
        "            avg_sysLevel_scores_LLMs.loc[language_system_task_counter] = [language, task, system, size, 'Average_LLM', average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score, average_coherence_score]\n",
        "          else:\n",
        "            avg_sysLevel_scores_LLMs.loc[language_system_task_counter] = [language, task, system, size, 'Average_LLM', average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "          language_system_task_counter += 1\n",
        "          # Dump dataframe in a CSV file\n",
        "          avg_sysLevel_scores_LLMs.to_csv(os.path.join(csv_sys_level_scores, 'sysLevel_scores_avgLLM_perSize.csv'), index=False)\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "print()\n",
        "print(avg_sysLevel_scores_LLMs)"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "IndlLd5aBqYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get average across all sizes\n",
        "\n",
        "perllmORavgllm = 'perLLM'#@param['perLLM', 'avgLLM']\n",
        "update_params_unzip = True #@param {type:\"boolean\"}\n",
        "if update_params_unzip:\n",
        "  zip_model = 'Claude2' #@param['GPT-o3', 'Claude', 'Claude2', 'Llama-13B', 'R1_Llama-70B', 'R1_Llama-70B_bis']\n",
        "\n",
        "path_csv_scores = '/content/csv_sysLevel_scores/sysLevel_scores_'+perllmORavgllm+'_perSize.csv'\n",
        "df_scores = pd.read_csv(path_csv_scores)\n",
        "# Get average grouping by same value in the \"system\" column\n",
        "# print(df_scores)\n",
        "\n",
        "sysLevel_scores_LLM = None\n",
        "if zip_model == 'Claude2':\n",
        "  sysLevel_scores_LLM = pd.DataFrame(columns=['language', 'task', 'system', 'LLM', 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency', 'avg_coherence'])\n",
        "else:\n",
        "  sysLevel_scores_LLM = pd.DataFrame(columns=['language', 'task', 'system', 'LLM', 'avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency'])\n",
        "\n",
        "system_task_counter = 0\n",
        "# Get all possible values for task and system column\n",
        "unique_languages = sorted(df_scores['language'].unique())\n",
        "unique_tasks = sorted(df_scores['task'].unique())\n",
        "unique_systems = sorted(df_scores['system'].unique())\n",
        "unique_input_sizes = sorted(df_scores['input_size'].unique())\n",
        "print(' Unique tasks:', unique_tasks)\n",
        "print(' Unique systems:', unique_systems)\n",
        "print(' Unique input sizes:', unique_input_sizes)\n",
        "for language in unique_languages:\n",
        "  for task in unique_tasks:\n",
        "    for system in unique_systems:\n",
        "      # get scores in each of the 5 score columns\n",
        "      no_omissions_scores = df_scores.loc[(df_scores['language'] == language) & (df_scores['task'] == task) & (df_scores['system'] == system), 'avg_no-omissions']\n",
        "      no_additions_scores = df_scores.loc[(df_scores['language'] == language) & (df_scores['task'] == task) & (df_scores['system'] == system), 'avg_no-additions']\n",
        "      grammaticality_scores = df_scores.loc[(df_scores['language'] == language) & (df_scores['task'] == task) & (df_scores['system'] == system), 'avg_grammaticality']\n",
        "      fluency_scores = df_scores.loc[(df_scores['language'] == language) & (df_scores['task'] == task) & (df_scores['system'] == system), 'avg_fluency']\n",
        "      #Get average scores\n",
        "      average_no_omissions_score = no_omissions_scores.mean()\n",
        "      average_no_additions_score = no_additions_scores.mean()\n",
        "      average_grammaticality_score = grammaticality_scores.mean()\n",
        "      average_fluency_score = fluency_scores.mean()\n",
        "      average_coherence_score = 0\n",
        "      if zip_model == 'Claude2':\n",
        "        coherence_scores = df_scores.loc[(df_scores['language'] == language) & (df_scores['task'] == task) & (df_scores['system'] == system), 'avg_coherence']\n",
        "        average_coherence_score = coherence_scores.mean()\n",
        "\n",
        "      # If there are no numbers to average, \"mean\" returns a float that prints 'nan'. Filter out these cases.\n",
        "      if not math.isnan(average_fluency_score):\n",
        "        # Add a row to sysLevel_scores_perLLM dataframe\n",
        "        if zip_model == 'Claude2':\n",
        "          sysLevel_scores_LLM.loc[system_task_counter] = [language, task, system, df_scores['LLM'][0], average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score, average_coherence_score]\n",
        "        else:\n",
        "          sysLevel_scores_LLM.loc[system_task_counter] = [language, task, system, df_scores['LLM'][0], average_no_omissions_score, average_no_additions_score, average_grammaticality_score, average_fluency_score]\n",
        "        system_task_counter += 1\n",
        "\n",
        "print()\n",
        "print(sysLevel_scores_LLM)\n",
        "# Dump dataframe in a CSV file\n",
        "sysLevel_scores_LLM.to_csv(os.path.join(csv_sys_level_scores, 'sysLevel_scores_'+perllmORavgllm+'_avgSize.csv'), index=False)\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "7DffK_3JdfWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download csv folders\n",
        "from google.colab import files\n",
        "\n",
        "! zip -r /content/csv_sysLevel_scores.zip /content/csv_sysLevel_scores\n",
        "files.download('/content/csv_sysLevel_scores.zip')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nRh21UwPFfqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compare 2 jsons with results\n",
        "import json\n",
        "\n",
        "list_dicos_1 = json.load(open('/content/EN_Claude_scores.json'))\n",
        "list_dicos_2 = json.load(open('/content/EN_Claude_scores (2).json'))\n",
        "\n",
        "list1 = sorted(list_dicos_1, key=lambda d: d['eid'])\n",
        "print(list1)\n",
        "list2 = sorted(list_dicos_2, key=lambda d: d['eid'])\n",
        "print(list2)\n",
        "\n",
        "def list_of_dicts_equal(a: list[dict], b: list[dict]) -> bool:\n",
        "  return a == b\n",
        "\n",
        "list_of_dicts_equal(list1, list2)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1NtG308ITBVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate system-level scores humans\n",
        "Code taken and adapted from the GEM 2024 code\n"
      ],
      "metadata": {
        "id": "RPdmeqN-U4KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate system_level scores D2T assigned by the set of human annotators for each size and create CSVs\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "DEBUG = False\n",
        "\n",
        "csv_sys_level_scores = '/content/csv_sysLevel_scores'\n",
        "if not os.path.exists(csv_sys_level_scores):\n",
        "  os.makedirs(csv_sys_level_scores)\n",
        "\n",
        "pd.set_option('display.max_rows', 10)\n",
        "allowed_scores = [1, 2, 3, 4, 5]\n",
        "dico_sizes = {'0570': '0', '0088': '0', '0246': '0', '0213': '1', '0456': '1', '0348': '1', '0652': '2', '0064': '2', '0332': '2', '0479': '3', '0000': '3', '0303': '3', '0025': '4', '0137': '4', '0141': '4', '0147': '5', '0052': '5', '0091': '5', '0126': '6', '0165': '6', '0074': '6', '0117': '7', '0124': '7', '0013': '7', '0076': '8', '0068': '8', '0034': '8', '0144': '9', '0053': '9', '0140': '9'}\n",
        "# Use this dictionary to map values to other values\n",
        "score_map = {0:0}\n",
        "\n",
        "def check_scores(score_list, allowed_scores):\n",
        "  new_score_list = []\n",
        "  for score in score_list:\n",
        "    if score in score_map:\n",
        "      score = score_map[score]\n",
        "    assert score in allowed_scores, f'Score out of range: {score}'\n",
        "    new_score_list.append(score)\n",
        "  return pd.Series(new_score_list)\n",
        "\n",
        "# Load the files one at a time; there is one file per language (since human annotators did not each do all annotations, unlike LLMs)\n",
        "json_folder = '/content/human-eval_final_jsons'\n",
        "json_files = glob.glob(os.path.join(json_folder, '*.json'))\n",
        "\n",
        "pd_list = [pd.read_json(json_file) for json_file in sorted(json_files)]\n",
        "\n",
        "# Create dataframes to store system-level scores\n",
        "sysLevel_scores_Humans = pd.DataFrame(columns=['language', 'task', 'system', 'input_size', 'Human', 'avg_grammaticality', 'avg_fluency', 'avg_coherence'])\n",
        "\n",
        "system_task_counter = 0\n",
        "for df in pd_list:\n",
        "  if DEBUG:\n",
        "    print(df)\n",
        "  # split id column using underscores and create corresponding columns in the dataframe\n",
        "  df[['language', 'task', 'id', 'system']] = df['id'].str.split('_', expand=True)\n",
        "  # NEW Add a column to df with the size of the input according to dico_sizes\n",
        "  df['input_size'] = df['id'].map(dico_sizes)\n",
        "\n",
        "  # Convert 'task' and 'system' columns to string to avoid TypeError during sorting\n",
        "  df['task'] = df['task'].astype(str)\n",
        "  df['system'] = df['system'].astype(str)\n",
        "\n",
        "  print('Getting human scores for', df['language'][0])\n",
        "\n",
        "  # Get all possible values for task and system column\n",
        "  unique_tasks = sorted(df['task'].unique())\n",
        "  unique_systems = sorted(df['system'].unique())\n",
        "  unique_input_sizes = sorted(df['input_size'].unique())\n",
        "  print(' Unique tasks:', unique_tasks)\n",
        "  print(' Unique systems:', unique_systems)\n",
        "  print(' Unique input sizes:', unique_input_sizes)\n",
        "\n",
        "  # Get the average scores for each system for each task\n",
        "  for task in unique_tasks:\n",
        "    if DEBUG:\n",
        "      print(f'Task: {task}\\n===========')\n",
        "    for system in unique_systems:\n",
        "      if DEBUG:\n",
        "        print(f'\\nSystem: {system}')\n",
        "      for size in unique_input_sizes:\n",
        "        if DEBUG:\n",
        "          print(f'Size = {size}')\n",
        "        # get scores in each of the 4 score columns\n",
        "        grammaticality_scores = df.loc[(df['task'] == task) & (df['system'] == system) & (df['input_size'] == size), 'grammaticality']\n",
        "        grammaticality_scores = check_scores(grammaticality_scores, allowed_scores)\n",
        "        fluency_scores = df.loc[(df['task'] == task) & (df['system'] == system) & (df['input_size'] == size), 'fluency']\n",
        "        fluency_scores = check_scores(fluency_scores, allowed_scores)\n",
        "        coherence_scores = df.loc[(df['task'] == task) & (df['system'] == system) & (df['input_size'] == size), 'coherence']\n",
        "        coherence_scores = check_scores(coherence_scores, allowed_scores)\n",
        "        #Get average scores\n",
        "        average_grammaticality_score = grammaticality_scores.mean()\n",
        "        average_fluency_score = fluency_scores.mean()\n",
        "        average_coherence_score = coherence_scores.mean()\n",
        "        if DEBUG:\n",
        "          if len(no_omissions_scores) > 0:\n",
        "            print(f'  Average grammaticality score: {average_grammaticality_score}')\n",
        "            print(f'  Average fluency score: {average_fluency_score}')\n",
        "            print(f'  Average coherence score: {average_coherence_score}')\n",
        "\n",
        "        # If there are no numbers to average, \"mean\" returns a float that prints 'nan'. Filter out these cases.\n",
        "        if not math.isnan(average_fluency_score):\n",
        "          assert coherence_scores.count() == grammaticality_scores.count() == fluency_scores.count() , 'There should be the same number of ratings for each criterion!'\n",
        "          # Add row to sysLevel_scores_Humans dataframe\n",
        "          sysLevel_scores_Humans.loc[system_task_counter] = [df['language'][0], task, system, size, 'Human', average_grammaticality_score, average_fluency_score, average_coherence_score]\n",
        "          system_task_counter += 1\n",
        "\n",
        "print()\n",
        "# At this point, all submissions, including the withdrawn one, were evaluated\n",
        "# EN = 46 rows: 7 systems * 6 datasets + 1 system * 3 datasets + human *1 dataset\n",
        "# ES = 18 rows: 3 systems * 6 datasets\n",
        "# SW = 15 rows: 2 systems * 6 datasets + 1 system * 3 datasets\n",
        "# All languages = 79 rows; 4 LLMS-> 316 rows\n",
        "# assert len(sysLevel_scores_Humans) == 64, 'There should be 64 rows (EN+ES) with scores for the human evals!'\n",
        "# print('Number of rows OK (64).')\n",
        "print(sysLevel_scores_Humans)\n",
        "# Dump dataframe in a CSV file\n",
        "sysLevel_scores_Humans.to_csv(os.path.join(csv_sys_level_scores, 'sysLevel_scores_Humans_perSize.csv'), index=False)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "oiQeuaqZU7w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get average across all sizes\n",
        "\n",
        "\n",
        "path_csv_scores = '/content/csv_sysLevel_scores/sysLevel_scores_Humans_perSize.csv'\n",
        "df_scores = pd.read_csv(path_csv_scores)\n",
        "# Get average grouping by same value in the \"system\" column\n",
        "# print(df_scores)\n",
        "\n",
        "sysLevel_scores_hum = pd.DataFrame(columns=['language', 'task', 'system', 'Human', 'avg_grammaticality', 'avg_fluency', 'avg_coherence'])\n",
        "\n",
        "system_task_counter = 0\n",
        "# Get all possible values for task and system column\n",
        "unique_languages = sorted(df_scores['language'].unique())\n",
        "unique_tasks = sorted(df_scores['task'].unique())\n",
        "unique_systems = sorted(df_scores['system'].unique())\n",
        "unique_input_sizes = sorted(df_scores['input_size'].unique())\n",
        "print(' Unique tasks:', unique_tasks)\n",
        "print(' Unique systems:', unique_systems)\n",
        "print(' Unique input sizes:', unique_input_sizes)\n",
        "for language in unique_languages:\n",
        "  for task in unique_tasks:\n",
        "    for system in unique_systems:\n",
        "      # get scores in each of the 5 score columns\n",
        "      grammaticality_scores = df_scores.loc[(df_scores['language'] == language) & (df_scores['task'] == task) & (df_scores['system'] == system), 'avg_grammaticality']\n",
        "      fluency_scores = df_scores.loc[(df_scores['language'] == language) & (df_scores['task'] == task) & (df_scores['system'] == system), 'avg_fluency']\n",
        "      coherence_scores = df_scores.loc[(df_scores['language'] == language) & (df_scores['task'] == task) & (df_scores['system'] == system), 'avg_coherence']\n",
        "      #Get average scores\n",
        "      average_grammaticality_score = grammaticality_scores.mean()\n",
        "      average_fluency_score = fluency_scores.mean()\n",
        "      average_coherence_score = 0\n",
        "      average_coherence_score = coherence_scores.mean()\n",
        "\n",
        "      # If there are no numbers to average, \"mean\" returns a float that prints 'nan'. Filter out these cases.\n",
        "      if not math.isnan(average_fluency_score):\n",
        "        # Add a row to sysLevel_scores_perLLM dataframe\n",
        "        sysLevel_scores_hum.loc[system_task_counter] = [language, task, system, df_scores['Human'][0], average_grammaticality_score, average_fluency_score, average_coherence_score]\n",
        "        system_task_counter += 1\n",
        "\n",
        "print()\n",
        "print(sysLevel_scores_hum)\n",
        "# Dump dataframe in a CSV file\n",
        "sysLevel_scores_hum.to_csv(os.path.join(csv_sys_level_scores, 'sysLevel_scores_Humans_avgSize.csv'), index=False)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qa7JIFLXkR1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create plots and LaTeX tables from scores aggregated in \"Qualitative evaluation\" above."
      ],
      "metadata": {
        "id": "hbB2X551I7r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Functions from GPT\n",
        "\n",
        "# Re-import essentials\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "if not os.path.exists('/content/figures'):\n",
        "  os.mkdir('/content/figures')\n",
        "\n",
        "update_params_unzip = True #@param {type:\"boolean\"}\n",
        "if update_params_unzip:\n",
        "  zip_model = 'Claude2' #@param['GPT-o3', 'Claude', 'Claude2', 'Llama-13B', 'R1_Llama-70B', 'R1_Llama-70B_bis']\n",
        "\n",
        "exclude_truncated_outputs = False #@param {type:\"boolean\"}\n",
        "exclude_outputs_no_truncated_counterpart = True #@param {type:\"boolean\"}\n",
        "systems_to_exclude = []\n",
        "if exclude_truncated_outputs:\n",
        "  systems_to_exclude.extend(['e2e-0.95', 'e2e-0.90', 'e2e-0.85', 'Hum-0.9', 'Hum-0.7', 'Hum-0.5'])\n",
        "if exclude_outputs_no_truncated_counterpart:\n",
        "  systems_to_exclude.extend(['sng', 'mul', 'Qwen3-32b (EN)', 'Llama2-13b (EN)', 'GPT4.1', 'R1-Llama-70b (EN)', 'Claude3.7-sonnet', 'FORGe', 'UCCIX (GA)'])\n",
        "\n",
        "# Load the CSV again\n",
        "file_path = \"/content/csv_sysLevel_scores/sysLevel_scores_avgLLM.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Provided system_IDnames_dico\n",
        "system_IDnames_dico = {}\n",
        "\n",
        "if zip_model == 'Claude2':\n",
        "  system_IDnames_dico = {\n",
        "    'en': {'00': 'e2e', '01': 'sng', '02': 'mul', '03': 'e2e-0.95', '04': 'e2e-0.90',\n",
        "           '05': 'e2e-0.85', '06': 'e2e-input0.95', '07': 'e2e-input0.90', '08': 'e2e-input0.85'},\n",
        "    'ga': {'00': 'e2e', '01': 'sng', '02': 'mul', '03': 'e2e-0.95', '04': 'e2e-0.90',\n",
        "           '05': 'e2e-0.85', '06': 'e2e-input0.95', '07': 'e2e-input0.90', '08': 'e2e-input0.85'},\n",
        "  }\n",
        "else:\n",
        "  system_IDnames_dico = {\n",
        "    'en': {'01': 'Qwen3-32b (EN)', '02': 'Llama2-13b (EN)', '03': 'GPT4.1', '04': 'R1-Llama-70b (EN)',\n",
        "           '05': 'Claude3.7-sonnet', '07': 'FORGe', '08': 'Hum-1.0', '09': 'Hum-0.9',\n",
        "           '10': 'Hum-0.7', '11': 'Hum-0.5'},\n",
        "    'ga': {'03': 'GPT4.1', '05': 'Claude3.7-sonnet', '06': 'UCCIX (GA)', '07': 'FORGe',\n",
        "           '08': 'Hum-1.0', '09': 'Hum-0.9', '10': 'Hum-0.7', '11': 'Hum-0.5'}\n",
        "  }\n",
        "\n",
        "# Map system IDs to names based on language\n",
        "def map_system_name(row):\n",
        "  lang = row['language']\n",
        "  sys_id = f\"{int(row['system']):02d}\"\n",
        "  return system_IDnames_dico.get(lang, {}).get(sys_id, sys_id)\n",
        "\n",
        "df['system_name'] = df.apply(map_system_name, axis=1)\n",
        "\n",
        "# Map input_size indices to bins\n",
        "size_bins = []\n",
        "if zip_model == 'Claude2':\n",
        "  size_bins = ['8-19', '20-39', '40-59', '60-79', '80-99', '100-119', '120-139', '140-159', '160-179', '180-199']\n",
        "else:\n",
        "  size_bins = ['8-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69']\n",
        "df['input_size_bin'] = df['input_size'].map(lambda x: size_bins[int(x)] if int(x) < len(size_bins) else str(x))\n",
        "\n",
        "value_vars_list = []\n",
        "if zip_model == 'Claude2':\n",
        "  value_vars_list=['avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency', 'avg_coherence']\n",
        "else:\n",
        "  value_vars_list=['avg_no-omissions', 'avg_no-additions', 'avg_grammaticality', 'avg_fluency']\n",
        "# Melt dataframe to long format for seaborn plotting\n",
        "df_long = df.melt(\n",
        "    id_vars=['language', 'system_name', 'input_size_bin'],\n",
        "    value_vars = value_vars_list,\n",
        "    var_name='criterion',\n",
        "    value_name='score'\n",
        ")\n",
        "\n",
        "# Prepare consistent color palette across plots for systems\n",
        "all_systems = sorted(set(df_long['system_name']))\n",
        "palette = sns.color_palette(\"tab10\", n_colors=len(all_systems))\n",
        "system_colors = dict(zip(all_systems, palette))\n",
        "\n",
        "# Plot generation: 4 criteria x 2 languages = 8 plots\n",
        "criteria_map = {}\n",
        "max_score = '7'\n",
        "if zip_model == 'Claude2':\n",
        "  max_score = '5'\n",
        "  criteria_map = {\n",
        "    'avg_no-omissions': 'No-Omissions',\n",
        "    'avg_no-additions': 'No-Additions',\n",
        "    'avg_grammaticality': 'Grammaticality',\n",
        "    'avg_fluency': 'Fluency',\n",
        "    'avg_coherence': 'Coherence'\n",
        "  \t}\n",
        "else:\n",
        "  criteria_map = {\n",
        "    'avg_no-omissions': 'No-Omissions',\n",
        "    'avg_no-additions': 'No-Additions',\n",
        "    'avg_grammaticality': 'Grammaticality',\n",
        "    'avg_fluency': 'Fluency'\n",
        "  }\n",
        "\n",
        "# Create a figure with 4 rows x 2 columns\n",
        "fig, axes = plt.subplots(len(criteria_map.keys()), 2, figsize=(14, 18), sharex=True, sharey=True)\n",
        "fig.subplots_adjust(hspace=0.3, wspace=0.15)\n",
        "\n",
        "handles_list = []\n",
        "labels_list = []\n",
        "for i, (crit_key, crit_name) in enumerate(criteria_map.items()):\n",
        "    for j, lang in enumerate(['en', 'ga']):\n",
        "        ax = axes[i, j]\n",
        "        subset = df_long[(df_long['criterion'] == crit_key) & (df_long['language'] == lang)]\n",
        "        systems_to_process = [system for system in sorted(subset['system_name'].unique()) if system not in systems_to_exclude]\n",
        "        for system in systems_to_process:\n",
        "            sys_data = subset[subset['system_name'] == system]\n",
        "            ax.plot(\n",
        "                sys_data['input_size_bin'],\n",
        "                sys_data['score'],\n",
        "                label=system,\n",
        "                marker='o',\n",
        "                color=system_colors[system]\n",
        "            )\n",
        "        ax.set_title(f\"{crit_name} - {'English' if lang == 'en' else 'Irish'}\", fontsize=13)\n",
        "        ax.set_ylabel(f'Score (1-{max_score})', fontsize=11)\n",
        "        ax.set_ylim(1, int(max_score))\n",
        "        ax.grid(True, linestyle=':', alpha=0.4)\n",
        "        if i == 3:\n",
        "            ax.set_xlabel('Input Size', fontsize=11)\n",
        "\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "        for handle in handles:\n",
        "            if handle not in handles_list:\n",
        "                handles_list.append(handle)\n",
        "        for label in labels:\n",
        "            if label not in labels_list:\n",
        "                labels_list.append(label)\n",
        "\n",
        "# Add a legend\n",
        "# Single legend outside\n",
        "# Choose colors of labels\n",
        "# fig.legend(handles_list, labels_list, title='Systems', bbox_to_anchor=(1.02, 0.5), loc='center left')\n",
        "fig.legend(handles_list, labels_list, title='Systems', bbox_to_anchor=(0.45, 0.48), loc='lower center')\n",
        "\n",
        "# plt.suptitle('System Performance Across Criteria, Input Sizes, and Languages', fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 0.88, 0.97])\n",
        "# Save image in a png file\n",
        "plt.savefig('/content/figures/system_performance_across_criteria_input_sizes_and_languages.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v2_ArQBFQ3l6",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download image\n",
        "! zip -r /content/figures.zip /content/figures\n",
        "files.download('/content/figures.zip')"
      ],
      "metadata": {
        "id": "2KNm1b-0UZqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run divesity metrics on output texts (use T4 GPU Runtime)"
      ],
      "metadata": {
        "id": "tf0fgiWddu8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load file with texts of one language for all systems\n",
        "\n",
        "import json\n",
        "import codecs\n",
        "import re\n",
        "\n",
        "# Open /content/en_longInputD2T_LLMtexts.json\n",
        "language = 'en' #@param['en', 'ga']\n",
        "data = json.load(codecs.open(f'/content/{language}_longInputD2T_LLMtexts.json', 'r', 'utf-8'))\n",
        "\n",
        "unique_sys_ids_dico = {}\n",
        "for i, datapoint in enumerate(data):\n",
        "  dp_id = datapoint['id'].rsplit('_', 1)[1]\n",
        "  # print(dp_id)\n",
        "  if dp_id not in unique_sys_ids_dico:\n",
        "    unique_sys_ids_dico[dp_id] = [datapoint['output']]\n",
        "  else:\n",
        "    unique_sys_ids_dico[dp_id].append(datapoint['output'])\n",
        "  # print(datapoint['text'])\n",
        "\n",
        "# print(unique_sys_ids)\n",
        "for key in sorted(unique_sys_ids_dico.keys()):\n",
        "  print(key, unique_sys_ids_dico[key])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "idLZXeJFd960"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Diversity metrics functions\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from nltk.util import ngrams\n",
        "from typing import List, Optional\n",
        "from collections import Counter\n",
        "import torch, gc\n",
        "from sklearn.metrics.pairwise import pairwise_distances_chunked\n",
        "\n",
        "from diversity import (\n",
        "    compression_ratio,\n",
        "    homogenization_score,\n",
        "    ngram_diversity_score#,\n",
        "    # self_repetition_score\n",
        ")\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Can't import chamfer_dist from \"diversity\", so copied it here\n",
        "# Also added code to flush memory before returning score\n",
        "def chamfer_dist(\n",
        "      data: List[str],\n",
        "      model: Optional[str] = 'Qwen/Qwen3-Embedding-0.6B',\n",
        "      verbose: Optional[bool] = True,\n",
        "      batch_size: Optional[int] = 64\n",
        ") -> float:\n",
        "  \"\"\"\n",
        "  Calculates the chamfer distance for a set of documents (corpus-level).\n",
        "  This is the average minimum pairwise distance of a data instance to other instances.\n",
        "  Args:\n",
        "      data (List[str]): Strings to score.\n",
        "      model(str, optional): Model to use for embedding. Defaults to 'Qwen/Qwen3-Embedding-0.6B'.\n",
        "      verbose(bool, optional): Whether to display progress bar. Defaults to True.\n",
        "      batch_size(int, optional): Batch size for embedding. Defaults to 64.\n",
        "  Returns:\n",
        "      float: Chamfer distance.\n",
        "  \"\"\"\n",
        "  model = SentenceTransformer(model)\n",
        "  embeddings = model.encode(data, batch_size=batch_size, show_progress_bar=verbose)\n",
        "  distances = cosine_distances(embeddings)\n",
        "  min_distances = np.min(distances + np.eye(len(distances)) * 1e9, axis=1)\n",
        "\n",
        "  # Free up memory\n",
        "  del embeddings, distances, model\n",
        "  gc.collect()\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "  return np.mean(min_distances).round(3)\n",
        "\n",
        "# Functions to avoid OOM errors with very long texts (some have up to 6k words and this produces problems when embedding)\n",
        "def embed_long_text(text, model, max_length=512, overlap=50):\n",
        "    \"\"\"Split long text into chunks, embed, then average-pool.\"\"\"\n",
        "    tokens = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_length - overlap):\n",
        "        chunk = \" \".join(tokens[i:i + max_length])\n",
        "        chunks.append(chunk)\n",
        "    with torch.no_grad():\n",
        "        chunk_embs = model.encode(chunks, convert_to_numpy=True, show_progress_bar=False)\n",
        "    return np.mean(chunk_embs, axis=0)  # mean pooling\n",
        "\n",
        "def chamfer_dist_large(\n",
        "    data,\n",
        "    model_name='Qwen/Qwen3-Embedding-0.6B',\n",
        "    batch_size=64,\n",
        "    verbose=True\n",
        "):\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    #  Embed each document safely (with progress bar)\n",
        "    embeddings = []\n",
        "    for doc in tqdm(data, desc=\"Encoding documents\", disable=not verbose):\n",
        "        emb = embed_long_text(doc, model)\n",
        "        embeddings.append(emb)\n",
        "    embeddings = np.vstack(embeddings)\n",
        "\n",
        "    #  Compute pairwise distances in chunks (with progress bar)\n",
        "    min_dists = []\n",
        "    for chunk in tqdm(\n",
        "        pairwise_distances_chunked(embeddings, embeddings, metric=\"cosine\"),\n",
        "        desc=\"Computing pairwise distances\",\n",
        "        disable=not verbose\n",
        "    ):\n",
        "        np.fill_diagonal(chunk, 1e9)\n",
        "        min_dists.extend(np.min(chunk, axis=1))\n",
        "\n",
        "    #  Cleanup\n",
        "    del embeddings, model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    result = float(np.mean(min_dists))\n",
        "    # print(f\"Chamfer distance: {result:.4f}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "# Using here because the GitHub version ignores the \"n\" parameter; changed the following \"4\" by an \"n\"\n",
        "# ngram_docs = [list(set([' '.join(ngram) for ngram in ngrams(doc.split(), 4)])) for doc in dataset]\n",
        "# ngram_docs = [list(set([' '.join(ngram) for ngram in ngrams(doc.split(), n)])) for doc in dataset]\n",
        "def self_repetition_score(\n",
        "        dataset: List[str],\n",
        "        n: int = 4,\n",
        "        verbose: bool = True\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates a self-repetition score for a dataset based on the\n",
        "    repetition of ngrams within the corpus.\n",
        "\n",
        "    Args:\n",
        "        dataset (List[str]): A list of documents (strings) to analyze.\n",
        "        n (int): Size of the ngrams to check for repetition. Defaults to 4.\n",
        "        verbose (bool): enable/disable show progress bar\n",
        "\n",
        "    Returns:\n",
        "        float: The self-repetition score, averaged over the dataset.\n",
        "    \"\"\"\n",
        "    total_sum = 0\n",
        "\n",
        "    # Get all unique ngrams per doc\n",
        "    ngram_docs = [list(set([' '.join(ngram) for ngram in ngrams(doc.split(), n)])) for doc in dataset]\n",
        "\n",
        "    # Count occurrences of unique ngrams across whole dataset\n",
        "    all_ngrams = sum(ngram_docs, [])\n",
        "    ngram_counts = Counter(all_ngrams)\n",
        "\n",
        "    for ngram_doc in tqdm(ngram_docs, desc=\"Calculating self-repetition score\", disable=(not verbose)):\n",
        "        # Find the total occurrence of an n-gram and subtract current doc's n-gram count\n",
        "        # to get the count of occurrences of an n-gram in other docs\n",
        "        sum_ni = sum([ngram_counts[ngram] for ngram in ngram_doc]) - len(ngram_doc)\n",
        "\n",
        "        # add-one to avoid zero error\n",
        "        total_sum += np.log(sum_ni + 1)\n",
        "    return total_sum / len(dataset)\n",
        "\n",
        "###############################################\n",
        "# Average text length in characters\n",
        "def avg_text_length(texts):\n",
        "  avg_length_c = sum(len(text) for text in texts) / len(texts)\n",
        "  avg_length_c_2f = round(avg_length_c, 2)\n",
        "  print(f\"Average text length: {avg_length_c:.2f} characters\")\n",
        "  # Average text length in words\n",
        "  avg_length_w = sum(len(text.split()) for text in texts) / len(texts)\n",
        "  avg_length_w_2f = round(avg_length_w, 2)\n",
        "  print(f\"Average text length: {avg_length_w:.2f} words\")\n",
        "  print('-----------')\n",
        "  return avg_length_c_2f, avg_length_w_2f\n",
        "\n",
        "###############################################\n",
        "# N-gram diversity (distinct-k); number of unique information units (distinct n-grams) divided by the total number of information units (all n-grams) (https://github.com/cshaib/diversity_)\n",
        "# Calculated using all n-grams until the indicated max size in the parameter\n",
        "def ngram_diversity_score_1to4(texts):\n",
        "  # ngd1 = ngram_diversity_score(texts, num_n=1)\n",
        "  # ngd1_4f = round(ngd1, 4)\n",
        "  # print(f\"1-gram Diversity: {ngd1:.4f}\")\n",
        "  # ngd2 = ngram_diversity_score(texts, num_n=2)\n",
        "  # ngd2_4f = round(ngd2, 4)\n",
        "  # print(f\"1-2-gram Diversity: {ngd2:.4f}\")\n",
        "  # ngd3 = ngram_diversity_score(texts, num_n=3)\n",
        "  # ngd3_4f = round(ngd3, 4)\n",
        "  # print(f\"1-2-3-gram Diversity: {ngd3:.4f}\")\n",
        "  ngd4 = ngram_diversity_score(texts, num_n=4)\n",
        "  ngd4_4f = round(ngd4, 4)\n",
        "  print(f\"n-gram Diversity (1 to 4): {ngd4:.4f}\")\n",
        "  print('-----------')\n",
        "  return ngd4_4f\n",
        "\n",
        "###############################################\n",
        "# Self repetition (https://github.com/cshaib/diversity)\n",
        "# Calculates a self-repetition score for a dataset based on the repetition of ngrams within the corpus.\n",
        "def self_repetition_score_1to4(texts):\n",
        "  srs1 = self_repetition_score(texts, 1)\n",
        "  srs1_4f = round(srs1, 4)\n",
        "  print(f\"Unigram Self-repetition: {srs1:.4f}\")\n",
        "  srs2 = self_repetition_score(texts, 2)\n",
        "  srs2_4f = round(srs2, 4)\n",
        "  print(f\"Bigram Self-repetition: {srs2:.4f}\")\n",
        "  srs3 = self_repetition_score(texts, 3)\n",
        "  srs3_4f = round(srs3, 4)\n",
        "  print(f\"Trigram Self-repetition: {srs3:.4f}\")\n",
        "  srs4 = self_repetition_score(texts, 4)\n",
        "  srs4_4f = round(srs4, 4)\n",
        "  print(f\"Quadrigram Self-repetition: {srs4:.4f}\")\n",
        "  print('-----------')\n",
        "  return srs1_4f, srs2_4f, srs3_4f, srs4_4f\n",
        "\n",
        "###############################################\n",
        "# Homogenization score: provides a scores representing how homogeneous a set of documents is (https://github.com/cshaib/diversity)\n",
        "# (used to compare e.g. texts written with and without assistance of LLMs, showing that texts written using LLMs are more homogeneous, i.e. have a higher Homogenization score).\n",
        "# metric = 'bleu'\n",
        "def homogenization_score_metric(texts, metric='rougel', use_stemmer = False):\n",
        "  hs = homogenization_score(texts, measure=metric, use_stemmer=use_stemmer)\n",
        "  hs_4f = round(hs, 4)\n",
        "  print(f\"Homogenization): {hs:.4f}\")\n",
        "  print('-----------')\n",
        "  return hs_4f\n",
        "\n",
        "###############################################\n",
        "# Compression ratio (https://github.com/cshaib/diversity)\n",
        "# Compression ratio calculates the ratio of the size of the compressed file to its original size. If the compression ratio is high, it indicates the file was highly compressible and thus had higher redundancy. This would indicate lower diversity in the file contents.\n",
        "def compression_ratio_func(texts):\n",
        "  cr = compression_ratio(texts)\n",
        "  cr_4f = round(cr, 4)\n",
        "  print(f\"Compression ratio: {cr:.4f}\")\n",
        "  print('-----------')\n",
        "  return cr_4f\n",
        "\n",
        "###############################################\n",
        "# Chamfer dist (https://github.com/cshaib/diversity)\n",
        "# Calculates the chamfer distance for a set of documents (corpus-level). This is the average minimum pairwise distance of a data instance to other instances.\n",
        "# Qwen/Qwen3-Embedding-0.6B is multilingual and does support Irish\n",
        "def chamfer_dist_qwen3(texts):\n",
        "  chd = chamfer_dist(texts, model = 'Qwen/Qwen3-Embedding-0.6B')\n",
        "  chd_4f = round(chd, 4)\n",
        "  print(f\"Chamfer distance: {chd:.4f}\")\n",
        "  print('-----------')\n",
        "  return chd_4f\n",
        "\n",
        "def chamfer_dist_large_qwen3(texts):\n",
        "  chd = chamfer_dist_large(texts, model_name = 'Qwen/Qwen3-Embedding-0.6B')\n",
        "  chd_4f = round(chd, 4)\n",
        "  print(f\"Chamfer distance: {chd:.4f}\")\n",
        "  print('-----------')\n",
        "  return chd_4f\n",
        "\n",
        "###############################################\n",
        "# N-gram Entropy (Ent-n) (https://amitness.com/posts/diversity-evals/)\n",
        "# The intuition behind it is that in an ideal case, all the texts generated from an LLM would be unique and no n-gram would be repeated more than once.\n",
        "# We can measure this by collecting all the unique bigrams in the text and calculating their count and the relative frequency. This would give us a probability distribution over the bigrams.\n",
        "# Diversity = Entropy calculated as follows:\n",
        "# probs = [0.25, 0.25, 0.25, 0.25]\n",
        "# -sum(p * math.log(p) for p in probs)\n",
        "def generate_ngrams(words, n: int):\n",
        "  return [\" \".join(words[i : i + n]) for i in range(len(words) - n + 1)]\n",
        "\n",
        "def ngram_entropy(texts: list[str], n: int = 2) -> float:\n",
        "  ngrams = []\n",
        "  for text in texts:\n",
        "    words = text.split()\n",
        "    ngrams.extend(generate_ngrams(words, n))\n",
        "\n",
        "  ngram_counts = Counter(ngrams)\n",
        "  total_ngrams = sum(ngram_counts.values())\n",
        "\n",
        "  ngram_frequencies = [count / total_ngrams for ngram, count in ngram_counts.items()]\n",
        "\n",
        "  entropy = -sum(freq * math.log(freq) for freq in ngram_frequencies)\n",
        "\n",
        "  return entropy\n",
        "\n",
        "# print(\"Unigram entropy:\", ngram_entropy(texts, n=1))\n",
        "# print(\"Bigram entropy:\", ngram_entropy(texts, n=2))\n",
        "# print(\"Trigram entropy:\", ngram_entropy(texts, n=3))\n",
        "# print('-----------')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r-MtJm93eb2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run metrics\n",
        "import pandas as pd\n",
        "\n",
        "quick_test = False#@param{'type':'boolean'}\n",
        "\n",
        "# Provided system_IDnames_dico\n",
        "system_IDnames_dico = {\n",
        "    'en': {'01': 'Qwen3-32b (EN)', '02': 'Llama2-13b (EN)', '03': 'GPT4.1', '04': 'R1-Llama-70b (EN)',\n",
        "           '05': 'Claude3.7-sonnet', '07': 'FORGe', '08': 'Hum-1.0', '09': 'Hum-0.9',\n",
        "           '10': 'Hum-0.7', '11': 'Hum-0.5'},\n",
        "    'ga': {'03': 'GPT4.1', '05': 'Claude3.7-sonnet', '06': 'UCCIX (GA)', '07': 'FORGe',\n",
        "           '08': 'Hum-1.0', '09': 'Hum-0.9', '10': 'Hum-0.7', '11': 'Hum-0.5'}\n",
        "}\n",
        "\n",
        "if quick_test:\n",
        "  texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The quick brown fox jumps over the lazy dog again.\",\n",
        "    \"Suddenly, the quick brown fox leaps swiftly over the sleeping dog.\"\n",
        "  ]\n",
        "  # Lexical diversity, length\n",
        "  avg_length_c_2f, avg_length_w_2f = avg_text_length(texts)\n",
        "  ngd4_4f = ngram_diversity_score_1to4(texts)\n",
        "  hs_4f = homogenization_score_metric(texts, 'bleu')\n",
        "  srs1_4f, srs2_4f, srs3_4f, srs4_4f = self_repetition_score_1to4(texts)\n",
        "  cr_4f = compression_ratio_func(texts)\n",
        "  # Semantic diversity\n",
        "  # chd_4f = chamfer_dist_qwen3(texts)\n",
        "  chd_4f = chamfer_dist_large_qwen3(texts)\n",
        "\n",
        "else:\n",
        "  rows = []\n",
        "  for key in sorted(unique_sys_ids_dico.keys()):\n",
        "  # key = '2'\n",
        "    print(f'############################\\nCalculating diversity scores for System {key}....')\n",
        "    texts = unique_sys_ids_dico[key]\n",
        "    # Lexical diversity, length\n",
        "    avg_length_c_2f, avg_length_w_2f = avg_text_length(texts)\n",
        "    ngd4_4f = ngram_diversity_score_1to4(texts)\n",
        "    # not sure how to interpret homogenization\n",
        "    hs_4f = homogenization_score_metric(texts, 'rougel', use_stemmer=True)\n",
        "    srs1_4f, srs2_4f, srs3_4f, srs4_4f = self_repetition_score_1to4(texts)\n",
        "    cr_4f = compression_ratio_func(texts)\n",
        "    # Semantic diversity\n",
        "    chd_4f = chamfer_dist_large_qwen3(texts)\n",
        "    rows.append({\n",
        "    \"System\": system_IDnames_dico[language][key.zfill(2)],\n",
        "    # \"AvgLenC\": f\"{avg_length_c_2f:.1f}\",\n",
        "    \"AvgLenW\": f\"{avg_length_w_2f:.1f}\",\n",
        "    \"NGD-1to4 ($\\uparrow$)\": f\"{ngd4_4f:.2f}\",\n",
        "    \"CompRate ($\\downarrow$)\": f\"{cr_4f:.2f}\",\n",
        "    # \"Rep1\": f\"{srs1_4f:.2f}\",\n",
        "    # \"Rep2\": f\"{srs2_4f:.2f}\",\n",
        "    # \"Rep3\": f\"{srs3_4f:.2f}\",\n",
        "    \"SelfRep-4 ($\\downarrow$)\": f\"{srs4_4f:.2f}\",\n",
        "    \"Homog-RL ($\\downarrow$)\": f\"{hs_4f:.3f}\",\n",
        "    \"ChamDist ($\\uparrow$)\": f\"{chd_4f:.2f}\"\n",
        "    })\n",
        "\n",
        "  df = pd.DataFrame(rows)\n",
        "  df = df.sort_values(by=\"System\").reset_index(drop=True)\n",
        "  latex = df.to_latex(index=False, escape=False, column_format=\"lccccccccccccc\")\n",
        "  print(latex)\n"
      ],
      "metadata": {
        "id": "oIvokqouiyyj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create CSVs for calculating statistical significance"
      ],
      "metadata": {
        "id": "fJl8l_XWnTC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create one file with all scores for each language+task+system+evaluator combination\n",
        "#These are the files for calculating correlations\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "if os.path.exists('/content/csv_for_correls.zip'):\n",
        "  ! rm -rf /content/csv_for_correls.zip\n",
        "\n",
        "if not os.path.exists('/content/csv_for_correls'):\n",
        "  os.mkdir('/content/csv_for_correls')\n",
        "\n",
        "DEBUG = True#@param{'type':'boolean'}\n",
        "eval_method = 'LLM' #@param ['LLM', 'Human']\n",
        "# List here all systems that have scores from all evaluators (hum+LLMS) on at least one dataset. Exclude withdrawn system (system 8)\n",
        "system_IDnames_dico = {\n",
        "  'en': {'0': 'e2e', '1': 'sng', '2': 'mul', '3': 'e2e-0.95', '4': 'e2e-0.90',\n",
        "          '5': 'e2e-0.85', '6': 'e2e-input0.95', '7': 'e2e-input0.90', '8': 'e2e-input0.85'},\n",
        "  'ga': {'0': 'e2e', '1': 'sng', '2': 'mul', '3': 'e2e-0.95', '4': 'e2e-0.90',\n",
        "          '5': 'e2e-0.85', '6': 'e2e-input0.95', '7': 'e2e-input0.90', '8': 'e2e-input0.85'},\n",
        "}\n",
        "\n",
        "json_files = ''\n",
        "# LLM and Human files use different IDs (eid VS id)\n",
        "id_label = ''\n",
        "if eval_method == 'LLM':\n",
        "  # Load the files one at a time; there is one file per LLM-language combination (4*3 = 12 files in total)\n",
        "  json_folder = '/content/LLM-eval_final_jsons'\n",
        "  json_files = glob.glob(os.path.join(json_folder, '*.json'))\n",
        "  id_label = 'eid'\n",
        "\n",
        "elif eval_method == 'Human':\n",
        "  # Load the files one at a time; there is one file per language (since human annotators did not each do all annotations, unlike LLMs)\n",
        "  json_folder = '/content/human-eval_final_jsons'\n",
        "  json_files = glob.glob(os.path.join(json_folder, '*.json'))\n",
        "  id_label = 'id'\n",
        "\n",
        "pd_list = [pd.read_json(file) for file in sorted(json_files)]\n",
        "\n",
        "count_systems_no_score = 0\n",
        "# The beginning of the following is the same as in the cell that calculates system-level scores\n",
        "for df in pd_list:\n",
        "  # if DEBUG:\n",
        "  #   print(df)\n",
        "  # split eid column using underscores and create corresponding columns in the dataframe\n",
        "  df[['language', 'task', 'input_id', 'system']] = df[id_label].str.split('_', expand=True)\n",
        "\n",
        "  unique_languages = sorted(df['language'].unique())\n",
        "  unique_tasks = sorted(df['task'].unique())\n",
        "  # id contains the whole info: language, task, input_id, system\n",
        "  # unique_ids = sorted(df['id'].unique())\n",
        "  # Exclude withdrawn system\n",
        "  unique_systems = [x for x in sorted(df['system'].unique()) if x in system_IDnames_dico[df['language'][0]].keys()]\n",
        "  if DEBUG:\n",
        "    if eval_method == 'LLM':\n",
        "      print(df['annotator_id'][0])\n",
        "    else:\n",
        "      print('Human')\n",
        "    print(unique_languages)\n",
        "    print(unique_tasks)\n",
        "    print(unique_systems)\n",
        "  for language in unique_languages:\n",
        "    if eval_method == 'LLM':\n",
        "      print(f'Processing {language} {df[\"annotator_id\"][0]}...')\n",
        "    else:\n",
        "      print(f'Processing {language} Human...')\n",
        "    for task in unique_tasks:\n",
        "      # We need to count unique_inputs at a task level, since in D2T-1 and D2T-2 the sampled inputs are not the same\n",
        "      unique_input_ids = sorted(df.loc[(df['language'] == language) & (df['task'] == task), 'input_id'].unique())\n",
        "      print(f'  {task}: # unique input IDs = {len(unique_input_ids)}')\n",
        "      for system in unique_systems:\n",
        "        filename = ''\n",
        "        if eval_method == 'LLM':\n",
        "          filename = f'{language}_{task}_{system}_{df[\"annotator_id\"][0]}.csv'\n",
        "        else:\n",
        "          filename = f'{language}_{task}_{system}_Human.csv'\n",
        "\n",
        "        # Check if there are results for the combination of language, task and system\n",
        "        if df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system)].empty:\n",
        "          if DEBUG:\n",
        "            print(f'No scores found for {language}-{task}-{system}')\n",
        "          count_systems_no_score += 1\n",
        "        else:\n",
        "          if DEBUG:\n",
        "            print(f'{language}-{task}-{system} OK')\n",
        "          # Make a new dataframe with the columns id, no_om_label, no_add_label, gram_label, flu_label and add rows that correspond to language, task and system\n",
        "          scores_df = ''\n",
        "          if eval_method == 'LLM':\n",
        "            scores_df = pd.DataFrame(columns=['id', 'no-omissions', 'no-additions', 'grammaticality', 'fluency', 'coherence', 'num_scores'])\n",
        "            # For LLM evals, we have on file per LLM/language in which there is one rating for each of the 180 datapoints for all datasets and all systems (except for the few missing ratings from R1)\n",
        "            # Thus we simply need to put select the 180 ratings of one system on one dataset (the language condition below is not needed but left just in case)\n",
        "            scores_df['id'] = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system), 'input_id']\n",
        "            scores_df['no-omissions'] = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system), 'no-omissions']\n",
        "            scores_df['no-additions'] = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system), 'no-additions']\n",
        "            scores_df['grammaticality'] = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system), 'grammaticality']\n",
        "            scores_df['fluency'] = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system), 'fluency']\n",
        "            scores_df['coherence'] = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system), 'coherence']\n",
        "            scores_df['num_scores'] = 1\n",
        "            # Actually, I'd like the LLM and Human files to be aligned for easier processing, so let's sort the rows\n",
        "            # I could use the same code as for the human files, but the latter is much slower (I checked, the outputs are the same in both cases)\n",
        "            scores_df = scores_df.sort_values(by=['id']).reset_index(drop=True)\n",
        "          else:\n",
        "            scores_df = pd.DataFrame(columns=['id', 'grammaticality', 'fluency', 'coherence', 'num_scores'])\n",
        "            # For human evals, we have one file per language with 2 to n annotation for each of the 180 datapoints for all datasets and all systems.\n",
        "            # We need to average these 2 to n annotations so as to get 180 scores per dataset per system per language, as for LLMs\n",
        "            # For each language/task/system, get the 2 to n scores for each input\n",
        "            for i, input_id in enumerate(unique_input_ids):\n",
        "              scores_gram = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system) & (df['input_id'] == input_id), 'grammaticality']\n",
        "              scores_flu = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system) & (df['input_id'] == input_id), 'fluency']\n",
        "              scores_coh = df.loc[(df['language'] == language) & (df['task'] == task) & (df['system'] == system) & (df['input_id'] == input_id), 'coherence']\n",
        "              scores_df.loc[i] = [input_id, scores_gram.mean(), scores_flu.mean(), scores_coh.mean(), scores_gram.count()]\n",
        "\n",
        "          if DEBUG:\n",
        "            print(scores_df)\n",
        "\n",
        "          # Save scores_df in csv_for_correls\n",
        "          scores_df.to_csv(os.path.join('/content/csv_for_correls', filename), index=False)\n",
        "\n",
        "if eval_method == 'LLM':\n",
        "  print(f'Number of combinations of language/dataset/system with no score: {str(count_systems_no_score)} (expected: 44)')\n",
        "else:\n",
        "  print(f'Number of combinations of language/dataset/system with no score: {str(count_systems_no_score)} (expected: 0)')"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "NGXF2gvbnX-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Zip and download csv folders\n",
        "from google.colab import files\n",
        "\n",
        "! zip -r /content/csv_for_correls.zip /content/csv_for_correls\n",
        "files.download('/content/csv_for_correls.zip')\n",
        "\n",
        "# ! zip -r /content/csv_sysLevel_scores.zip /content/csv_sysLevel_scores\n",
        "# files.download('/content/csv_sysLevel_scores.zip')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lkadFRh_tQsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tukey HSD\n",
        "These cells takes as input a zip with the CSVs created above for correlations (not exactly the zip created by this notebook). The code was adapted from the WebNLG shared task code with the help of J. Sedoc."
      ],
      "metadata": {
        "id": "jruNwAoF_vW1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoRLgqcye3r5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!Rscript -e 'install.packages(c(\"agricolae\", \"irr\", \"reshape2\", \"Hmisc\", \"corrplot\"), repos=\"https://cloud.r-project.org/\")'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip files\n",
        "!unzip csv_for_correls.zip"
      ],
      "metadata": {
        "id": "j1RJCgVvkBOL",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select parameters, create empty folders\n",
        "import os\n",
        "DATASET = \"D2T-1\"   #@param[\"D2T-1\",\"D2T-2\",\"*\"]\n",
        "SUBSET  = \"FA\"     #@param[\"FA\", \"CFA\", \"FI\", \"*\"]\n",
        "LANGUAGE = \"ga\"     #@param[\"en\", \"ga\"]\n",
        "EVALUATOR = \"human\" #@param[\"human\", \"llm\"]\n",
        "# --------------------------\n",
        "if not os.path.exists('/content/tables_hum-llm-separated'):\n",
        "  os.mkdir('/content/tables_hum-llm-separated')\n",
        "\n",
        "if not os.path.exists('/content/tables_hum-llm-together'):\n",
        "  os.mkdir('/content/tables_hum-llm-together')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iHYrZNOwkleK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create TeX tables"
      ],
      "metadata": {
        "id": "iyOOYjOWkcZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Put CSVs together for computing rankings\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "\n",
        "# Map of system IDs to system names\n",
        "system_IDnames_dico = {\n",
        "  'en': {'0': 'e2e', '1': 'sng', '2': 'mul', '3': 'e2e-0.95', '4': 'e2e-0.90',\n",
        "          '5': 'e2e-0.85', '6': 'e2e-input0.95', '7': 'e2e-input0.90', '8': 'e2e-input0.85'},\n",
        "  'ga': {'0': 'e2e', '1': 'sng', '2': 'mul', '3': 'e2e-0.95', '4': 'e2e-0.90',\n",
        "          '5': 'e2e-0.85', '6': 'e2e-input0.95', '7': 'e2e-input0.90', '8': 'e2e-input0.85'},\n",
        "}\n",
        "\n",
        "system_ID_datasets = {'0':['en_D2T-1-FA', 'ga_D2T-1-FA'],\n",
        "                      '1':['en_D2T-1-FA', 'ga_D2T-1-FA'],\n",
        "                      '2':['en_D2T-1-FA', 'ga_D2T-1-FA'],\n",
        "                      '3':['en_D2T-1-FA', 'ga_D2T-1-FA'],\n",
        "                      '4':['en_D2T-1-FA', 'ga_D2T-1-FA'],\n",
        "                      '5':['en_D2T-1-FA', 'ga_D2T-1-FA'],\n",
        "                      '6':['en_D2T-1-FA', 'ga_D2T-1-FA'],\n",
        "                      '7':['en_D2T-1-FA', 'ga_D2T-1-FA'],\n",
        "                      '8':['en_D2T-1-FA', 'ga_D2T-1-FA'],\n",
        "                      }\n",
        "# path to the files to consider for calculating the rankings\n",
        "pattern = ''\n",
        "if EVALUATOR == 'human':\n",
        "  pattern = f\"{LANGUAGE}/{LANGUAGE}_{DATASET}-{SUBSET}_*_Human.csv\"\n",
        "elif EVALUATOR == 'llm':\n",
        "  # Models are GPT-4o, GPT-o3, Gemini or R1, while humans are \"Human\", so looking for G or R should be enough here\n",
        "  pattern = f\"{LANGUAGE}/{LANGUAGE}_{DATASET}-{SUBSET}_*_[GRC]*.csv\"\n",
        "# print(pattern)\n",
        "files_to_consider = glob(pattern)\n",
        "\n",
        "total_num_annot = 0\n",
        "items_IAA = []\n",
        "data = pd.DataFrame()\n",
        "for fn in sorted(files_to_consider):\n",
        "  print(fn)\n",
        "  # Get team ID\n",
        "  team = fn.split(\"_\")[2]\n",
        "  # Get subset to use (are we using this afterwards?)\n",
        "  # data_segment = fn.split(\"_\")[1].split(\"-\")[-1]\n",
        "  df = pd.read_csv(fn)\n",
        "  # Check whether a system needs to be considered for a ranking\n",
        "  dataset_ID_4_rankings = f'{LANGUAGE}_{DATASET}-{SUBSET}'\n",
        "  if dataset_ID_4_rankings in system_ID_datasets[team]:\n",
        "    # Get team name from disctionary above\n",
        "    df['team'] = system_IDnames_dico[LANGUAGE][team]\n",
        "    # df['type'] = data_segment\n",
        "    df.rename(columns={'no-additions': 'no_additions',\n",
        "                      'no-omissions': 'no_omissions'}, inplace=True)\n",
        "    number_before_duplication = len(df)\n",
        "\n",
        "    # For each row, get the number in the number of annotators from which the score was calculated; if more than 1, duplicate the row as many times\n",
        "    for row in df.iterrows():\n",
        "      num_annot_to_add = (row[1].iloc[-2])-1\n",
        "      total_num_annot += row[1].iloc[-2]\n",
        "      if row[1].iloc[-2] > 2:\n",
        "        items_IAA.append(fn+str(row[1].iloc[-2]))\n",
        "      # Add other instances of row to the dataframe\n",
        "      x = 0\n",
        "      while x < num_annot_to_add:\n",
        "        df = pd.concat([df, pd.DataFrame([row[1]], columns=df.columns)])\n",
        "        x = x + 1\n",
        "    number_after_duplication = len(df)\n",
        "    # Print original number of scores and final number of scores used for computing the averages\n",
        "    print(f'  {number_before_duplication} -> {number_after_duplication} scores used after duplicating rows that have several annotators.')\n",
        "    data = pd.concat([data, df])\n",
        "  else:\n",
        "    print(f'  ID {LANGUAGE}_{DATASET}-{SUBSET} not found for team {team}!')\n",
        "#data\n",
        "data.to_csv(\"all_data_final_averaged.csv\", index=False)\n",
        "print(f'\\n----------\\nTotal number of human ratings: {total_num_annot} per criterion ({total_num_annot*4} individual ratings.)')\n",
        "# To get the following number, add 'en_*-*' for systems 0 and 1 in system_ID_datasets\n",
        "print(f'IAA items: {items_IAA}.')"
      ],
      "metadata": {
        "id": "5i_iikF_kgGS",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Python variables into R\n",
        "%load_ext rpy2.ipython\n",
        "%R -i EVALUATOR\n",
        "%R -i LANGUAGE\n",
        "%R -i DATASET\n",
        "%R -i SUBSET"
      ],
      "metadata": {
        "id": "9-w_vuJ7hWXj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LLM ONLY: Get Mean scores, Tukey HSD and create one LaTeX table for each criterion\n",
        "%%R\n",
        "library(xtable)\n",
        "library(ggplot2)\n",
        "library(agricolae)\n",
        "library(reshape2)\n",
        "library(plyr)\n",
        "library(Hmisc)\n",
        "library(corrplot)\n",
        "\n",
        "label_text <- paste(EVALUATOR, LANGUAGE, DATASET, SUBSET, sep=\"-\")\n",
        "print(label_text)\n",
        "all_data_avg <- data.frame(read.csv(\"all_data_final_averaged.csv\"))\n",
        "\n",
        "# We use anova for the sake of handy calculation of means and putting them to latex tables\n",
        "# significance and groups are calculated later\n",
        "anova_function <- function(dep_var, df) {\n",
        "  fit_semantics <- aov(dep_var ~ team, data=df)\n",
        "  summary(fit_semantics)\n",
        "  TukeyHSD(fit_semantics)\n",
        "  tt <- HSD.test(fit_semantics, \"team\", group=TRUE)\n",
        "  tt\n",
        "}\n",
        "anova_grammar <- anova_function(all_data_avg$grammaticality, all_data_avg)\n",
        "anova_fluency <- anova_function(all_data_avg$fluency, all_data_avg)\n",
        "anova_coherence <- anova_function(all_data_avg$coherence, all_data_avg)\n",
        "anova_no_omissions <- anova_function(all_data_avg$no_omissions, all_data_avg)\n",
        "anova_no_additions <- anova_function(all_data_avg$no_additions, all_data_avg)\n",
        "\n",
        "# seen_data = subset(all_data_avg, type == 'seen')\n",
        "# unseen_data = subset(all_data_avg, type == 'unseen')\n",
        "\n",
        "# anova_semantics_seen <- anova_function(seen_data$semantics, seen_data)\n",
        "# anova_grammar_seen <- anova_function(seen_data$grammar, seen_data)\n",
        "# anova_fluency_seen <- anova_function(seen_data$fluency, seen_data)\n",
        "# anova_semantics_unseen <- anova_function(unseen_data$semantics, unseen_data)\n",
        "# anova_grammar_unseen <- anova_function(unseen_data$grammar, unseen_data)\n",
        "# anova_fluency_unseen <- anova_function(unseen_data$fluency, unseen_data)\n",
        "\n",
        "# add_commands <- list(\n",
        "#   pos = list(0),\n",
        "#   command = \"\\\\small\\n\"\n",
        "# )\n",
        "# Previous block works with the \",add.to.row = add_commands\" parameter in the \"print\" below. But it inserts the \\small at the wrong place so I don't use it now.\n",
        "\n",
        "# generate latex tables\n",
        "\n",
        "label_noOm = paste(\"No-omissions\", label_text)\n",
        "label_noOm_table = paste0(\"tukey-noOM-\", label_text)\n",
        "df_no_omissions <- data.frame(anova_no_omissions$groups)\n",
        "####### vvv Lines to have the group as superscript to the mean vvv #######\n",
        "# df_no_omissions$Mean <- sprintf(\"%.2f$^{%s}$\", df_no_omissions$dep_var, df_no_omissions$groups)\n",
        "# # remove the raw dep_var and groups columns\n",
        "# df_no_omissions$dep_var <- NULL\n",
        "# df_no_omissions$groups <- NULL\n",
        "# colnames(df_no_omissions) <- c(\"Mean\")\n",
        "####### ^^^ Lines to have the group as superscript to the mean ^^^ #######\n",
        "####### vvv Lines to have the group as a separate column vvv #######\n",
        "colnames(df_no_omissions) <- c(\"Mean\", \"Group\")\n",
        "df_no_omissions$Group <- toupper(df_no_omissions$Group)\n",
        "####### ^^^ Lines to have the group as a separate column ^^^ #######\n",
        "print('no omissions')\n",
        "print(xtable(df_no_omissions, caption = label_noOm, label = paste0(\"tab:\", label_noOm_table)), include.rownames = TRUE, sanitize.text.function = identity, file = paste0(label_noOm_table, \".tex\"))\n",
        "\n",
        "label_noAd = paste(\"No-additions\", label_text)\n",
        "label_noAd_table = paste0(\"tukey-noAD-\", label_text)\n",
        "df_no_additions <- data.frame(anova_no_additions$groups)\n",
        "####### vvv Lines to have the group as superscript to the mean vvv #######\n",
        "# df_no_additions$Mean <- sprintf(\"%.2f$^{%s}$\", df_no_additions$dep_var, df_no_additions$groups)\n",
        "# # remove the raw dep_var and groups columns\n",
        "# df_no_additions$dep_var <- NULL\n",
        "# df_no_additions$groups <- NULL\n",
        "# colnames(df_no_additions) <- c(\"Mean\")\n",
        "####### ^^^ Lines to have the group as superscript to the mean ^^^ #######\n",
        "####### vvv Lines to have the group as a separate column vvv #######\n",
        "colnames(df_no_additions) <- c(\"Mean\", \"Group\")\n",
        "df_no_additions$Group <- toupper(df_no_additions$Group)\n",
        "####### ^^^ Lines to have the group as a separate column ^^^ #######\n",
        "print('no additions')\n",
        "print(xtable(df_no_additions, caption = label_noAd, label = paste0(\"tab:\", label_noAd_table)), include.rownames = TRUE, sanitize.text.function = identity, file = paste0(label_noAd_table, \".tex\"))\n",
        "\n",
        "label_gram = paste(\"Grammaticality\", label_text)\n",
        "label_gram_table = paste0(\"tukey-gram-\", label_text)\n",
        "df_grammar <- data.frame(anova_grammar$groups)\n",
        "####### vvv Lines to have the group as superscript to the mean vvv #######\n",
        "# df_grammar$Mean <- sprintf(\"%.2f$^{%s}$\", df_grammar$dep_var, df_grammar$groups)\n",
        "# # remove the raw dep_var and groups columns\n",
        "# df_grammar$dep_var <- NULL\n",
        "# df_grammar$groups <- NULL\n",
        "# colnames(df_grammar) <- c(\"Mean\")\n",
        "####### ^^^ Lines to have the group as superscript to the mean ^^^ #######\n",
        "####### vvv Lines to have the group as a separate column vvv #######\n",
        "colnames(df_grammar) <- c(\"Mean\", \"Group\")\n",
        "df_grammar$Group <- toupper(df_grammar$Group)\n",
        "####### ^^^ Lines to have the group as a separate column ^^^ #######\n",
        "print('grammaticality')\n",
        "print(xtable(df_grammar, caption = label_gram, label = paste0(\"tab:\", label_gram_table)), include.rownames = TRUE, sanitize.text.function = identity, file = paste0(label_gram_table, \".tex\"))\n",
        "\n",
        "label_flu = paste(\"Fluency\", label_text)\n",
        "label_flu_table = paste0(\"tukey-flu-\", label_text)\n",
        "df_fluency <- data.frame(anova_fluency$groups)\n",
        "####### vvv Lines to have the group as superscript to the mean vvv #######\n",
        "# df_fluency$Mean <- sprintf(\"%.2f$^{%s}$\", df_fluency$dep_var, df_fluency$groups)\n",
        "# # remove the raw dep_var and groups columns\n",
        "# df_fluency$dep_var <- NULL\n",
        "# df_fluency$groups <- NULL\n",
        "# colnames(df_fluency) <- c(\"Mean\")\n",
        "####### ^^^ Lines to have the group as superscript to the mean ^^^ #######\n",
        "####### vvv Lines to have the group as a separate column vvv #######\n",
        "colnames(df_fluency) <- c(\"Mean\", \"Group\")\n",
        "df_fluency$Group <- toupper(df_fluency$Group)\n",
        "####### ^^^ Lines to have the group as a separate column ^^^ #######\n",
        "print('fluency')\n",
        "print(xtable(df_fluency, caption = label_flu, label = paste0(\"tab:\", label_flu_table)), include.rownames = TRUE, sanitize.text.function = identity, file = paste0(label_flu_table, \".tex\"))\n",
        "\n",
        "label_coh = paste(\"Coherence\", label_text)\n",
        "label_coh_table = paste0(\"tukey-coh-\", label_text)\n",
        "df_coherence <- data.frame(anova_coherence$groups)\n",
        "####### vvv Lines to have the group as superscript to the mean vvv #######\n",
        "# df_fluency$Mean <- sprintf(\"%.2f$^{%s}$\", df_fluency$dep_var, df_fluency$groups)\n",
        "# # remove the raw dep_var and groups columns\n",
        "# df_fluency$dep_var <- NULL\n",
        "# df_fluency$groups <- NULL\n",
        "# colnames(df_fluency) <- c(\"Mean\")\n",
        "####### ^^^ Lines to have the group as superscript to the mean ^^^ #######\n",
        "####### vvv Lines to have the group as a separate column vvv #######\n",
        "colnames(df_coherence) <- c(\"Mean\", \"Group\")\n",
        "df_coherence$Group <- toupper(df_coherence$Group)\n",
        "####### ^^^ Lines to have the group as a separate column ^^^ #######\n",
        "print('coherence')\n",
        "print(xtable(df_coherence, caption = label_coh, label = paste0(\"tab:\", label_coh_table)), include.rownames = TRUE, sanitize.text.function = identity, file = paste0(label_coh_table, \".tex\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "A6Xd5Cege4el",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title HUMAN ONLY: Get Mean scores, Tukey HSD and create one LaTeX table for each criterion\n",
        "%%R\n",
        "library(xtable)\n",
        "library(ggplot2)\n",
        "library(agricolae)\n",
        "library(reshape2)\n",
        "library(plyr)\n",
        "library(Hmisc)\n",
        "library(corrplot)\n",
        "\n",
        "label_text <- paste(EVALUATOR, LANGUAGE, DATASET, SUBSET, sep=\"-\")\n",
        "print(label_text)\n",
        "all_data_avg <- data.frame(read.csv(\"all_data_final_averaged.csv\"))\n",
        "\n",
        "# We use anova for the sake of handy calculation of means and putting them to latex tables\n",
        "# significance and groups are calculated later\n",
        "anova_function <- function(dep_var, df) {\n",
        "  fit_semantics <- aov(dep_var ~ team, data=df)\n",
        "  summary(fit_semantics)\n",
        "  TukeyHSD(fit_semantics)\n",
        "  tt <- HSD.test(fit_semantics, \"team\", group=TRUE)\n",
        "  tt\n",
        "}\n",
        "anova_grammar <- anova_function(all_data_avg$grammaticality, all_data_avg)\n",
        "anova_fluency <- anova_function(all_data_avg$fluency, all_data_avg)\n",
        "anova_coherence <- anova_function(all_data_avg$coherence, all_data_avg)\n",
        "\n",
        "# seen_data = subset(all_data_avg, type == 'seen')\n",
        "# unseen_data = subset(all_data_avg, type == 'unseen')\n",
        "\n",
        "# anova_semantics_seen <- anova_function(seen_data$semantics, seen_data)\n",
        "# anova_grammar_seen <- anova_function(seen_data$grammar, seen_data)\n",
        "# anova_fluency_seen <- anova_function(seen_data$fluency, seen_data)\n",
        "# anova_semantics_unseen <- anova_function(unseen_data$semantics, unseen_data)\n",
        "# anova_grammar_unseen <- anova_function(unseen_data$grammar, unseen_data)\n",
        "# anova_fluency_unseen <- anova_function(unseen_data$fluency, unseen_data)\n",
        "\n",
        "# add_commands <- list(\n",
        "#   pos = list(0),\n",
        "#   command = \"\\\\small\\n\"\n",
        "# )\n",
        "# Previous block works with the \",add.to.row = add_commands\" parameter in the \"print\" below. But it inserts the \\small at the wrong place so I don't use it now.\n",
        "\n",
        "# generate latex tables\n",
        "\n",
        "label_gram = paste(\"Grammaticality\", label_text)\n",
        "label_gram_table = paste0(\"tukey-gram-\", label_text)\n",
        "df_grammar <- data.frame(anova_grammar$groups)\n",
        "####### vvv Lines to have the group as superscript to the mean vvv #######\n",
        "# df_grammar$Mean <- sprintf(\"%.2f$^{%s}$\", df_grammar$dep_var, df_grammar$groups)\n",
        "# # remove the raw dep_var and groups columns\n",
        "# df_grammar$dep_var <- NULL\n",
        "# df_grammar$groups <- NULL\n",
        "# colnames(df_grammar) <- c(\"Mean\")\n",
        "####### ^^^ Lines to have the group as superscript to the mean ^^^ #######\n",
        "####### vvv Lines to have the group as a separate column vvv #######\n",
        "colnames(df_grammar) <- c(\"Mean\", \"Group\")\n",
        "df_grammar$Group <- toupper(df_grammar$Group)\n",
        "####### ^^^ Lines to have the group as a separate column ^^^ #######\n",
        "print('grammaticality')\n",
        "print(xtable(df_grammar, caption = label_gram, label = paste0(\"tab:\", label_gram_table)), include.rownames = TRUE, sanitize.text.function = identity, file = paste0(label_gram_table, \".tex\"))\n",
        "\n",
        "label_flu = paste(\"Fluency\", label_text)\n",
        "label_flu_table = paste0(\"tukey-flu-\", label_text)\n",
        "df_fluency <- data.frame(anova_fluency$groups)\n",
        "####### vvv Lines to have the group as superscript to the mean vvv #######\n",
        "# df_fluency$Mean <- sprintf(\"%.2f$^{%s}$\", df_fluency$dep_var, df_fluency$groups)\n",
        "# # remove the raw dep_var and groups columns\n",
        "# df_fluency$dep_var <- NULL\n",
        "# df_fluency$groups <- NULL\n",
        "# colnames(df_fluency) <- c(\"Mean\")\n",
        "####### ^^^ Lines to have the group as superscript to the mean ^^^ #######\n",
        "####### vvv Lines to have the group as a separate column vvv #######\n",
        "colnames(df_fluency) <- c(\"Mean\", \"Group\")\n",
        "df_fluency$Group <- toupper(df_fluency$Group)\n",
        "####### ^^^ Lines to have the group as a separate column ^^^ #######\n",
        "print('fluency')\n",
        "print(xtable(df_fluency, caption = label_flu, label = paste0(\"tab:\", label_flu_table)), include.rownames = TRUE, sanitize.text.function = identity, file = paste0(label_flu_table, \".tex\"))\n",
        "\n",
        "label_coh = paste(\"Coherence\", label_text)\n",
        "label_coh_table = paste0(\"tukey-coh-\", label_text)\n",
        "df_coherence <- data.frame(anova_coherence$groups)\n",
        "####### vvv Lines to have the group as superscript to the mean vvv #######\n",
        "# df_fluency$Mean <- sprintf(\"%.2f$^{%s}$\", df_fluency$dep_var, df_fluency$groups)\n",
        "# # remove the raw dep_var and groups columns\n",
        "# df_fluency$dep_var <- NULL\n",
        "# df_fluency$groups <- NULL\n",
        "# colnames(df_fluency) <- c(\"Mean\")\n",
        "####### ^^^ Lines to have the group as superscript to the mean ^^^ #######\n",
        "####### vvv Lines to have the group as a separate column vvv #######\n",
        "colnames(df_coherence) <- c(\"Mean\", \"Group\")\n",
        "df_coherence$Group <- toupper(df_coherence$Group)\n",
        "####### ^^^ Lines to have the group as a separate column ^^^ #######\n",
        "print('coherence')\n",
        "print(xtable(df_coherence, caption = label_coh, label = paste0(\"tab:\", label_coh_table)), include.rownames = TRUE, sanitize.text.function = identity, file = paste0(label_coh_table, \".tex\"))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mjVr7vZk1cwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export variables to Python\n",
        "%R -o label_noOm_table\n",
        "%R -o label_noAd_table\n",
        "%R -o label_gram_table\n",
        "%R -o label_flu_table\n",
        "%R -o label_coh_table\n",
        "%R -o label_text"
      ],
      "metadata": {
        "id": "zOuC_Z22SnY9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Combine tables for 4 criteria into one figure\n",
        "import re\n",
        "\n",
        "noOm_tex = label_noOm_table[0]+'.tex'\n",
        "noAd_tex = label_noAd_table[0]+'.tex'\n",
        "gram_tex = label_gram_table[0]+'.tex'\n",
        "flu_tex = label_flu_table[0]+'.tex'\n",
        "coh_tex = label_coh_table[0]+'.tex'\n",
        "\n",
        "list_tex_tables = []\n",
        "if EVALUATOR == 'llm':\n",
        "  list_tex_tables = [noOm_tex, noAd_tex, gram_tex, flu_tex, coh_tex]#\n",
        "else:\n",
        "  list_tex_tables = [gram_tex, flu_tex, coh_tex]\n",
        "\n",
        "print(list_tex_tables)\n",
        "\n",
        "combined_lines = []\n",
        "\n",
        "# Start figure environment\n",
        "combined_lines.append(\"\\\\begin{figure}[!tbh]\")\n",
        "combined_lines.append(\"\\\\small\")\n",
        "combined_lines.append(\"\\\\centering\")\n",
        "\n",
        "# Loop through tables and insert as subfigures\n",
        "for i, f in enumerate(list_tex_tables):\n",
        "    with open(f, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Remove the table environment lines (\\begin{table}...\\end{table})\n",
        "    table_lines = []\n",
        "    inside_table = False\n",
        "    subcaption = ''\n",
        "    for line in lines:\n",
        "        if line.strip().startswith(\"\\\\begin{table\"):\n",
        "            inside_table = True\n",
        "            continue\n",
        "        if line.strip().startswith(\"\\\\end{table\"):\n",
        "            inside_table = False\n",
        "            continue\n",
        "        if line.strip().startswith(\"\\\\caption{\"):\n",
        "            subcaption = line.strip()[9:-1]\n",
        "            continue\n",
        "        if inside_table or not (\"\\\\begin{table\" in line or \"\\\\end{table\" in line):\n",
        "            # Add phantom letters for vetical alignment in final table\n",
        "            if re.search(' & B[\\\\sC]', line):\n",
        "                line = line.replace(' & B', ' & \\\\phantom{A}B')\n",
        "            elif re.search(' & C[\\\\sD]', line):\n",
        "                line = line.replace(' & C', ' & \\\\phantom{A}\\\\phantom{B}C')\n",
        "            elif re.search(' & D[\\\\sE]', line):\n",
        "                line = line.replace(' & D', ' & \\\\phantom{A}\\\\phantom{B}\\\\phantom{C}D')\n",
        "            elif re.search(' & E[\\\\sF]', line):\n",
        "                line = line.replace(' & E', ' & \\\\phantom{A}\\\\phantom{B}\\\\phantom{C}\\\\phantom{D}E')\n",
        "            elif re.search(' & F[\\\\sG]', line):\n",
        "                line = line.replace(' & F', ' & \\\\phantom{A}\\\\phantom{B}\\\\phantom{C}\\\\phantom{D}\\\\phantom{E}F')\n",
        "            elif re.search(' & G[\\\\sH]', line):\n",
        "                line = line.replace(' & G', ' & \\\\phantom{A}\\\\phantom{B}\\\\phantom{C}\\\\phantom{D}\\\\phantom{E}\\\\phantom{F}G')\n",
        "            elif re.search(' & H[\\\\sI]', line):\n",
        "                line = line.replace(' & H', ' & \\\\phantom{A}\\\\phantom{B}\\\\phantom{C}\\\\phantom{D}\\\\phantom{E}\\\\phantom{F}\\\\phantom{G}H')\n",
        "            table_lines.append(line.rstrip())\n",
        "\n",
        "    # Wrap each table in a minipage (50% width) to make 1x4 column\n",
        "    combined_lines.append(\"\\\\begin{minipage}{0.48\\\\textwidth}\")\n",
        "    combined_lines.append(\"\\\\centering\")\n",
        "    combined_lines.append(f\"\\\\subcaption{{{subcaption}}}\")  # optional subcaption\n",
        "    combined_lines.extend(table_lines)\n",
        "    combined_lines.append(\"\\\\end{minipage}\")\n",
        "\n",
        "    combined_lines.append(\"\\\\vspace{0.3cm}\\n\")  # vertical space\n",
        "    # Add spacing and line break after every 2 tables\n",
        "    # if (i + 1) % 2 == 0:\n",
        "    #     combined_lines.append(\"\\\\vspace{0.5cm}\\n\")  # vertical space\n",
        "    # else:\n",
        "    #     combined_lines.append(\"\\\\hspace{0.02\\\\textwidth}\")  # horizontal space between tables\n",
        "\n",
        "# Optional: main figure caption\n",
        "combined_lines.append(\"\\\\caption{System rankings \"+str(label_text[0])+\"}\")\n",
        "combined_lines.append(\"\\\\end{figure}\")\n",
        "\n",
        "clean_label = label_text[0].replace('*', '[]')\n",
        "output_file = os.path.join('/content/tables_hum-llm-separated', f\"table_{clean_label}.tex\")\n",
        "# Write to output file\n",
        "with open(output_file, \"w\") as out:\n",
        "    out.write(\"\\n\".join(combined_lines))\n",
        "\n",
        "print(f\"Combined 1x4 table figure saved to {output_file}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vIrh5NiRT1Os",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KdXUnCiTYBm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Move individual tables, Zip and download individual and separated hum-llm tables folders"
      ],
      "metadata": {
        "id": "I_SwzzY0w4cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# zip and download tables folder\n",
        "from google.colab import files\n",
        "!zip -r /content/tables_separated.zip /content/tables_hum-llm-separated\n",
        "\n",
        "files.download(\"/content/tables_separated.zip\")\n"
      ],
      "metadata": {
        "id": "SqvW6jXAwj83",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip and download tables folder\n",
        "from google.colab import files\n",
        "import glob\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/tables_individual'):\n",
        "  os.mkdir('/content/tables_individual')\n",
        "\n",
        "list_indiv_tables = glob.glob('/content/*.tex')\n",
        "# Move all files to tables_individual\n",
        "for indiv_table in list_indiv_tables:\n",
        "  !mv $indiv_table /content/tables_individual\n",
        "\n",
        "list_indiv_tables_newLoc = glob.glob('/content/tables_individual/*.tex')\n",
        "# Rename all files, replacing asterisk by []\n",
        "for indiv_table_reloc in list_indiv_tables_newLoc:\n",
        "  if re.search('\\\\*', indiv_table_reloc):\n",
        "    new_name = indiv_table_reloc.replace('*', '[]')\n",
        "    os.rename(indiv_table_reloc, new_name)\n",
        "\n",
        "!zip -r /content/tables_individual.zip /content/tables_individual\n",
        "files.download(\"/content/tables_individual.zip\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tTyJKNp3kf-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine TeX tables for  hum and llm results for the 4 criteria.\n",
        "This block can only be run when you have generated individual tables for each criterion for each evaluation method (llm or human). 8 tables are being merged in one here."
      ],
      "metadata": {
        "id": "qOdWHN29MzBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip folder with all individual tables if not created with the cells above\n",
        "import os\n",
        "\n",
        "use_uploaded_zip = False#@param{type:'boolean'}\n",
        "\n",
        "if use_uploaded_zip:\n",
        "# Unzip /content/tables_tukey_v2_individual.zip to the folder /content/tables_individual\n",
        "  !unzip /content/tables_tukey_v2_individual.zip\n",
        "\n",
        "  # Move folder tables_individual up\n",
        "  !mv /content/content/tables_individual /content/tables_individual\n",
        "  !rm -rf /content/content\n",
        "\n",
        "  if not os.path.exists('/content/tables_hum-llm-together'):\n",
        "    os.mkdir('/content/tables_hum-llm-together')\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "eYbS3XKI9FjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select parameters\n",
        "import os\n",
        "DATASET = \"D2T-1\"   #@param[\"D2T-1\",\"D2T-2\",\"*\"]\n",
        "SUBSET  = \"FA\"     #@param[\"FA\", \"CFA\", \"FI\", \"*\"]\n",
        "LANGUAGE = \"en\"     #@param[\"en\", \"es\", \"sw\"]\n",
        "# --------------------------"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vxAAOkTYNOA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Combine llm and human tables for 4 criteria into one figure\n",
        "import re\n",
        "import statistics\n",
        "\n",
        "def count_tukey_groupings(list_groups):\n",
        "    \"\"\"\n",
        "    Takes as input a list with all groups found in a table, e.g. ['A', 'A', 'B', 'B', 'C', 'C', 'C', 'D', 'D', 'E', 'E', 'E']\n",
        "    Returns a count of how many times each group is repeated.\n",
        "    \"\"\"\n",
        "    # Create a dico where instances of each group are counted, e.g. for the list above {'A': 2, 'B': 2, 'C': 3, 'D': 2, 'E': 3}\n",
        "    dico_count_group_instances = {i:list_groups.count(i) for i in list_groups}\n",
        "    # Get a list of group duplications (each number in dico above minus 1), e.g. [1, 1, 2, 1, 2] for the dico above.\n",
        "    list_count_repeated = [dico_count_group_instances[i]-1 for i in dico_count_group_instances]\n",
        "    # Get the sum of repeated groups\n",
        "    sum_repeated = sum(list_count_repeated)\n",
        "    return sum_repeated\n",
        "\n",
        "def removeAsterisk(original_string):\n",
        "    clean_string = original_string.replace('*', '[]')\n",
        "    return(clean_string)\n",
        "\n",
        "noOm_hum_tex = 'tukey-noOM-human-'+LANGUAGE+'-'+DATASET+'-'+SUBSET+'.tex'\n",
        "noOm_llm_tex = 'tukey-noOM-llm-'+LANGUAGE+'-'+DATASET+'-'+SUBSET+'.tex'\n",
        "noAd_hum_tex = 'tukey-noAD-human-'+LANGUAGE+'-'+DATASET+'-'+SUBSET+'.tex'\n",
        "noAd_llm_tex = 'tukey-noAD-llm-'+LANGUAGE+'-'+DATASET+'-'+SUBSET+'.tex'\n",
        "gram_hum_tex = 'tukey-gram-human-'+LANGUAGE+'-'+DATASET+'-'+SUBSET+'.tex'\n",
        "gram_llm_tex = 'tukey-gram-llm-'+LANGUAGE+'-'+DATASET+'-'+SUBSET+'.tex'\n",
        "flu_hum_tex = 'tukey-flu-human-'+LANGUAGE+'-'+DATASET+'-'+SUBSET+'.tex'\n",
        "flu_llm_tex = 'tukey-flu-llm-'+LANGUAGE+'-'+DATASET+'-'+SUBSET+'.tex'\n",
        "\n",
        "new_label_text = LANGUAGE+'-'+DATASET+'-'+SUBSET\n",
        "clean_label = removeAsterisk(new_label_text)\n",
        "\n",
        "dico_table_names = {'en-*-*':'English overall', 'en-D2T-1-*':'English in-domain data', 'en-D2T-2-*':'English out-of-domain data', 'en-*-FA':'English factual data', 'en-*-CFA':'English counterfactual data', 'en-*-FI':'English fictional data', 'en-D2T-1-FA':'English in-domain factual data', 'es-*-*':'Spanish overall', 'es-D2T-1-*':'Spanish in-domain data', 'es-D2T-2-*':'Spanish out-of-domain data', 'es-*-FA':'Spanish factual data', 'es-*-CFA':'Spanish counterfactual data', 'es-*-FI':'Spanish fictional data'}\n",
        "\n",
        "# Order matters here because we're going to insert the tables in this order\n",
        "list_tex_tables = [noOm_hum_tex, noOm_llm_tex, noAd_hum_tex, noAd_llm_tex, gram_hum_tex, gram_llm_tex, flu_hum_tex, flu_llm_tex]\n",
        "\n",
        "print(list_tex_tables)\n",
        "\n",
        "combined_lines = []\n",
        "# Start figure environment\n",
        "combined_lines.append(\"\\\\begin{figure*}[!tbh]\")\n",
        "combined_lines.append(\"\\\\small\")\n",
        "combined_lines.append(\"\\\\centering\")\n",
        "\n",
        "# Store scores to get mean scores across systems (useful for analysis)\n",
        "human_scores_noOm = []\n",
        "human_scores_noAd = []\n",
        "human_scores_gram = []\n",
        "human_scores_flu = []\n",
        "llm_scores_noOm = []\n",
        "llm_scores_noAd = []\n",
        "llm_scores_gram = []\n",
        "llm_scores_flu = []\n",
        "# Only count groupings overall, to see which eval methods gives the most ties\n",
        "human_tukey_global = []\n",
        "llm_tukey_global = []\n",
        "\n",
        "# Loop through tables and insert as subfigures\n",
        "for i, f in enumerate(list_tex_tables):\n",
        "    human_tukey =  []\n",
        "    llm_tukey = []\n",
        "    f_path = removeAsterisk(os.path.join('/content', 'tables_individual', f))\n",
        "    with open(f_path, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Remove the table environment lines (\\begin{table}...\\end{table})\n",
        "    table_lines = []\n",
        "    inside_table = False\n",
        "    subcaption = ''\n",
        "    for line in lines:\n",
        "        if line.strip().startswith(\"\\\\begin{table\"):\n",
        "            inside_table = True\n",
        "            continue\n",
        "        if line.strip().startswith(\"\\\\end{table\"):\n",
        "            inside_table = False\n",
        "            continue\n",
        "        if line.strip().startswith(\"\\\\caption{\"):\n",
        "            subcaption = line.strip()[9:-1]\n",
        "            continue\n",
        "        if inside_table or not (\"\\\\begin{table\" in line or \"\\\\end{table\" in line):\n",
        "            # Get scores and groups\n",
        "            if re.search('\\\\&\\\\s[0-9]\\\\.[0-9]+\\\\s\\\\&', line):\n",
        "                score = float(re.search('[0-9]\\\\.[0-9]+', line).group(0))\n",
        "                tukey = list(re.subn('^.*[0-9]\\\\.[0-9]+\\\\s\\\\&\\\\s([A-Z]+).*$', '\\\\g<1>', line.strip())[0])\n",
        "                # Filter out DCU-ADAPT-modPB for the averages of D2T-1-* EN dataset, so we can compare numbers to D2T-2 (for which we have no DCU-ADAPT-modPB).\n",
        "                # Only use for generating the means file, not for the tables, in which we do want DCU-ADAPT-modPB to appear.\n",
        "                # if not (new_label_text == 'en-D2T-1-*' and re.search('DCU-ADAPT-modPB', line)):\n",
        "                if re.search('-human-', f):\n",
        "                    human_tukey.extend(tukey)\n",
        "                    if re.search('-noOM-', f):\n",
        "                        human_scores_noOm. append(score)\n",
        "                    elif re.search('-noAD-', f):\n",
        "                        human_scores_noAd.append(score)\n",
        "                    elif re.search('-gram-', f):\n",
        "                        human_scores_gram.append(score)\n",
        "                    elif re.search('-flu-', f):\n",
        "                        human_scores_flu.append(score)\n",
        "                elif re.search('-llm-', f):\n",
        "                    llm_tukey.extend(tukey)\n",
        "                    if re.search('-noOM-', f):\n",
        "                        llm_scores_noOm. append(score)\n",
        "                    elif re.search('-noAD-', f):\n",
        "                        llm_scores_noAd.append(score)\n",
        "                    elif re.search('-gram-', f):\n",
        "                        llm_scores_gram.append(score)\n",
        "                    elif re.search('-flu-', f):\n",
        "                        llm_scores_flu.append(score)\n",
        "\n",
        "            # Add phantom letters for vetical alignment in final table\n",
        "            if re.search(' & B[\\\\sC]', line):\n",
        "                line = line.replace(' & B', ' & \\\\phantom{A}B')\n",
        "            elif re.search(' & C[\\\\sD]', line):\n",
        "                line = line.replace(' & C', ' & \\\\phantom{A}\\\\phantom{B}C')\n",
        "            elif re.search(' & D[\\\\sE]', line):\n",
        "                line = line.replace(' & D', ' & \\\\phantom{A}\\\\phantom{B}\\\\phantom{C}D')\n",
        "            elif re.search(' & E[\\\\sF]', line):\n",
        "                line = line.replace(' & E', ' & \\\\phantom{A}\\\\phantom{B}\\\\phantom{C}\\\\phantom{D}E')\n",
        "            elif re.search(' & F[\\\\sG]', line):\n",
        "                line = line.replace(' & F', ' & \\\\phantom{A}\\\\phantom{B}\\\\phantom{C}\\\\phantom{D}\\\\phantom{E}F')\n",
        "            elif re.search(' & G[\\\\sH]', line):\n",
        "                line = line.replace(' & G', ' & \\\\phantom{A}\\\\phantom{B}\\\\phantom{C}\\\\phantom{D}\\\\phantom{E}\\\\phantom{F}G')\n",
        "            elif re.search(' & H[\\\\sI]', line):\n",
        "                line = line.replace(' & H', ' & \\\\phantom{A}\\\\phantom{B}\\\\phantom{C}\\\\phantom{D}\\\\phantom{E}\\\\phantom{F}\\\\phantom{G}H')\n",
        "            table_lines.append(line.rstrip())\n",
        "\n",
        "    human_tukey_global.append(count_tukey_groupings(human_tukey))\n",
        "    llm_tukey_global.append(count_tukey_groupings(llm_tukey))\n",
        "\n",
        "    # Wrap each table in a minipage (50% width) to make 1x4 column\n",
        "    combined_lines.append(\"\\\\begin{minipage}{0.48\\\\textwidth}\")\n",
        "    combined_lines.append(\"\\\\centering\")\n",
        "    combined_lines.append(f\"\\\\subcaption{{{subcaption}}}\")  # optional subcaption\n",
        "    combined_lines.extend(table_lines)\n",
        "    combined_lines.append(\"\\\\end{minipage}\")\n",
        "\n",
        "    if re.search('-human-', f):\n",
        "        combined_lines.append(\"\\\\hfill\")\n",
        "    elif re.search('-llm-', f):\n",
        "        combined_lines.append(\"\\\\vspace{0.3cm}\\n\")  # vertical space\n",
        "    # Add spacing and line break after every 2 tables\n",
        "    # if (i + 1) % 2 == 0:\n",
        "    #     combined_lines.append(\"\\\\vspace{0.5cm}\\n\")  # vertical space\n",
        "    # else:\n",
        "    #     combined_lines.append(\"\\\\hspace{0.02\\\\textwidth}\")  # horizontal space between tables\n",
        "\n",
        "# Optional: main figure caption\n",
        "combined_lines.append(\"\\\\caption{System rankings for \"+str(dico_table_names[new_label_text])+\" (left tables: human ratings, right tables: llm ratings) \\\\label{fig:tukey-\"+str(new_label_text)+\"}}\")\n",
        "combined_lines.append(\"\\\\end{figure*}\")\n",
        "\n",
        "output_file = os.path.join('/content/tables_hum-llm-together', f\"table_{clean_label}.tex\")\n",
        "# Write to output file\n",
        "with open(output_file, \"w\") as out:\n",
        "    out.write(\"\\n\".join(combined_lines))\n",
        "\n",
        "print(f\"Combined 2x4 table figure saved to {output_file}\")\n",
        "\n",
        "mean_file = os.path.join('/content/tables_hum-llm-together', f\"{clean_label}_means.txt\")\n",
        "with open(mean_file, \"w\") as out:\n",
        "    out.write(f'{new_label_text} Human No-Omissions mean = {round(statistics.mean(human_scores_noOm), 2)}\\n')\n",
        "    out.write(f'{new_label_text} Human No-Additions mean = {round(statistics.mean(human_scores_noAd), 2)}\\n')\n",
        "    out.write(f'{new_label_text} Human Grammaticality mean = {round(statistics.mean(human_scores_gram), 2)}\\n')\n",
        "    out.write(f'{new_label_text} Human Fluency mean = {round(statistics.mean(human_scores_flu), 2)}\\n\\n')\n",
        "\n",
        "    out.write(f'{new_label_text} LLM No-Omissions mean = {round(statistics.mean(llm_scores_noOm), 2)}\\n')\n",
        "    out.write(f'{new_label_text} LLM No-Additions mean = {round(statistics.mean(llm_scores_noAd), 2)}\\n')\n",
        "    out.write(f'{new_label_text} LLM Grammaticality mean = {round(statistics.mean(llm_scores_gram), 2)}\\n')\n",
        "    out.write(f'{new_label_text} LLM Fluency mean = {round(statistics.mean(llm_scores_flu), 2)}\\n\\n')\n",
        "\n",
        "    out.write(f'Human Tukey groupings: {str(sum(human_tukey_global))}\\n')\n",
        "    out.write(f'LLM Tukey groupings: {str(sum(llm_tukey_global))}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JMeZbXbwNfTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zip and download combined hum-llm tables folder"
      ],
      "metadata": {
        "id": "u0x6uAczQ7Oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# zip and download tables folder\n",
        "from google.colab import files\n",
        "!zip -r /content/tables_combined.zip /content/tables_hum-llm-together\n",
        "\n",
        "files.download(\"/content/tables_combined.zip\")"
      ],
      "metadata": {
        "id": "GkT8Y44eQ2eL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}